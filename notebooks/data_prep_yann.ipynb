{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5011417e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ CONVERSION OPTIMIS√âE POUR GROS FICHIERS\n",
      "üìÅ Fichier source: ../data/raw/Clothing_Shoes_and_Jewelry.jsonl\n",
      "üìÅ Fichier cible: ../data/processed/Clothing_Shoes_and_Jewelry.parquet\n",
      "‚öôÔ∏è Taille des chunks: 50,000\n",
      "üíæ RAM disponible: 14.7 GB\n",
      "üîÑ Comptage des lignes...\n",
      "üìä Total de lignes: 66,033,346\n",
      "üîÑ Traitement par chunks de 50,000 enregistrements...\n",
      "üíæ M√©moire initiale: 36.72 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   4%|‚ñç         | 2800000/66033346 [2:07:44<48:04:46, 365.33 lines/s, chunks=55, records=2,750,000, mem=36.95GB, errors=0] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 166\u001b[39m\n\u001b[32m    163\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33müíæ RAM disponible: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpsutil.virtual_memory().available\u001b[38;5;250m \u001b[39m/\u001b[38;5;250m \u001b[39m\u001b[32m1024\u001b[39m**\u001b[32m3\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m GB\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m     processed, errors = \u001b[43mconvert_large_jsonl_to_parquet_optimized\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCHUNK_SIZE\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    172\u001b[39m     \u001b[38;5;66;03m# Statistiques finales\u001b[39;00m\n\u001b[32m    173\u001b[39m     original_size = os.path.getsize(input_file) / (\u001b[32m1024\u001b[39m**\u001b[32m3\u001b[39m)  \u001b[38;5;66;03m# GB\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 82\u001b[39m, in \u001b[36mconvert_large_jsonl_to_parquet_optimized\u001b[39m\u001b[34m(input_file, output_file, chunk_size)\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m chunk_df\n\u001b[32m     81\u001b[39m chunk_data = []\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[43mgc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Force garbage collection\u001b[39;00m\n\u001b[32m     84\u001b[39m chunk_num += \u001b[32m1\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[38;5;66;03m# Afficher le progr√®s\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Version optimis√©e pour TR√àS GROS fichiers JSONL vers Parquet\n",
    "import json\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from pathlib import Path\n",
    "import os\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import psutil\n",
    "\n",
    "def get_memory_usage():\n",
    "    \"\"\"Retourne l'usage m√©moire actuel\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / 1024 / 1024 / 1024  # GB\n",
    "\n",
    "def convert_large_jsonl_to_parquet_optimized(input_file, output_file, chunk_size=100000):\n",
    "    \"\"\"\n",
    "    Convertit un tr√®s gros fichier JSONL en Parquet par chunks\n",
    "    optimis√© pour la m√©moire\n",
    "    \"\"\"\n",
    "    \n",
    "    # Cr√©er le dossier de sortie\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "    \n",
    "    # Compter le nombre total de lignes pour la barre de progression\n",
    "    print(\"üîÑ Comptage des lignes...\")\n",
    "    total_lines = 0\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        for _ in f:\n",
    "            total_lines += 1\n",
    "    print(f\"üìä Total de lignes: {total_lines:,}\")\n",
    "    \n",
    "    # Variables de traitement\n",
    "    chunk_data = []\n",
    "    chunk_files = []\n",
    "    processed_records = 0\n",
    "    chunk_num = 0\n",
    "    errors = 0\n",
    "    \n",
    "    print(f\"üîÑ Traitement par chunks de {chunk_size:,} enregistrements...\")\n",
    "    print(f\"üíæ M√©moire initiale: {get_memory_usage():.2f} GB\")\n",
    "    \n",
    "    # Traitement avec barre de progression\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        with tqdm(total=total_lines, desc=\"Processing\", unit=\" lines\") as pbar:\n",
    "            \n",
    "            for line_num, line in enumerate(f, 1):\n",
    "                pbar.update(1)\n",
    "                \n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    data_point = json.loads(line)\n",
    "                    chunk_data.append(data_point)\n",
    "                    processed_records += 1\n",
    "                    \n",
    "                except json.JSONDecodeError:\n",
    "                    errors += 1\n",
    "                    continue\n",
    "                \n",
    "                # Sauvegarder le chunk quand il est plein\n",
    "                if len(chunk_data) >= chunk_size:\n",
    "                    chunk_file = f\"{output_file}.chunk_{chunk_num:04d}.parquet\"\n",
    "                    \n",
    "                    # Cr√©er DataFrame et sauvegarder\n",
    "                    chunk_df = pd.DataFrame(chunk_data)\n",
    "                    chunk_df.to_parquet(\n",
    "                        chunk_file,\n",
    "                        compression='snappy',\n",
    "                        index=False,\n",
    "                        engine='pyarrow'\n",
    "                    )\n",
    "                    \n",
    "                    chunk_files.append(chunk_file)\n",
    "                    \n",
    "                    # Lib√©rer la m√©moire\n",
    "                    del chunk_df\n",
    "                    chunk_data = []\n",
    "                    gc.collect()  # Force garbage collection\n",
    "                    \n",
    "                    chunk_num += 1\n",
    "                    \n",
    "                    # Afficher le progr√®s\n",
    "                    memory_usage = get_memory_usage()\n",
    "                    pbar.set_postfix({\n",
    "                        'chunks': chunk_num,\n",
    "                        'records': f\"{processed_records:,}\",\n",
    "                        'mem': f\"{memory_usage:.2f}GB\",\n",
    "                        'errors': errors\n",
    "                    })\n",
    "    \n",
    "    # Traiter le dernier chunk\n",
    "    if chunk_data:\n",
    "        chunk_file = f\"{output_file}.chunk_{chunk_num:04d}.parquet\"\n",
    "        chunk_df = pd.DataFrame(chunk_data)\n",
    "        chunk_df.to_parquet(chunk_file, compression='snappy', index=False)\n",
    "        chunk_files.append(chunk_file)\n",
    "        del chunk_df\n",
    "        chunk_num += 1\n",
    "    \n",
    "    print(f\"\\n‚úÖ Traitement termin√©!\")\n",
    "    print(f\"üìä {processed_records:,} enregistrements trait√©s\")\n",
    "    print(f\"‚ö†Ô∏è {errors} erreurs de parsing\")\n",
    "    print(f\"üìÅ {len(chunk_files)} fichiers chunks cr√©√©s\")\n",
    "    \n",
    "    # Combiner tous les chunks en un seul fichier Parquet\n",
    "    print(f\"\\nüîÑ Fusion des chunks en un seul fichier...\")\n",
    "    \n",
    "    # Lire et combiner par batches pour √©viter la saturation m√©moire\n",
    "    parquet_writer = None\n",
    "    schema = None\n",
    "    \n",
    "    for i, chunk_file in enumerate(tqdm(chunk_files, desc=\"Merging chunks\")):\n",
    "        # Lire le chunk\n",
    "        chunk_df = pd.read_parquet(chunk_file)\n",
    "        \n",
    "        # Convertir en PyArrow Table\n",
    "        table = pa.Table.from_pandas(chunk_df)\n",
    "        \n",
    "        if parquet_writer is None:\n",
    "            # Premier chunk : initialiser le writer\n",
    "            schema = table.schema\n",
    "            parquet_writer = pq.ParquetWriter(\n",
    "                output_file,\n",
    "                schema,\n",
    "                compression='snappy'\n",
    "            )\n",
    "        \n",
    "        # √âcrire le chunk\n",
    "        parquet_writer.write_table(table)\n",
    "        \n",
    "        # Lib√©rer la m√©moire\n",
    "        del chunk_df, table\n",
    "        gc.collect()\n",
    "        \n",
    "        # Supprimer le fichier chunk temporaire\n",
    "        os.remove(chunk_file)\n",
    "    \n",
    "    # Fermer le writer\n",
    "    if parquet_writer:\n",
    "        parquet_writer.close()\n",
    "    \n",
    "    return processed_records, errors\n",
    "\n",
    "# ================================\n",
    "# UTILISATION OPTIMIS√âE\n",
    "# ================================\n",
    "\n",
    "# Chemins des fichiers\n",
    "input_file = \"../data/raw/Clothing_Shoes_and_Jewelry.jsonl\"\n",
    "output_file = \"../data/processed/Clothing_Shoes_and_Jewelry.parquet\"\n",
    "\n",
    "# Param√®tres optimis√©s pour 25GB\n",
    "CHUNK_SIZE = 50000  # Ajustez selon votre RAM (plus petit = moins de RAM)\n",
    "\n",
    "print(f\"üöÄ CONVERSION OPTIMIS√âE POUR GROS FICHIERS\")\n",
    "print(f\"üìÅ Fichier source: {input_file}\")\n",
    "print(f\"üìÅ Fichier cible: {output_file}\")\n",
    "print(f\"‚öôÔ∏è Taille des chunks: {CHUNK_SIZE:,}\")\n",
    "print(f\"üíæ RAM disponible: {psutil.virtual_memory().available / 1024**3:.1f} GB\")\n",
    "\n",
    "try:\n",
    "    processed, errors = convert_large_jsonl_to_parquet_optimized(\n",
    "        input_file, \n",
    "        output_file, \n",
    "        chunk_size=CHUNK_SIZE\n",
    "    )\n",
    "    \n",
    "    # Statistiques finales\n",
    "    original_size = os.path.getsize(input_file) / (1024**3)  # GB\n",
    "    parquet_size = os.path.getsize(output_file) / (1024**3)  # GB\n",
    "    compression_ratio = original_size / parquet_size\n",
    "    \n",
    "    print(f\"\\nüéâ CONVERSION TERMIN√âE!\")\n",
    "    print(f\"üìä {processed:,} enregistrements convertis\")\n",
    "    print(f\"‚ö†Ô∏è {errors} erreurs\")\n",
    "    print(f\"üìÅ Taille JSONL: {original_size:.2f} GB\")\n",
    "    print(f\"üìÅ Taille Parquet: {parquet_size:.2f} GB\") \n",
    "    print(f\"üóúÔ∏è Compression: {compression_ratio:.1f}x\")\n",
    "    print(f\"üíæ M√©moire finale: {get_memory_usage():.2f} GB\")\n",
    "    \n",
    "    # Test rapide de lecture\n",
    "    print(f\"\\nüîÑ Test de lecture...\")\n",
    "    df_sample = pd.read_parquet(output_file, engine='pyarrow').head(1000)\n",
    "    print(f\"‚úÖ Lecture r√©ussie! Colonnes: {list(df_sample.columns)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cbd3c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cudf\n",
      "  Downloading cudf-0.6.1.post1.tar.gz (1.1 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Building wheels for collected packages: cudf\n",
      "  Building wheel for cudf (pyproject.toml): started\n",
      "  Building wheel for cudf (pyproject.toml): finished with status 'error'\n",
      "Failed to build cudf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  √ó Building wheel for cudf (pyproject.toml) did not run successfully.\n",
      "  ‚îÇ exit code: 1\n",
      "  ‚ï∞‚îÄ> [84 lines of output]\n",
      "      C:\\Users\\Yann\\AppData\\Local\\Temp\\pip-build-env-w8w1okbh\\overlay\\Lib\\site-packages\\setuptools\\dist.py:759: SetuptoolsDeprecationWarning: License classifiers are deprecated.\n",
      "      !!\n",
      "      \n",
      "              ********************************************************************************\n",
      "              Please consider removing the following classifiers in favor of a SPDX license expression:\n",
      "      \n",
      "              License :: OSI Approved :: Apache Software License\n",
      "      \n",
      "              See https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#license for details.\n",
      "              ********************************************************************************\n",
      "      \n",
      "      !!\n",
      "        self._finalize_license_expression()\n",
      "      running bdist_wheel\n",
      "      running build\n",
      "      installing to build\\bdist.win-amd64\\wheel\n",
      "      running install\n",
      "      Traceback (most recent call last):\n",
      "        File \u001b[35m\"C:\\Users\\Yann\\Desktop\\DEV\\School\\ml_m1\\NLP\\CustomGPT\\scraper_env\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\"\u001b[0m, line \u001b[35m389\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "          \u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "          \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "        File \u001b[35m\"C:\\Users\\Yann\\Desktop\\DEV\\School\\ml_m1\\NLP\\CustomGPT\\scraper_env\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\"\u001b[0m, line \u001b[35m373\u001b[0m, in \u001b[35mmain\u001b[0m\n",
      "          json_out[\"return_val\"] = \u001b[31mhook\u001b[0m\u001b[1;31m(**hook_input[\"kwargs\"])\u001b[0m\n",
      "                                   \u001b[31m~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "        File \u001b[35m\"C:\\Users\\Yann\\Desktop\\DEV\\School\\ml_m1\\NLP\\CustomGPT\\scraper_env\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\"\u001b[0m, line \u001b[35m280\u001b[0m, in \u001b[35mbuild_wheel\u001b[0m\n",
      "          return \u001b[31m_build_backend().build_wheel\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "                 \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "              \u001b[1;31mwheel_directory, config_settings, metadata_directory\u001b[0m\n",
      "              \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "          \u001b[1;31m)\u001b[0m\n",
      "          \u001b[1;31m^\u001b[0m\n",
      "        File \u001b[35m\"C:\\Users\\Yann\\AppData\\Local\\Temp\\pip-build-env-w8w1okbh\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\"\u001b[0m, line \u001b[35m435\u001b[0m, in \u001b[35mbuild_wheel\u001b[0m\n",
      "          return _build(['bdist_wheel', '--dist-info-dir', str(metadata_directory)])\n",
      "        File \u001b[35m\"C:\\Users\\Yann\\AppData\\Local\\Temp\\pip-build-env-w8w1okbh\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\"\u001b[0m, line \u001b[35m423\u001b[0m, in \u001b[35m_build\u001b[0m\n",
      "          return \u001b[31mself._build_with_temp_dir\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "                 \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "              \u001b[1;31mcmd,\u001b[0m\n",
      "              \u001b[1;31m^^^^\u001b[0m\n",
      "          ...<3 lines>...\n",
      "              \u001b[1;31mself._arbitrary_args(config_settings),\u001b[0m\n",
      "              \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "          \u001b[1;31m)\u001b[0m\n",
      "          \u001b[1;31m^\u001b[0m\n",
      "        File \u001b[35m\"C:\\Users\\Yann\\AppData\\Local\\Temp\\pip-build-env-w8w1okbh\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\"\u001b[0m, line \u001b[35m404\u001b[0m, in \u001b[35m_build_with_temp_dir\u001b[0m\n",
      "          \u001b[31mself.run_setup\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "          \u001b[31m~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "        File \u001b[35m\"C:\\Users\\Yann\\AppData\\Local\\Temp\\pip-build-env-w8w1okbh\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\"\u001b[0m, line \u001b[35m512\u001b[0m, in \u001b[35mrun_setup\u001b[0m\n",
      "          \u001b[31msuper().run_setup\u001b[0m\u001b[1;31m(setup_script=setup_script)\u001b[0m\n",
      "          \u001b[31m~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "        File \u001b[35m\"C:\\Users\\Yann\\AppData\\Local\\Temp\\pip-build-env-w8w1okbh\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\"\u001b[0m, line \u001b[35m317\u001b[0m, in \u001b[35mrun_setup\u001b[0m\n",
      "          \u001b[31mexec\u001b[0m\u001b[1;31m(code, locals())\u001b[0m\n",
      "          \u001b[31m~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^\u001b[0m\n",
      "        File \u001b[35m\"<string>\"\u001b[0m, line \u001b[35m18\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "        File \u001b[35m\"C:\\Users\\Yann\\AppData\\Local\\Temp\\pip-build-env-w8w1okbh\\overlay\\Lib\\site-packages\\setuptools\\__init__.py\"\u001b[0m, line \u001b[35m115\u001b[0m, in \u001b[35msetup\u001b[0m\n",
      "          return \u001b[31mdistutils.core.setup\u001b[0m\u001b[1;31m(**attrs)\u001b[0m\n",
      "                 \u001b[31m~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^\u001b[0m\n",
      "        File \u001b[35m\"C:\\Users\\Yann\\AppData\\Local\\Temp\\pip-build-env-w8w1okbh\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\core.py\"\u001b[0m, line \u001b[35m186\u001b[0m, in \u001b[35msetup\u001b[0m\n",
      "          return run_commands(dist)\n",
      "        File \u001b[35m\"C:\\Users\\Yann\\AppData\\Local\\Temp\\pip-build-env-w8w1okbh\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\core.py\"\u001b[0m, line \u001b[35m202\u001b[0m, in \u001b[35mrun_commands\u001b[0m\n",
      "          \u001b[31mdist.run_commands\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "          \u001b[31m~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "        File \u001b[35m\"C:\\Users\\Yann\\AppData\\Local\\Temp\\pip-build-env-w8w1okbh\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\dist.py\"\u001b[0m, line \u001b[35m1002\u001b[0m, in \u001b[35mrun_commands\u001b[0m\n",
      "          \u001b[31mself.run_command\u001b[0m\u001b[1;31m(cmd)\u001b[0m\n",
      "          \u001b[31m~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^\u001b[0m\n",
      "        File \u001b[35m\"C:\\Users\\Yann\\AppData\\Local\\Temp\\pip-build-env-w8w1okbh\\overlay\\Lib\\site-packages\\setuptools\\dist.py\"\u001b[0m, line \u001b[35m1102\u001b[0m, in \u001b[35mrun_command\u001b[0m\n",
      "          \u001b[31msuper().run_command\u001b[0m\u001b[1;31m(command)\u001b[0m\n",
      "          \u001b[31m~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^\u001b[0m\n",
      "        File \u001b[35m\"C:\\Users\\Yann\\AppData\\Local\\Temp\\pip-build-env-w8w1okbh\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\dist.py\"\u001b[0m, line \u001b[35m1021\u001b[0m, in \u001b[35mrun_command\u001b[0m\n",
      "          \u001b[31mcmd_obj.run\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "          \u001b[31m~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "        File \u001b[35m\"C:\\Users\\Yann\\AppData\\Local\\Temp\\pip-build-env-w8w1okbh\\overlay\\Lib\\site-packages\\setuptools\\command\\bdist_wheel.py\"\u001b[0m, line \u001b[35m405\u001b[0m, in \u001b[35mrun\u001b[0m\n",
      "          \u001b[31mself.run_command\u001b[0m\u001b[1;31m(\"install\")\u001b[0m\n",
      "          \u001b[31m~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^\u001b[0m\n",
      "        File \u001b[35m\"C:\\Users\\Yann\\AppData\\Local\\Temp\\pip-build-env-w8w1okbh\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\cmd.py\"\u001b[0m, line \u001b[35m357\u001b[0m, in \u001b[35mrun_command\u001b[0m\n",
      "          \u001b[31mself.distribution.run_command\u001b[0m\u001b[1;31m(command)\u001b[0m\n",
      "          \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^\u001b[0m\n",
      "        File \u001b[35m\"C:\\Users\\Yann\\AppData\\Local\\Temp\\pip-build-env-w8w1okbh\\overlay\\Lib\\site-packages\\setuptools\\dist.py\"\u001b[0m, line \u001b[35m1102\u001b[0m, in \u001b[35mrun_command\u001b[0m\n",
      "          \u001b[31msuper().run_command\u001b[0m\u001b[1;31m(command)\u001b[0m\n",
      "          \u001b[31m~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^\u001b[0m\n",
      "        File \u001b[35m\"C:\\Users\\Yann\\AppData\\Local\\Temp\\pip-build-env-w8w1okbh\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\dist.py\"\u001b[0m, line \u001b[35m1021\u001b[0m, in \u001b[35mrun_command\u001b[0m\n",
      "          \u001b[31mcmd_obj.run\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "          \u001b[31m~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "        File \u001b[35m\"<string>\"\u001b[0m, line \u001b[35m15\u001b[0m, in \u001b[35mrun\u001b[0m\n",
      "      \u001b[1;35mException\u001b[0m: \u001b[35mPlease install cudf via the rapidsai conda channel. See https://rapids.ai/start.html for instructions.\u001b[0m\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for cudf\n",
      "ERROR: Failed to build installable wheels for some pyproject.toml based projects (cudf)\n"
     ]
    }
   ],
   "source": [
    "!pip install cudf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23aaead8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ CuPy non install√© - Mode CPU uniquement\n",
      "üéØ TRAITEMENT DE VOTRE FICHIER 25GB\n",
      "üìÅ Source: ../data/raw/Clothing_Shoes_and_Jewelry.jsonl\n",
      "üìÅ Cible: ../data/raw/Clothing_Shoes_and_Jewelry.parquet\n",
      "üöÄ CONVERSION ULTRA-OPTIMIS√âE RTX 4080 + WINDOWS\n",
      "============================================================\n",
      "üíª CPU Cores: 24\n",
      "üíæ RAM: 63.8 GB\n",
      "‚öôÔ∏è Chunk size: 200,000\n",
      "üßµ Workers: 6\n",
      "üîÑ Analyse du fichier...\n",
      "üìä 66,033,346 lignes (25.90 GB)\n",
      "\n",
      "üîÑ Traitement en cours...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üìä Processing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 66033346/66033346 [1:09:29<00:00]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Traitement du dernier chunk...\n",
      "\n",
      "‚úÖ Parsing termin√©!\n",
      "üìä 66,033,346 enregistrements trait√©s\n",
      "‚ö†Ô∏è 0 erreurs de parsing\n",
      "üìÅ 331 chunks cr√©√©s\n",
      "\n",
      "üîÑ Fusion finale des chunks...\n",
      "üöÄ Fusion de 331 chunks avec PyArrow...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üìñ Loading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 331/331 [01:31<00:00,  3.62 chunks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Concat√©nation...\n",
      "üíæ √âcriture finale...\n",
      "‚ùå Erreur: BYTE_STREAM_SPLIT only supports FLOAT, DOUBLE, INT32, INT64 and FIXED_LEN_BYTE_ARRAY\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Yann\\AppData\\Local\\Temp\\ipykernel_25564\\113660458.py\", line 351, in main\n",
      "    processed, errors = convert_large_jsonl_final(input_file, output_file)\n",
      "                        ~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Yann\\AppData\\Local\\Temp\\ipykernel_25564\\113660458.py\", line 275, in convert_large_jsonl_final\n",
      "    merge_chunks_ultra_fast(chunk_files, output_file)\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Yann\\AppData\\Local\\Temp\\ipykernel_25564\\113660458.py\", line 304, in merge_chunks_ultra_fast\n",
      "    pq.write_table(\n",
      "    ~~~~~~~~~~~~~~^\n",
      "        combined_table,\n",
      "        ^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "        use_byte_stream_split=True  # Compression am√©lior√©e\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Users\\Yann\\Desktop\\DEV\\School\\ml_m1\\NLP\\CustomGPT\\scraper_env\\Lib\\site-packages\\pyarrow\\parquet\\core.py\", line 1909, in write_table\n",
      "    writer.write_table(table, row_group_size=row_group_size)\n",
      "    ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Yann\\Desktop\\DEV\\School\\ml_m1\\NLP\\CustomGPT\\scraper_env\\Lib\\site-packages\\pyarrow\\parquet\\core.py\", line 1115, in write_table\n",
      "    self.writer.write_table(table, row_group_size=row_group_size)\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"pyarrow\\\\_parquet.pyx\", line 2226, in pyarrow._parquet.ParquetWriter.write_table\n",
      "  File \"pyarrow\\\\error.pxi\", line 92, in pyarrow.lib.check_status\n",
      "OSError: BYTE_STREAM_SPLIT only supports FLOAT, DOUBLE, INT32, INT64 and FIXED_LEN_BYTE_ARRAY\n"
     ]
    }
   ],
   "source": [
    "# Version ultra-optimis√©e pour RTX 4080 + Windows (SANS cuDF)\n",
    "import json\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import psutil\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import multiprocessing as mp\n",
    "\n",
    "# CuPy pour GPU (que vous avez d√©j√†)\n",
    "try:\n",
    "    import cupy as cp\n",
    "    GPU_AVAILABLE = cp.cuda.is_available()\n",
    "    if GPU_AVAILABLE:\n",
    "        device = cp.cuda.Device()\n",
    "        print(f\"üöÄ GPU D√âTECT√â: {device.name}\")\n",
    "        print(f\"üíæ VRAM: {device.mem_info[1] / 1024**3:.1f} GB\")\n",
    "        # Limiter l'usage GPU √† 80% pour √©viter les crashes\n",
    "        cp.cuda.MemoryPool().set_limit(int(device.mem_info[1] * 0.8))\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è GPU non disponible\")\n",
    "except ImportError:\n",
    "    GPU_AVAILABLE = False\n",
    "    print(\"üì¶ CuPy non install√© - Mode CPU uniquement\")\n",
    "\n",
    "def get_system_info():\n",
    "    \"\"\"Affiche les infos syst√®me pour optimiser\"\"\"\n",
    "    cpu_count = mp.cpu_count()\n",
    "    ram_gb = psutil.virtual_memory().total / 1024**3\n",
    "    \n",
    "    print(f\"üíª CPU Cores: {cpu_count}\")\n",
    "    print(f\"üíæ RAM: {ram_gb:.1f} GB\")\n",
    "    \n",
    "    if GPU_AVAILABLE:\n",
    "        gpu_memory = cp.cuda.Device().mem_info[1] / 1024**3\n",
    "        print(f\"üéÆ VRAM: {gpu_memory:.1f} GB\")\n",
    "        return cpu_count, ram_gb, gpu_memory\n",
    "    \n",
    "    return cpu_count, ram_gb, 0\n",
    "\n",
    "def calculate_optimal_params():\n",
    "    \"\"\"Calcule les param√®tres optimaux selon votre mat√©riel\"\"\"\n",
    "    cpu_count, ram_gb, gpu_memory = get_system_info()\n",
    "    \n",
    "    # Taille des chunks adaptative\n",
    "    if GPU_AVAILABLE and gpu_memory > 10:\n",
    "        chunk_size = 300000  # RTX 4080 peut g√©rer de gros chunks\n",
    "        workers = min(cpu_count, 8)\n",
    "    elif ram_gb > 16:\n",
    "        chunk_size = 200000\n",
    "        workers = min(cpu_count, 6)\n",
    "    elif ram_gb > 8:\n",
    "        chunk_size = 100000\n",
    "        workers = min(cpu_count, 4)\n",
    "    else:\n",
    "        chunk_size = 50000\n",
    "        workers = 2\n",
    "    \n",
    "    print(f\"‚öôÔ∏è Chunk size: {chunk_size:,}\")\n",
    "    print(f\"üßµ Workers: {workers}\")\n",
    "    \n",
    "    return chunk_size, workers\n",
    "\n",
    "def parse_json_chunk_parallel(lines_chunk, worker_id=0):\n",
    "    \"\"\"Parse JSON en parall√®le avec gestion d'erreurs\"\"\"\n",
    "    parsed_data = []\n",
    "    errors = 0\n",
    "    \n",
    "    for line in lines_chunk:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            try:\n",
    "                parsed_data.append(json.loads(line))\n",
    "            except json.JSONDecodeError:\n",
    "                errors += 1\n",
    "                continue\n",
    "    \n",
    "    return parsed_data, errors\n",
    "\n",
    "def optimize_dataframe_gpu(df):\n",
    "    \"\"\"Optimise le DataFrame avec GPU si disponible\"\"\"\n",
    "    if not GPU_AVAILABLE:\n",
    "        return df\n",
    "    \n",
    "    try:\n",
    "        # Optimisations GPU pour colonnes num√©riques\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        \n",
    "        for col in numeric_cols:\n",
    "            if col in df.columns and not df[col].isna().all():\n",
    "                # Transf√©rer vers GPU\n",
    "                values = df[col].fillna(0).values.astype(np.float32)\n",
    "                gpu_array = cp.asarray(values)\n",
    "                \n",
    "                # Op√©rations GPU rapides\n",
    "                if col == 'user_rating':\n",
    "                    # Normaliser les ratings\n",
    "                    mean_val = cp.mean(gpu_array)\n",
    "                    std_val = cp.std(gpu_array)\n",
    "                    df[f'{col}_normalized'] = cp.asnumpy((gpu_array - mean_val) / (std_val + 1e-8))\n",
    "                \n",
    "                # Nettoyer GPU\n",
    "                del gpu_array\n",
    "        \n",
    "        # Optimisations pour le texte\n",
    "        if 'review_text' in df.columns:\n",
    "            # Calculer longueurs sur GPU\n",
    "            text_lengths = df['review_text'].str.len().fillna(0).values\n",
    "            if len(text_lengths) > 0:\n",
    "                gpu_lengths = cp.asarray(text_lengths)\n",
    "                \n",
    "                # Stats rapides\n",
    "                mean_length = float(cp.mean(gpu_lengths))\n",
    "                max_length = float(cp.max(gpu_lengths))\n",
    "                \n",
    "                df['text_length'] = cp.asnumpy(gpu_lengths)\n",
    "                \n",
    "                del gpu_lengths\n",
    "        \n",
    "        # Forcer le nettoyage GPU\n",
    "        cp.get_default_memory_pool().free_all_blocks()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è GPU optimization failed: {e}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def convert_large_jsonl_final(input_file, output_file, chunk_size=None, num_workers=None):\n",
    "    \"\"\"\n",
    "    VERSION FINALE - Optimis√©e pour RTX 4080 + Windows\n",
    "    SANS cuDF mais avec toutes les autres optimisations\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üöÄ CONVERSION ULTRA-OPTIMIS√âE RTX 4080 + WINDOWS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Param√®tres adaptatifs\n",
    "    if chunk_size is None or num_workers is None:\n",
    "        chunk_size, num_workers = calculate_optimal_params()\n",
    "    \n",
    "    # Cr√©er dossier de sortie\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "    \n",
    "    # Compter les lignes pour la progression\n",
    "    print(\"üîÑ Analyse du fichier...\")\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        total_lines = sum(1 for _ in f)\n",
    "    \n",
    "    file_size_gb = os.path.getsize(input_file) / 1024**3\n",
    "    print(f\"üìä {total_lines:,} lignes ({file_size_gb:.2f} GB)\")\n",
    "    \n",
    "    # Variables de traitement\n",
    "    chunk_files = []\n",
    "    processed_records = 0\n",
    "    total_errors = 0\n",
    "    chunk_num = 0\n",
    "    \n",
    "    # Buffer pour lecture par chunks\n",
    "    lines_buffer = []\n",
    "    \n",
    "    print(f\"\\nüîÑ Traitement en cours...\")\n",
    "    \n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        with tqdm(total=total_lines, desc=\"üìä Processing\", unit=\" lines\", \n",
    "                  bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]\") as pbar:\n",
    "            \n",
    "            for line_num, line in enumerate(f, 1):\n",
    "                pbar.update(1)\n",
    "                lines_buffer.append(line)\n",
    "                \n",
    "                # Traiter quand le buffer est plein\n",
    "                if len(lines_buffer) >= chunk_size:\n",
    "                    \n",
    "                    # Diviser en sous-chunks pour traitement parall√®le\n",
    "                    sub_chunk_size = len(lines_buffer) // num_workers\n",
    "                    if sub_chunk_size == 0:\n",
    "                        sub_chunk_size = len(lines_buffer)\n",
    "                    \n",
    "                    sub_chunks = [\n",
    "                        lines_buffer[i:i + sub_chunk_size]\n",
    "                        for i in range(0, len(lines_buffer), sub_chunk_size)\n",
    "                    ]\n",
    "                    \n",
    "                    # Parse JSON en parall√®le\n",
    "                    all_parsed_data = []\n",
    "                    chunk_errors = 0\n",
    "                    \n",
    "                    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "                        future_to_chunk = {\n",
    "                            executor.submit(parse_json_chunk_parallel, chunk, i): i \n",
    "                            for i, chunk in enumerate(sub_chunks)\n",
    "                        }\n",
    "                        \n",
    "                        for future in future_to_chunk:\n",
    "                            try:\n",
    "                                parsed_data, errors = future.result()\n",
    "                                all_parsed_data.extend(parsed_data)\n",
    "                                chunk_errors += errors\n",
    "                            except Exception as e:\n",
    "                                print(f\"‚ö†Ô∏è Worker error: {e}\")\n",
    "                                chunk_errors += 1\n",
    "                    \n",
    "                    # Cr√©er DataFrame si on a des donn√©es\n",
    "                    if all_parsed_data:\n",
    "                        df = pd.DataFrame(all_parsed_data)\n",
    "                        \n",
    "                        # üöÄ OPTIMISATIONS GPU\n",
    "                        df = optimize_dataframe_gpu(df)\n",
    "                        \n",
    "                        # Sauvegarder le chunk\n",
    "                        chunk_file = f\"{output_file}.chunk_{chunk_num:04d}.parquet\"\n",
    "                        df.to_parquet(\n",
    "                            chunk_file,\n",
    "                            compression='snappy',\n",
    "                            index=False,\n",
    "                            engine='pyarrow',\n",
    "                            use_dictionary=True  # Optimisation suppl√©mentaire\n",
    "                        )\n",
    "                        \n",
    "                        chunk_files.append(chunk_file)\n",
    "                        processed_records += len(all_parsed_data)\n",
    "                        total_errors += chunk_errors\n",
    "                        \n",
    "                        # Nettoyer la m√©moire\n",
    "                        del df, all_parsed_data\n",
    "                        gc.collect()\n",
    "                        \n",
    "                        chunk_num += 1\n",
    "                    \n",
    "                    # Vider le buffer\n",
    "                    lines_buffer = []\n",
    "                    \n",
    "                    # Mise √† jour de la barre de progression\n",
    "                    memory_usage = psutil.Process().memory_info().rss / 1024**3\n",
    "                    \n",
    "                    postfix = {\n",
    "                        'chunks': chunk_num,\n",
    "                        'records': f\"{processed_records:,}\",\n",
    "                        'RAM': f\"{memory_usage:.1f}GB\",\n",
    "                        'errors': total_errors\n",
    "                    }\n",
    "                    \n",
    "                    if GPU_AVAILABLE:\n",
    "                        gpu_memory = cp.get_default_memory_pool().used_bytes() / 1024**3\n",
    "                        postfix['GPU'] = f\"{gpu_memory:.1f}GB\"\n",
    "                    \n",
    "                    pbar.set_postfix(postfix)\n",
    "    \n",
    "    # Traiter le dernier buffer\n",
    "    if lines_buffer:\n",
    "        print(\"üîÑ Traitement du dernier chunk...\")\n",
    "        parsed_data, errors = parse_json_chunk_parallel(lines_buffer)\n",
    "        if parsed_data:\n",
    "            df = pd.DataFrame(parsed_data)\n",
    "            df = optimize_dataframe_gpu(df)\n",
    "            \n",
    "            chunk_file = f\"{output_file}.chunk_{chunk_num:04d}.parquet\"\n",
    "            df.to_parquet(chunk_file, compression='snappy', index=False)\n",
    "            chunk_files.append(chunk_file)\n",
    "            processed_records += len(parsed_data)\n",
    "            total_errors += errors\n",
    "            del df\n",
    "    \n",
    "    print(f\"\\n‚úÖ Parsing termin√©!\")\n",
    "    print(f\"üìä {processed_records:,} enregistrements trait√©s\")\n",
    "    print(f\"‚ö†Ô∏è {total_errors} erreurs de parsing\")\n",
    "    print(f\"üìÅ {len(chunk_files)} chunks cr√©√©s\")\n",
    "    \n",
    "    # üîÑ FUSION FINALE ULTRA-RAPIDE\n",
    "    print(f\"\\nüîÑ Fusion finale des chunks...\")\n",
    "    merge_chunks_ultra_fast(chunk_files, output_file)\n",
    "    \n",
    "    return processed_records, total_errors\n",
    "\n",
    "def merge_chunks_ultra_fast(chunk_files, output_file):\n",
    "    \"\"\"Fusion ultra-rapide avec PyArrow pur - VERSION CORRIG√âE\"\"\"\n",
    "    \n",
    "    print(f\"üöÄ Fusion de {len(chunk_files)} chunks avec PyArrow...\")\n",
    "    \n",
    "    # Lire tous les chunks comme tables PyArrow\n",
    "    tables = []\n",
    "    \n",
    "    for chunk_file in tqdm(chunk_files, desc=\"üìñ Loading\", unit=\" chunks\"):\n",
    "        try:\n",
    "            table = pq.read_table(chunk_file)\n",
    "            tables.append(table)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Erreur lecture {chunk_file}: {e}\")\n",
    "    \n",
    "    if not tables:\n",
    "        print(\"‚ùå Aucun chunk valide trouv√©!\")\n",
    "        return\n",
    "    \n",
    "    # Concat√©nation PyArrow (ultra-rapide)\n",
    "    print(\"üîÑ Concat√©nation...\")\n",
    "    combined_table = pa.concat_tables(tables)\n",
    "    \n",
    "    # √âcriture avec optimisations COMPATIBLES\n",
    "    print(\"üíæ √âcriture finale...\")\n",
    "    try:\n",
    "        # Essayer avec toutes les optimisations\n",
    "        pq.write_table(\n",
    "            combined_table,\n",
    "            output_file,\n",
    "            compression='snappy',\n",
    "            use_dictionary=True,\n",
    "            write_statistics=True,\n",
    "            row_group_size=100000,\n",
    "            # use_byte_stream_split=True  # ‚ùå SUPPRIM√â - cause l'erreur\n",
    "        )\n",
    "        print(\"‚úÖ √âcriture avec optimisations compl√®tes\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Erreur avec optimisations: {e}\")\n",
    "        print(\"üîÑ Fallback vers √©criture basique...\")\n",
    "        \n",
    "        # Fallback vers √©criture simple\n",
    "        try:\n",
    "            pq.write_table(\n",
    "                combined_table,\n",
    "                output_file,\n",
    "                compression='snappy',\n",
    "                use_dictionary=False,  # D√©sactiver si probl√®me\n",
    "                write_statistics=False\n",
    "            )\n",
    "            print(\"‚úÖ √âcriture basique r√©ussie\")\n",
    "        except Exception as e2:\n",
    "            print(f\"‚ùå Erreur critique: {e2}\")\n",
    "            # Dernier recours avec pandas\n",
    "            print(\"üîÑ Dernier recours avec pandas...\")\n",
    "            df = combined_table.to_pandas()\n",
    "            df.to_parquet(output_file, compression='snappy', index=False)\n",
    "            print(\"‚úÖ Sauvegarde pandas r√©ussie\")\n",
    "    \n",
    "    # Nettoyage\n",
    "    print(\"üßπ Nettoyage...\")\n",
    "    for chunk_file in chunk_files:\n",
    "        try:\n",
    "            os.remove(chunk_file)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Lib√©rer m√©moire\n",
    "    del tables, combined_table\n",
    "    gc.collect()\n",
    "    \n",
    "    print(\"‚úÖ Fusion termin√©e!\")\n",
    "\n",
    "# ================================\n",
    "# UTILISATION SIMPLE\n",
    "# ================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Fonction principale - lancez √ßa!\"\"\"\n",
    "    \n",
    "    # Vos fichiers\n",
    "    input_file = \"../data/raw/Clothing_Shoes_and_Jewelry.jsonl\"\n",
    "    output_file = \"../data/raw/Clothing_Shoes_and_Jewelry.parquet\"\n",
    "    \n",
    "    print(\"üéØ TRAITEMENT DE VOTRE FICHIER 25GB\")\n",
    "    print(f\"üìÅ Source: {input_file}\")\n",
    "    print(f\"üìÅ Cible: {output_file}\")\n",
    "    \n",
    "    # V√©rifier que le fichier existe\n",
    "    if not os.path.exists(input_file):\n",
    "        print(f\"‚ùå Fichier non trouv√©: {input_file}\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        processed, errors = convert_large_jsonl_final(input_file, output_file)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        processing_time = end_time - start_time\n",
    "        \n",
    "        # Statistiques finales\n",
    "        if os.path.exists(output_file):\n",
    "            original_size = os.path.getsize(input_file) / (1024**3)\n",
    "            parquet_size = os.path.getsize(output_file) / (1024**3)\n",
    "            compression_ratio = original_size / parquet_size\n",
    "            speed = processed / processing_time\n",
    "            \n",
    "            print(f\"\\nüéâ CONVERSION TERMIN√âE!\")\n",
    "            print(f\"‚è±Ô∏è Temps: {processing_time/60:.1f} minutes\")\n",
    "            print(f\"üìä {processed:,} enregistrements\")\n",
    "            print(f\"‚ö†Ô∏è {errors} erreurs ({errors/processed*100:.2f}%)\")\n",
    "            print(f\"üìÅ {original_size:.2f} GB ‚Üí {parquet_size:.2f} GB\")\n",
    "            print(f\"üóúÔ∏è Compression: {compression_ratio:.1f}x\")\n",
    "            print(f\"üöÄ Vitesse: {speed:,.0f} records/sec\")\n",
    "            \n",
    "            # Test de lecture\n",
    "            print(f\"\\nüîç Test de lecture...\")\n",
    "            sample = pd.read_parquet(output_file).head(5)\n",
    "            print(f\"‚úÖ Lecture OK! Shape: {sample.shape}\")\n",
    "            print(f\"üìã Colonnes: {list(sample.columns)}\")\n",
    "            \n",
    "        else:\n",
    "            print(\"‚ùå Fichier de sortie non cr√©√©\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import time\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30ff1184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Fusion de 56 fichiers en ../data/processed/Clothing_Shoes_and_Jewelry.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üîó Fusion des chunks: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 56/56 [00:13<00:00,  4.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Fusion termin√©e : ../data/processed/Clothing_Shoes_and_Jewelry.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def merge_parquet_chunks(chunk_files, output_file):\n",
    "    \"\"\"\n",
    "    Fusionne une liste de fichiers Parquet en un seul.\n",
    "    \n",
    "    Param√®tres :\n",
    "        chunk_files (list of str): chemins des fichiers Parquet √† fusionner\n",
    "        output_file (str): chemin du fichier Parquet final\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"üîÑ Fusion de {len(chunk_files)} fichiers en {output_file}\")\n",
    "    \n",
    "    # S'assurer que le dossier de sortie existe\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "    \n",
    "    parquet_writer = None\n",
    "    schema = None\n",
    "\n",
    "    for chunk_path in tqdm(chunk_files, desc=\"üîó Fusion des chunks\"):\n",
    "        try:\n",
    "            # Lire chunk\n",
    "            chunk_df = pd.read_parquet(chunk_path)\n",
    "            table = pa.Table.from_pandas(chunk_df)\n",
    "\n",
    "            if parquet_writer is None:\n",
    "                # Initialisation du writer avec le sch√©ma du premier chunk\n",
    "                schema = table.schema\n",
    "                parquet_writer = pq.ParquetWriter(\n",
    "                    output_file,\n",
    "                    schema=schema,\n",
    "                    compression=\"snappy\"\n",
    "                )\n",
    "\n",
    "            # √âcriture du chunk\n",
    "            parquet_writer.write_table(table)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur sur {chunk_path}: {e}\")\n",
    "        finally:\n",
    "            # Lib√©ration m√©moire\n",
    "            del chunk_df, table\n",
    "            gc.collect()\n",
    "    \n",
    "    # Finaliser\n",
    "    if parquet_writer:\n",
    "        parquet_writer.close()\n",
    "\n",
    "    print(f\"\\n‚úÖ Fusion termin√©e : {output_file}\")\n",
    "\n",
    "# =========================\n",
    "# Exemple d'utilisation\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    # Exemple de d√©tection automatique des fichiers chunk√©s\n",
    "    import glob\n",
    "\n",
    "    # Chemin de base (adapter selon votre structure) Clothing_Shoes_and_Jewelry.parquet.chunk_0330.parquet\n",
    "    output_file = \"../data/processed/Clothing_Shoes_and_Jewelry.parquet\"\n",
    "    chunk_pattern = os.path.join(\n",
    "        os.path.dirname(output_file),\n",
    "        \"Clothing_Shoes_and_Jewelry.parquet.chunk_*.parquet\"\n",
    "    )\n",
    "\n",
    "\n",
    "    chunk_files = sorted(glob.glob(chunk_pattern))\n",
    "    \n",
    "    if not chunk_files:\n",
    "        print(\"‚ö†Ô∏è Aucun fichier chunk trouv√©.\")\n",
    "    else:\n",
    "        merge_parquet_chunks(chunk_files, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81d367e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç 331 fichiers trouv√©s avec le motif : ../data/raw\\Clothing_Shoes_and_Jewelry.parquet.chunk_*.parquet\n",
      "üîÑ Fusion de 331 fichiers en ../data/processed/Clothing_Shoes_and_Jewelry.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üîó Fusion des chunks: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 331/331 [04:08<00:00,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Fusion termin√©e : ../data/processed/Clothing_Shoes_and_Jewelry.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import glob\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from tqdm import tqdm\n",
    "\n",
    "def merge_parquet_chunks(chunk_files, output_file):\n",
    "    \"\"\"\n",
    "    Fusionne une liste de fichiers Parquet en un seul.\n",
    "    \n",
    "    Args:\n",
    "        chunk_files (list of str): chemins des fichiers Parquet √† fusionner\n",
    "        output_file (str): chemin du fichier Parquet final\n",
    "    \"\"\"\n",
    "    print(f\"üîÑ Fusion de {len(chunk_files)} fichiers en {output_file}\")\n",
    "    \n",
    "    # Cr√©er le dossier de sortie si n√©cessaire\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "    \n",
    "    parquet_writer = None\n",
    "\n",
    "    for chunk_path in tqdm(chunk_files, desc=\"üîó Fusion des chunks\"):\n",
    "        try:\n",
    "            chunk_df = pd.read_parquet(chunk_path)\n",
    "            table = pa.Table.from_pandas(chunk_df)\n",
    "\n",
    "            if parquet_writer is None:\n",
    "                parquet_writer = pq.ParquetWriter(\n",
    "                    output_file,\n",
    "                    table.schema,\n",
    "                    compression=\"snappy\"\n",
    "                )\n",
    "\n",
    "            parquet_writer.write_table(table)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur avec {chunk_path} : {e}\")\n",
    "        finally:\n",
    "            del chunk_df, table\n",
    "            gc.collect()\n",
    "    \n",
    "    if parquet_writer:\n",
    "        parquet_writer.close()\n",
    "\n",
    "    print(f\"\\n‚úÖ Fusion termin√©e : {output_file}\")\n",
    "\n",
    "# =========================\n",
    "# Exemple d'utilisation\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    # Dossier contenant les fichiers chunk√©s\n",
    "    chunk_dir = \"../data/raw\"  # <- Change ici si n√©cessaire\n",
    "    output_file = \"../data/processed/Clothing_Shoes_and_Jewelry.parquet\"\n",
    "\n",
    "    # Motif pour trouver tous les chunks\n",
    "    chunk_pattern = os.path.join(chunk_dir, \"Clothing_Shoes_and_Jewelry.parquet.chunk_*.parquet\")\n",
    "\n",
    "    # Lire tous les chunks\n",
    "    chunk_files = sorted(glob.glob(chunk_pattern))\n",
    "\n",
    "    print(f\"üîç {len(chunk_files)} fichiers trouv√©s avec le motif : {chunk_pattern}\")\n",
    "    \n",
    "    if not chunk_files:\n",
    "        print(\"‚ö†Ô∏è Aucun fichier chunk trouv√©.\")\n",
    "    else:\n",
    "        merge_parquet_chunks(chunk_files, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169d5177",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Chemin vers le fichier Parquet fusionn√©\n",
    "parquet_file = \"../data/processed/Clothing_Shoes_and_Jewelry.parquet\"\n",
    "\n",
    "# Lire le fichier complet (attention √† la taille en RAM !)\n",
    "df = pd.read_parquet(parquet_file, engine=\"pyarrow\")\n",
    "\n",
    "# Afficher les premi√®res lignes\n",
    "print(\"‚úÖ Aper√ßu du DataFrame fusionn√© :\")\n",
    "print(df.head())\n",
    "\n",
    "# Afficher quelques infos utiles\n",
    "print(\"\\nüìä Infos g√©n√©rales :\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\nüìè Dimensions :\")\n",
    "print(f\"Lignes : {df.shape[0]:,}\")\n",
    "print(f\"Colonnes : {df.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4ed5ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 3,000,000 lignes lues\n",
      "   rating                                    title  \\\n",
      "0     3.0  Arrived Damaged : liquid in hub locker!   \n",
      "1     3.0                Useless under 40 degrees.   \n",
      "2     4.0   Not waterproof, but a very comfy shoe.   \n",
      "3     4.0        Lovely, but QA issues with sewing   \n",
      "4     2.0                                  Just ok   \n",
      "\n",
      "                                                text  \\\n",
      "0  Unfortunately Amazon in their wisdom (cough, c...   \n",
      "1  Useless under 40 degrees unless you‚Äôre just ru...   \n",
      "2  I purchased these bc they are supposed to be w...   \n",
      "3  I‚Äôll start by saying I love this robe!  I trul...   \n",
      "4  Don't be fooled by the description. I was free...   \n",
      "\n",
      "                                              images        asin parent_asin  \\\n",
      "0  [{'attachment_type': 'IMAGE', 'large_image_url...  B096S6LZV4  B09NSZ5QMF   \n",
      "1                                                 []  B09KMDBDCN  B08NGL3X17   \n",
      "2                                                 []  B096N5WK8Q  B07RGM3DYC   \n",
      "3  [{'attachment_type': 'IMAGE', 'large_image_url...  B07JR4QBZ4  B07BWS4CSM   \n",
      "4                                                 []  B09GY958RK  B09GY6SG2C   \n",
      "\n",
      "                        user_id      timestamp  helpful_vote  \\\n",
      "0  AFKZENTNBQ7A7V7UXW5JJI6UGRYQ  1677938767351             0   \n",
      "1  AFKZENTNBQ7A7V7UXW5JJI6UGRYQ  1677083819242             0   \n",
      "2  AFKZENTNBQ7A7V7UXW5JJI6UGRYQ  1675524098918            11   \n",
      "3  AFKZENTNBQ7A7V7UXW5JJI6UGRYQ  1545114577507            26   \n",
      "4  AGGZ357AO26RQZVRLGU4D4N52DZQ  1645223372746             1   \n",
      "\n",
      "   verified_purchase  \n",
      "0               True  \n",
      "1              False  \n",
      "2               True  \n",
      "3               True  \n",
      "4               True  \n",
      "\n",
      "üìä Infos g√©n√©rales :\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3000000 entries, 0 to 2999999\n",
      "Data columns (total 10 columns):\n",
      " #   Column             Dtype  \n",
      "---  ------             -----  \n",
      " 0   rating             float64\n",
      " 1   title              object \n",
      " 2   text               object \n",
      " 3   images             object \n",
      " 4   asin               object \n",
      " 5   parent_asin        object \n",
      " 6   user_id            object \n",
      " 7   timestamp          int64  \n",
      " 8   helpful_vote       int64  \n",
      " 9   verified_purchase  bool   \n",
      "dtypes: bool(1), float64(1), int64(2), object(6)\n",
      "memory usage: 208.9+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.dataset as ds\n",
    "import pandas as pd\n",
    "\n",
    "parquet_file = \"../data/processed/merged/Clothing_Shoes_and_Jewelry.parquet\"\n",
    "dataset = ds.dataset(parquet_file, format=\"parquet\")\n",
    "\n",
    "batch_reader = dataset.to_batches()\n",
    "\n",
    "\n",
    "rows_collected = 0\n",
    "max_rows = 3_000_000\n",
    "frames = []\n",
    "\n",
    "for batch in batch_reader:\n",
    "    batch_df = batch.to_pandas()\n",
    "    batch_len = len(batch_df)\n",
    "    \n",
    "    if rows_collected + batch_len >= max_rows:\n",
    "        # Prendre uniquement le reste\n",
    "        needed = max_rows - rows_collected\n",
    "        frames.append(batch_df.iloc[:needed])\n",
    "        break\n",
    "    else:\n",
    "        frames.append(batch_df)\n",
    "        rows_collected += batch_len\n",
    "\n",
    "df = pd.concat(frames, ignore_index=True)\n",
    "print(f\"‚úÖ {len(df):,} lignes lues\")\n",
    "print(df.head())\n",
    "print(\"\\nüìä Infos g√©n√©rales :\")\n",
    "print(df.info())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3b63c2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         2023-03-04 14:06:07.351\n",
       "1         2023-02-22 16:36:59.242\n",
       "2         2023-02-04 15:21:38.918\n",
       "3         2018-12-18 06:29:37.507\n",
       "4         2022-02-18 22:29:32.746\n",
       "                    ...          \n",
       "2999995   2022-09-01 18:22:35.935\n",
       "2999996   2020-12-31 04:24:51.163\n",
       "2999997   2020-08-11 23:57:21.001\n",
       "2999998   2020-07-31 18:10:50.754\n",
       "2999999   2018-01-15 19:04:41.829\n",
       "Name: date, Length: 3000000, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df[\"date\"] = pd.to_datetime(df[\"timestamp\"], unit=\"ms\")\n",
    "df[\"date\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdc01fc",
   "metadata": {},
   "source": [
    "#### Conversion timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6d5aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Lecture du dataset parquet en batchs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üß™ Traitement batchs: 661it [03:38,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ 661 chunks trait√©s. Fusion en un seul fichier final...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üîó Fusion finale:   2%|‚ñè         | 16/661 [00:05<03:23,  3.18it/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.dataset as ds\n",
    "import pyarrow.parquet as pq\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_parquet_in_batches(input_file, output_file, batch_size=100_000):\n",
    "    \"\"\"\n",
    "    Lit un gros fichier Parquet par batchs, ajoute une colonne date convertie depuis timestamp,\n",
    "    sauvegarde chaque batch en chunk parquet, puis fusionne tous les chunks.\n",
    "    \"\"\"\n",
    "    # Dossiers de travail\n",
    "    output_dir = os.path.dirname(output_file)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    temp_chunks_dir = os.path.join(output_dir, \"tmp_chunks\")\n",
    "    os.makedirs(temp_chunks_dir, exist_ok=True)\n",
    "\n",
    "    print(\"üì• Lecture du dataset parquet en batchs...\")\n",
    "    dataset = ds.dataset(input_file, format=\"parquet\")\n",
    "    batch_reader = dataset.to_batches(batch_size=batch_size)\n",
    "\n",
    "    chunk_paths = []\n",
    "    total_rows = 0\n",
    "\n",
    "    for i, batch in enumerate(tqdm(batch_reader, desc=\"üß™ Traitement batchs\")):\n",
    "        df = batch.to_pandas()\n",
    "\n",
    "        # Ajout de la colonne date (√† partir de timestamp)\n",
    "        df[\"date\"] = pd.to_datetime(df[\"timestamp\"], unit=\"ms\")\n",
    "\n",
    "        # Sauvegarde du batch trait√©\n",
    "        chunk_path = os.path.join(temp_chunks_dir, f\"processed_chunk_{i:04d}.parquet\")\n",
    "        df.to_parquet(chunk_path, index=False, engine=\"pyarrow\", compression=\"snappy\")\n",
    "        chunk_paths.append(chunk_path)\n",
    "\n",
    "        total_rows += len(df)\n",
    "\n",
    "        # Nettoyage m√©moire\n",
    "        del df, batch\n",
    "        gc.collect()\n",
    "\n",
    "    print(f\"\\n‚úÖ {len(chunk_paths)} chunks trait√©s. Fusion en un seul fichier final...\")\n",
    "\n",
    "    # Fusion des chunks\n",
    "    parquet_writer = None\n",
    "    for chunk_file in tqdm(chunk_paths, desc=\"üîó Fusion finale\"):\n",
    "        chunk_df = pd.read_parquet(chunk_file)\n",
    "        table = pa.Table.from_pandas(chunk_df)\n",
    "\n",
    "        if parquet_writer is None:\n",
    "            parquet_writer = pq.ParquetWriter(output_file, table.schema, compression=\"snappy\")\n",
    "\n",
    "        parquet_writer.write_table(table)\n",
    "\n",
    "        # Nettoyage\n",
    "        del chunk_df, table\n",
    "        gc.collect()\n",
    "\n",
    "    if parquet_writer:\n",
    "        parquet_writer.close()\n",
    "\n",
    "    print(f\"\\nüéâ Fichier final trait√© enregistr√© √† : {output_file}\")\n",
    "    print(f\"üìä Lignes totales : {total_rows:,}\")\n",
    "    print(\"üßπ Suppression des fichiers temporaires...\")\n",
    "    \n",
    "    # Suppression des chunks temporaires\n",
    "    for f in chunk_paths:\n",
    "        try:\n",
    "            os.remove(f)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Erreur suppression {f}: {e}\")\n",
    "    os.rmdir(temp_chunks_dir)\n",
    "\n",
    "# ===============================\n",
    "# Exemple d'utilisation\n",
    "# ===============================\n",
    "if __name__ == \"__main__\":\n",
    "    input_parquet = \"../data/processed/merged/Clothing_Shoes_and_Jewelry.parquet\"\n",
    "    output_parquet = \"../data/processed/final/Clothing_Shoes_and_Jewelry_timestamped.parquet\"\n",
    "\n",
    "    process_parquet_in_batches(input_parquet, output_parquet, batch_size=100_000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738b622d",
   "metadata": {},
   "source": [
    "#### partitionnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ef9b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import traceback\n",
    "\n",
    "# === PARAM√àTRES ===\n",
    "input_file = \"../data/processed/final/Clothing_Shoes_and_Jewelry_processed.parquet\"\n",
    "partition_dir = \"../data/processed/partitions\"\n",
    "partition_size = 6_000_000  # lignes par partition\n",
    "overwrite_existing = False  # Mettre √† True pour r√©√©craser les partitions existantes\n",
    "\n",
    "# === CR√âATION DU DOSSIER DE SORTIE ===\n",
    "try:\n",
    "    os.makedirs(partition_dir, exist_ok=True)\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Impossible de cr√©er le dossier {partition_dir} : {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# === CHARGEMENT DU DATAFRAME ===\n",
    "try:\n",
    "    print(\"üì• Chargement du DataFrame depuis le Parquet...\")\n",
    "    df = pd.read_parquet(input_file, engine=\"pyarrow\")\n",
    "    total_rows = len(df)\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå Fichier introuvable : {input_file}\")\n",
    "    sys.exit(1)\n",
    "except Exception:\n",
    "    print(\"‚ùå Erreur inattendue lors du chargement du Parquet :\")\n",
    "    traceback.print_exc()\n",
    "    sys.exit(1)\n",
    "\n",
    "# === CALCUL DES PARTITIONS ===\n",
    "num_partitions = math.ceil(total_rows / partition_size)\n",
    "print(f\"üìä Total lignes: {total_rows:,} ‚Üí {num_partitions} partitions de {partition_size:,} lignes\")\n",
    "\n",
    "# === TRAITEMENT PAR PARTITION ===\n",
    "for i in range(num_partitions):\n",
    "    start_idx = i * partition_size\n",
    "    end_idx = min(start_idx + partition_size, total_rows)\n",
    "    part_path = os.path.join(partition_dir, f\"partition_{i:02d}.parquet\")\n",
    "    \n",
    "    if os.path.exists(part_path) and not overwrite_existing:\n",
    "        print(f\"‚ö†Ô∏è Partition d√©j√† existante, ignor√©e : {part_path}\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        df_part = df.iloc[start_idx:end_idx]\n",
    "        df_part.to_parquet(part_path, index=False, compression=\"snappy\")\n",
    "        print(f\"‚úÖ Partition {i+1}/{num_partitions} sauvegard√©e : {part_path} ({len(df_part):,} lignes)\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur lors de l‚Äô√©criture de la partition {i} : {e}\")\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        del df_part\n",
    "\n",
    "print(\"\\nüéâ Partitionnement termin√© avec succ√®s.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scraper_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
