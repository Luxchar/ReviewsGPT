{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation\n",
    "\n",
    "L'objectif dans cette partie, est d'√† partir les √©motions d√©tect√©es (28), g√©nerer le texte de support client ad√©quat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline par r√®gles ‚Äî version 28 √©motions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On cr√©e un dictionnaire avec un template de r√©ponse pour chaque √©motion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule_based_generation_28(emotion, user_input):\n",
    "    templates = {\n",
    "        \"admiration\": \"Merci pour vos compliments, cela nous touche beaucoup !\",\n",
    "        \"amusement\": \"Content que cela vous amuse ! On aime garder une ambiance d√©tendue üòä\",\n",
    "        \"anger\": \"Nous sommes sinc√®rement d√©sol√©s. Nous allons tout faire pour r√©gler cela rapidement.\",\n",
    "        \"annoyance\": \"Merci de nous le signaler, nous allons corriger cela au plus vite.\",\n",
    "        \"approval\": \"Merci pour votre validation, c‚Äôest tr√®s appr√©ci√© !\",\n",
    "        \"caring\": \"Merci pour votre bienveillance, cela nous fait chaud au c≈ìur.\",\n",
    "        \"confusion\": \"Laissez-moi vous √©claircir cela en quelques points.\",\n",
    "        \"curiosity\": \"Bonne question ! Je vais vous apporter tous les d√©tails.\",\n",
    "        \"desire\": \"Nous comprenons parfaitement votre int√©r√™t. Voici comment proc√©der.\",\n",
    "        \"disappointment\": \"Je suis navr√© que cela ne vous ait pas satisfait. Nous allons faire mieux.\",\n",
    "        \"disapproval\": \"Merci pour votre retour. Nous allons examiner cela tr√®s attentivement.\",\n",
    "        \"disgust\": \"Nous nous excusons sinc√®rement. Ce n‚Äôest pas l‚Äôexp√©rience que nous voulons offrir.\",\n",
    "        \"embarrassment\": \"Ne vous inqui√©tez pas, cela arrive √† tout le monde. Nous sommes l√† pour vous aider.\",\n",
    "        \"excitement\": \"C‚Äôest super de vous sentir aussi enthousiaste ! Voici la suite.\",\n",
    "        \"fear\": \"Pas d‚Äôinqui√©tude, nous allons vous accompagner √©tape par √©tape.\",\n",
    "        \"gratitude\": \"Avec plaisir ! Merci √† vous pour votre message chaleureux.\",\n",
    "        \"grief\": \"Nous sommes de tout c≈ìur avec vous dans cette p√©riode difficile.\",\n",
    "        \"joy\": \"Ravis de voir que tout se passe bien pour vous !\",\n",
    "        \"love\": \"Votre fid√©lit√© et votre gentillesse nous touchent profond√©ment ‚ù§Ô∏è\",\n",
    "        \"nervousness\": \"Pas de panique, on s‚Äôen occupe pour vous.\",\n",
    "        \"optimism\": \"On adore votre √©nergie positive ! On va assurer derri√®re.\",\n",
    "        \"pride\": \"Vous avez de quoi √™tre fier ! Bravo √† vous.\",\n",
    "        \"realization\": \"Merci pour cette prise de conscience. On est avec vous.\",\n",
    "        \"relief\": \"Heureux d‚Äôavoir pu vous soulager. On reste disponibles.\",\n",
    "        \"remorse\": \"Merci pour vos excuses. Nous allons repartir sur de bonnes bases.\",\n",
    "        \"sadness\": \"Nous comprenons votre peine et nous sommes l√† pour vous.\",\n",
    "        \"surprise\": \"C‚Äôest surprenant en effet ! Je vous explique cela tout de suite.\",\n",
    "        \"sympathy\": \"Merci pour votre compassion, cela compte √©norm√©ment.\"\n",
    "    }\n",
    "\n",
    "    return templates.get(emotion, \"Merci pour votre message.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning (Seq2Seq LSTM) ‚Äî avec 28 √©motions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/emotion_datasets/emo_reviews.csv'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Liste des 28 √©motions de GoEmotions\n",
    "emotions = [\n",
    "    \"admiration\", \"amusement\", \"anger\", \"annoyance\", \"approval\", \"caring\", \"confusion\", \"curiosity\",\n",
    "    \"desire\", \"disappointment\", \"disapproval\", \"disgust\", \"embarrassment\", \"excitement\", \"fear\",\n",
    "    \"gratitude\", \"grief\", \"joy\", \"love\", \"nervousness\", \"optimism\", \"pride\", \"realization\",\n",
    "    \"relief\", \"remorse\", \"sadness\", \"surprise\", \"sympathy\"\n",
    "]\n",
    "\n",
    "# G√©n√©rateurs de reviews par √©motion\n",
    "review_templates = {\n",
    "    \"admiration\": [\n",
    "        \"Je suis impressionn√© par la qualit√© de votre service.\",\n",
    "        \"Vraiment admirable, merci pour tout.\",\n",
    "        \"C‚Äôest rare de voir un tel professionnalisme.\",\n",
    "    ],\n",
    "    \"amusement\": [\n",
    "        \"Votre message m'a bien fait rire.\",\n",
    "        \"Ce bug √©tait presque dr√¥le.\",\n",
    "        \"Une exp√©rience inattendue mais amusante.\",\n",
    "    ],\n",
    "    \"anger\": [\n",
    "        \"Je suis furieux contre votre service.\",\n",
    "        \"C'est inacceptable ce que vous avez fait.\",\n",
    "        \"Je veux un remboursement imm√©diat.\",\n",
    "    ],\n",
    "    \"annoyance\": [\n",
    "        \"C'est aga√ßant d'attendre si longtemps.\",\n",
    "        \"Toujours des erreurs, c'est p√©nible.\",\n",
    "        \"Je commence √† perdre patience.\",\n",
    "    ],\n",
    "    \"approval\": [\n",
    "        \"Je valide compl√®tement ce que vous proposez.\",\n",
    "        \"C‚Äôest une bonne d√©cision.\",\n",
    "        \"Vous avez fait du bon travail.\",\n",
    "    ],\n",
    "    \"caring\": [\n",
    "        \"Je m‚Äôinqui√®te vraiment pour les autres clients.\",\n",
    "        \"Prenez soin de vos utilisateurs svp.\",\n",
    "        \"J‚Äôesp√®re que vous traitez bien vos employ√©s.\",\n",
    "    ],\n",
    "    \"confusion\": [\n",
    "        \"Je ne comprends rien √† votre interface.\",\n",
    "        \"C‚Äôest trop compliqu√© pour moi.\",\n",
    "        \"Je suis compl√®tement perdu.\",\n",
    "    ],\n",
    "    \"curiosity\": [\n",
    "        \"Comment fonctionne votre syst√®me ?\",\n",
    "        \"Je me demande ce qui se passe en arri√®re-plan.\",\n",
    "        \"Pouvez-vous m'expliquer le processus ?\",\n",
    "    ],\n",
    "    \"desire\": [\n",
    "        \"J‚Äôaimerais vraiment avoir cette fonctionnalit√©.\",\n",
    "        \"J‚Äôattends avec impatience la nouvelle version.\",\n",
    "        \"J‚Äôaimerais que cela fonctionne sur mobile.\",\n",
    "    ],\n",
    "    \"disappointment\": [\n",
    "        \"Je suis tr√®s d√©√ßu de votre produit.\",\n",
    "        \"√áa ne r√©pond pas √† mes attentes.\",\n",
    "        \"Je m‚Äôattendais √† mieux.\",\n",
    "    ],\n",
    "    \"disapproval\": [\n",
    "        \"Je d√©sapprouve totalement cette politique.\",\n",
    "        \"C‚Äôest une mauvaise direction que vous prenez.\",\n",
    "        \"Je ne suis pas d‚Äôaccord avec cette d√©cision.\",\n",
    "    ],\n",
    "    \"disgust\": [\n",
    "        \"Je suis √©coeur√© par votre r√©ponse.\",\n",
    "        \"C‚Äôest r√©pugnant ce que j‚Äôai vu.\",\n",
    "        \"Votre comportement est inacceptable.\",\n",
    "    ],\n",
    "    \"embarrassment\": [\n",
    "        \"J‚Äôai honte de ce message envoy√© par erreur.\",\n",
    "        \"Je me sens b√™te d‚Äôavoir mal compris.\",\n",
    "        \"C‚Äô√©tait embarrassant pour moi.\",\n",
    "    ],\n",
    "    \"excitement\": [\n",
    "        \"Je suis super excit√© par ce nouveau produit !\",\n",
    "        \"Trop h√¢te de tester la prochaine mise √† jour.\",\n",
    "        \"C‚Äôest exactement ce que j‚Äôattendais !\",\n",
    "    ],\n",
    "    \"fear\": [\n",
    "        \"J‚Äôai peur de perdre mes donn√©es.\",\n",
    "        \"Et si mon compte √©tait pirat√© ?\",\n",
    "        \"Je suis inquiet pour ma s√©curit√©.\",\n",
    "    ],\n",
    "    \"gratitude\": [\n",
    "        \"Merci infiniment pour votre aide.\",\n",
    "        \"Je vous remercie pour votre r√©activit√©.\",\n",
    "        \"C‚Äô√©tait tr√®s professionnel, merci.\",\n",
    "    ],\n",
    "    \"grief\": [\n",
    "        \"Je suis en deuil et j'ai du mal √† g√©rer √ßa.\",\n",
    "        \"C‚Äôest une p√©riode tr√®s difficile pour moi.\",\n",
    "        \"Je pleure encore cette perte.\",\n",
    "    ],\n",
    "    \"joy\": [\n",
    "        \"Je suis tr√®s heureux de cette exp√©rience.\",\n",
    "        \"Tout s‚Äôest super bien pass√©, merci !\",\n",
    "        \"C‚Äôest g√©nial ce que vous avez fait.\",\n",
    "    ],\n",
    "    \"love\": [\n",
    "        \"J‚Äôadore votre marque.\",\n",
    "        \"Je suis fan depuis le d√©but.\",\n",
    "        \"C‚Äôest l‚Äôamour fou avec ce service.\",\n",
    "    ],\n",
    "    \"nervousness\": [\n",
    "        \"Je suis stress√© √† l‚Äôid√©e d‚Äôutiliser ce produit.\",\n",
    "        \"Je tremble en envoyant ce message.\",\n",
    "        \"J‚Äôesp√®re que tout se passera bien.\",\n",
    "    ],\n",
    "    \"optimism\": [\n",
    "        \"Je suis confiant pour la suite.\",\n",
    "        \"Je pense que vous allez r√©ussir.\",\n",
    "        \"Il y a de l‚Äôespoir, je le sens.\",\n",
    "    ],\n",
    "    \"pride\": [\n",
    "        \"Je suis fier d‚Äôavoir choisi votre service.\",\n",
    "        \"Bravo √† moi d‚Äôavoir trouv√© cette p√©pite.\",\n",
    "        \"Je suis satisfait de ma d√©cision.\",\n",
    "    ],\n",
    "    \"realization\": [\n",
    "        \"Je viens de comprendre pourquoi √ßa ne marchait pas.\",\n",
    "        \"En fait, c‚Äô√©tait de ma faute.\",\n",
    "        \"Je r√©alise maintenant l‚Äôerreur que j‚Äôai faite.\",\n",
    "    ],\n",
    "    \"relief\": [\n",
    "        \"Ouf, tout est rentr√© dans l‚Äôordre.\",\n",
    "        \"Heureux que ce soit enfin r√©gl√©.\",\n",
    "        \"Quel soulagement d‚Äôavoir fini.\",\n",
    "    ],\n",
    "    \"remorse\": [\n",
    "        \"Je regrette d‚Äôavoir √©t√© si dur dans mon pr√©c√©dent message.\",\n",
    "        \"Je suis d√©sol√© de m‚Äô√™tre emport√©.\",\n",
    "        \"J‚Äôai mal agi et je le reconnais.\",\n",
    "    ],\n",
    "    \"sadness\": [\n",
    "        \"Je suis d√©√ßu et triste de cette issue.\",\n",
    "        \"C‚Äôest vraiment dommage.\",\n",
    "        \"Je m‚Äôattendais √† mieux et je suis triste.\",\n",
    "    ],\n",
    "    \"surprise\": [\n",
    "        \"Wow, je ne m‚Äôattendais pas √† √ßa !\",\n",
    "        \"C‚Äôest une belle surprise.\",\n",
    "        \"Incroyable retournement de situation.\",\n",
    "    ],\n",
    "    \"sympathy\": [\n",
    "        \"Je compatis avec votre √©quipe en ces temps difficiles.\",\n",
    "        \"Je comprends que ce ne soit pas facile.\",\n",
    "        \"Courage √† vous tous.\",\n",
    "    ]\n",
    "}\n",
    "\n",
    "# G√©n√©rer 100 reviews par √©motion\n",
    "generated_data = []\n",
    "for emotion in emotions:\n",
    "    templates = review_templates.get(emotion, [f\"Exemple de review exprimant {emotion.lower()}\"])\n",
    "    for i in range(1500):\n",
    "        review = random.choice(templates)\n",
    "        generated_data.append({\n",
    "            \"emotion\": emotion,\n",
    "            \"text_input\": review,\n",
    "            \"text_output\": f\"Merci pour votre message concernant {emotion}. Nous allons vous r√©pondre au mieux.\"\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(generated_data)\n",
    "output_path = \"../data/emotion_datasets/emo_reviews.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "output_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\yann\\anaconda3\\envs\\hf_gpu_env\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\yann\\anaconda3\\envs\\hf_gpu_env\\lib\\site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\yann\\anaconda3\\envs\\hf_gpu_env\\lib\\site-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\yann\\anaconda3\\envs\\hf_gpu_env\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\yann\\anaconda3\\envs\\hf_gpu_env\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\yann\\anaconda3\\envs\\hf_gpu_env\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Yann/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.1783\n",
      "Epoch 2/10, Loss: 0.0010\n",
      "Epoch 3/10, Loss: 0.0003\n",
      "Epoch 4/10, Loss: 0.0001\n",
      "Epoch 5/10, Loss: 0.0001\n",
      "Epoch 6/10, Loss: 0.0000\n",
      "Epoch 7/10, Loss: 0.0000\n",
      "Epoch 8/10, Loss: 0.0000\n",
      "Epoch 9/10, Loss: 0.0000\n",
      "Epoch 10/10, Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pickle\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# === 1. Chargement des donn√©es ===\n",
    "df = pd.read_csv(\"../data/emotion_datasets/emo_reviews.csv\")\n",
    "df[\"input_seq\"] = df[\"emotion\"] + \" [SEP] \" + df[\"text_input\"]\n",
    "df[\"output_seq\"] = \"<start> \" + df[\"text_output\"] + \" <end>\"\n",
    "\n",
    "# === 2. Construction des vocabulaires ===\n",
    "def build_vocab(sentences, min_freq=1):\n",
    "    counter = Counter()\n",
    "    for sentence in sentences:\n",
    "        counter.update(word_tokenize(sentence.lower()))\n",
    "    vocab = {\"<pad>\": 0, \"<unk>\": 1}\n",
    "    for word, freq in counter.items():\n",
    "        if freq >= min_freq:\n",
    "            vocab[word] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "input_vocab = build_vocab(df[\"input_seq\"])\n",
    "output_vocab = build_vocab(df[\"output_seq\"])\n",
    "\n",
    "def encode_sentence(sentence, vocab):\n",
    "    return [vocab.get(tok, vocab[\"<unk>\"]) for tok in word_tokenize(sentence.lower())]\n",
    "\n",
    "# === 3. Tokenisation & encodage ===\n",
    "MAX_LEN = 30\n",
    "\n",
    "X = [torch.tensor(encode_sentence(s, input_vocab))[:MAX_LEN] for s in df[\"input_seq\"]]\n",
    "y = [torch.tensor(encode_sentence(s, output_vocab))[:MAX_LEN] for s in df[\"output_seq\"]]\n",
    "\n",
    "X_pad = pad_sequence(X, batch_first=True, padding_value=0)\n",
    "y_pad = pad_sequence(y, batch_first=True, padding_value=0)\n",
    "\n",
    "# === 4. Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pad, y_pad, test_size=0.1, random_state=42)\n",
    "\n",
    "# === 5. Dataset PyTorch\n",
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, inputs, targets):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.targets[idx]\n",
    "\n",
    "train_dataset = EmotionDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# === 6. Mod√®le Seq2Seq\n",
    "class Seq2SeqModel(nn.Module):\n",
    "    def __init__(self, input_vocab_size, output_vocab_size, embedding_dim=128, hidden_dim=256):\n",
    "        super(Seq2SeqModel, self).__init__()\n",
    "        self.embedding_enc = nn.Embedding(input_vocab_size, embedding_dim)\n",
    "        self.embedding_dec = nn.Embedding(output_vocab_size, embedding_dim)\n",
    "        self.encoder = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.decoder = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        embedded_src = self.embedding_enc(src)\n",
    "        _, (h, c) = self.encoder(embedded_src)\n",
    "\n",
    "        embedded_tgt = self.embedding_dec(tgt)\n",
    "        output, _ = self.decoder(embedded_tgt, (h, c))\n",
    "        logits = self.fc(output)\n",
    "        return logits\n",
    "\n",
    "# === 7. Entra√Ænement\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Seq2SeqModel(len(input_vocab), len(output_vocab)).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "EPOCHS = 10\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        decoder_input = batch_y[:, :-1]\n",
    "        target = batch_y[:, 1:]\n",
    "\n",
    "        output = model(batch_x, decoder_input)\n",
    "        output = output.reshape(-1, output.shape[2])\n",
    "        target = target.reshape(-1)\n",
    "\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {epoch_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# === 8. Sauvegarde\n",
    "torch.save(model.state_dict(), \"emotion_seq2seq_lstm_pytorch.pt\")\n",
    "with open(\"input_vocab.pkl\", \"wb\") as f:\n",
    "    pickle.dump(input_vocab, f)\n",
    "with open(\"output_vocab.pkl\", \"wb\") as f:\n",
    "    pickle.dump(output_vocab, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Fonction de padding PyTorch custom\n",
    "def pad_input(input_seq, max_len=30):\n",
    "    if len(input_seq[0]) < max_len:\n",
    "        padding = [0] * (max_len - len(input_seq[0]))\n",
    "        input_seq[0].extend(padding)\n",
    "    else:\n",
    "        input_seq[0] = input_seq[0][:max_len]\n",
    "    return input_seq\n",
    "\n",
    "# Fonction Greedy\n",
    "def generate_response_greedy(model, input_text, input_tokenizer, output_tokenizer, max_len=30):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    input_seq = input_tokenizer.texts_to_sequences([input_text])\n",
    "    input_seq = pad_input(input_seq, max_len)\n",
    "    input_tensor = torch.LongTensor(input_seq).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        enc_emb = model.embedding_enc(input_tensor)\n",
    "        _, (h, c) = model.encoder(enc_emb)\n",
    "\n",
    "        start_token = output_tokenizer.word_index['<start>']\n",
    "        end_token = output_tokenizer.word_index['<end>']\n",
    "        current_token = torch.LongTensor([[start_token]]).to(device)\n",
    "\n",
    "        generated_tokens = []\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            dec_emb = model.embedding_dec(current_token)\n",
    "            output, (h, c) = model.decoder(dec_emb, (h, c))\n",
    "            logits = model.fc(output[:, -1, :])\n",
    "            predicted_token = torch.argmax(logits, dim=-1)\n",
    "            predicted_id = predicted_token.item()\n",
    "\n",
    "            if predicted_id == end_token:\n",
    "                break\n",
    "\n",
    "            generated_tokens.append(predicted_id)\n",
    "            current_token = predicted_token.unsqueeze(0)\n",
    "\n",
    "    inv_vocab = {v: k for k, v in output_tokenizer.word_index.items()}\n",
    "    decoded_words = [inv_vocab.get(tok, '') for tok in generated_tokens]\n",
    "    return ' '.join(decoded_words)\n",
    "\n",
    "# Fonction Beam Search\n",
    "def generate_response_beam(model, input_text, input_tokenizer, output_tokenizer, max_len=30, beam_width=3):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    input_seq = input_tokenizer.texts_to_sequences([input_text])\n",
    "    input_seq = pad_input(input_seq, max_len)\n",
    "    input_tensor = torch.LongTensor(input_seq).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        enc_emb = model.embedding_enc(input_tensor)\n",
    "        _, (h, c) = model.encoder(enc_emb)\n",
    "\n",
    "        start_token = output_tokenizer.word_index['<start>']\n",
    "        end_token = output_tokenizer.word_index['<end>']\n",
    "\n",
    "        sequences = [[list(), 0.0, h, c]]\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            all_candidates = []\n",
    "            for seq, score, h, c in sequences:\n",
    "                if seq and seq[-1] == end_token:\n",
    "                    all_candidates.append((seq, score, h, c))\n",
    "                    continue\n",
    "                current_token = torch.LongTensor([[seq[-1]]] if seq else [[start_token]]).to(device)\n",
    "                dec_emb = model.embedding_dec(current_token)\n",
    "                output, (h_new, c_new) = model.decoder(dec_emb, (h, c))\n",
    "                logits = model.fc(output[:, -1, :])\n",
    "                probs = F.log_softmax(logits, dim=-1)\n",
    "                topk_probs, topk_idxs = torch.topk(probs, beam_width)\n",
    "\n",
    "                for i in range(beam_width):\n",
    "                    candidate = seq + [topk_idxs[0, i].item()]\n",
    "                    candidate_score = score + topk_probs[0, i].item()\n",
    "                    all_candidates.append((candidate, candidate_score, h_new, c_new))\n",
    "\n",
    "            sequences = sorted(all_candidates, key=lambda tup: tup[1], reverse=True)[:beam_width]\n",
    "\n",
    "        best_seq = sequences[0][0]\n",
    "        inv_vocab = {v: k for k, v in output_tokenizer.word_index.items()}\n",
    "        decoded_words = [inv_vocab.get(tok, '') for tok in best_seq if tok != end_token]\n",
    "        return ' '.join(decoded_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge\n",
      "  Downloading rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: six in c:\\users\\yann\\anaconda3\\envs\\hf_gpu_env\\lib\\site-packages (from rouge) (1.17.0)\n",
      "Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
      "Installing collected packages: rouge\n",
      "Successfully installed rouge-1.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge import Rouge\n",
    "import math\n",
    "\n",
    "# BLEU\n",
    "def compute_bleu(reference, generated):\n",
    "    ref_tokens = reference.lower().split()\n",
    "    gen_tokens = generated.lower().split()\n",
    "    return sentence_bleu([ref_tokens], gen_tokens)\n",
    "\n",
    "# ROUGE\n",
    "rouge = Rouge()\n",
    "def compute_rouge(reference, generated):\n",
    "    return rouge.get_scores(generated, reference)[0]\n",
    "\n",
    "# Perplexit√© (approxim√©e)\n",
    "def compute_perplexity(model, input_seq, target_seq):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    input_tensor = torch.LongTensor([input_seq]).to(device)\n",
    "    target_tensor = torch.LongTensor([target_seq]).to(device)\n",
    "\n",
    "    decoder_input = target_tensor[:, :-1]\n",
    "    target_output = target_tensor[:, 1:]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_tensor, decoder_input)\n",
    "        logits = logits.view(-1, logits.shape[-1])\n",
    "        target_output = target_output.view(-1)\n",
    "        loss = F.cross_entropy(logits, target_output, ignore_index=0)\n",
    "\n",
    "    return math.exp(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: anger [SEP] Ce service est scandaleux !\n",
      "Reference output: Je comprends votre frustration. Nous allons vous aider au plus vite.\n",
      "Input vocab: {'<pad>': 0, '<unk>': 1, 'anger': 2, '[sep]': 3, 'ce': 4, 'service': 5, 'est': 6, 'scandaleux': 7}\n",
      "Output vocab: {'<pad>': 0, '<unk>': 1, '<start>': 2, '<end>': 3, 'je': 4, 'comprends': 5, 'votre': 6, 'frustration': 7}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge import Rouge\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "# Fonction de padding PyTorch custom corrig√©e\n",
    "def pad_input(input_seq, max_len=30):\n",
    "    if len(input_seq) < max_len:\n",
    "        padding = [0] * (max_len - len(input_seq))\n",
    "        input_seq.extend(padding)\n",
    "    else:\n",
    "        input_seq = input_seq[:max_len]\n",
    "    return input_seq\n",
    "\n",
    "# Fonction d'encodage pour utiliser avec nos vocabulaires\n",
    "def encode_sentence(sentence, vocab):\n",
    "    return [vocab.get(tok, vocab[\"<unk>\"]) for tok in word_tokenize(sentence.lower())]\n",
    "\n",
    "# Fonction Greedy corrig√©e\n",
    "def generate_response_greedy(model, input_text, input_vocab, output_vocab, max_len=30):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    # Encodage avec notre vocabulaire personnalis√©\n",
    "    input_seq = encode_sentence(input_text, input_vocab)\n",
    "    input_seq = pad_input(input_seq, max_len)\n",
    "    input_tensor = torch.LongTensor([input_seq]).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        enc_emb = model.embedding_enc(input_tensor)\n",
    "        _, (h, c) = model.encoder(enc_emb)\n",
    "\n",
    "        # Utilisation des tokens sp√©ciaux de notre vocabulaire\n",
    "        start_token = output_vocab.get('<start>', 0)\n",
    "        end_token = output_vocab.get('<end>', 1)\n",
    "        current_token = torch.LongTensor([[start_token]]).to(device)\n",
    "\n",
    "        generated_tokens = []\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            dec_emb = model.embedding_dec(current_token)\n",
    "            output, (h, c) = model.decoder(dec_emb, (h, c))\n",
    "            logits = model.fc(output[:, -1, :])\n",
    "            predicted_token = torch.argmax(logits, dim=-1)\n",
    "            predicted_id = predicted_token.item()\n",
    "\n",
    "            if predicted_id == end_token:\n",
    "                break\n",
    "\n",
    "            generated_tokens.append(predicted_id)\n",
    "            current_token = predicted_token.unsqueeze(0)\n",
    "\n",
    "    # D√©codage avec notre vocabulaire\n",
    "    inv_vocab = {v: k for k, v in output_vocab.items()}\n",
    "    decoded_words = [inv_vocab.get(tok, '') for tok in generated_tokens]\n",
    "    return ' '.join(decoded_words)\n",
    "\n",
    "# Fonction Beam Search corrig√©e\n",
    "def generate_response_beam(model, input_text, input_vocab, output_vocab, max_len=30, beam_width=3):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    # Encodage avec notre vocabulaire personnalis√©\n",
    "    input_seq = encode_sentence(input_text, input_vocab)\n",
    "    input_seq = pad_input(input_seq, max_len)\n",
    "    input_tensor = torch.LongTensor([input_seq]).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        enc_emb = model.embedding_enc(input_tensor)\n",
    "        _, (h, c) = model.encoder(enc_emb)\n",
    "\n",
    "        # Utilisation des tokens sp√©ciaux de notre vocabulaire\n",
    "        start_token = output_vocab.get('<start>', 0)\n",
    "        end_token = output_vocab.get('<end>', 1)\n",
    "\n",
    "        sequences = [[list(), 0.0, h, c]]\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            all_candidates = []\n",
    "            for seq, score, h, c in sequences:\n",
    "                if seq and seq[-1] == end_token:\n",
    "                    all_candidates.append((seq, score, h, c))\n",
    "                    continue\n",
    "                current_token = torch.LongTensor([[seq[-1]] if seq else [start_token]]).to(device)\n",
    "                dec_emb = model.embedding_dec(current_token)\n",
    "                output, (h_new, c_new) = model.decoder(dec_emb, (h, c))\n",
    "                logits = model.fc(output[:, -1, :])\n",
    "                probs = F.log_softmax(logits, dim=-1)\n",
    "                topk_probs, topk_idxs = torch.topk(probs, beam_width)\n",
    "\n",
    "                for i in range(beam_width):\n",
    "                    candidate = seq + [topk_idxs[0, i].item()]\n",
    "                    candidate_score = score + topk_probs[0, i].item()\n",
    "                    all_candidates.append((candidate, candidate_score, h_new, c_new))\n",
    "\n",
    "            sequences = sorted(all_candidates, key=lambda tup: tup[1], reverse=True)[:beam_width]\n",
    "\n",
    "        best_seq = sequences[0][0]\n",
    "        inv_vocab = {v: k for k, v in output_vocab.items()}\n",
    "        decoded_words = [inv_vocab.get(tok, '') for tok in best_seq if tok != end_token]\n",
    "        return ' '.join(decoded_words)\n",
    "\n",
    "# Fonctions d'√©valuation\n",
    "def compute_bleu(reference, generated):\n",
    "    ref_tokens = reference.lower().split()\n",
    "    gen_tokens = generated.lower().split()\n",
    "    return sentence_bleu([ref_tokens], gen_tokens)\n",
    "\n",
    "# ROUGE\n",
    "rouge = Rouge()\n",
    "def compute_rouge(reference, generated):\n",
    "    return rouge.get_scores(generated, reference)[0]\n",
    "\n",
    "# Perplexit√© (approxim√©e)\n",
    "def compute_perplexity(model, input_seq, target_seq):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    input_tensor = torch.LongTensor([input_seq]).to(device)\n",
    "    target_tensor = torch.LongTensor([target_seq]).to(device)\n",
    "\n",
    "    decoder_input = target_tensor[:, :-1]\n",
    "    target_output = target_tensor[:, 1:]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_tensor, decoder_input)\n",
    "        logits = logits.view(-1, logits.shape[-1])\n",
    "        target_output = target_output.view(-1)\n",
    "        loss = F.cross_entropy(logits, target_output, ignore_index=0)\n",
    "\n",
    "    return math.exp(loss.item())\n",
    "\n",
    "# Fonction pour charger le mod√®le et les vocabulaires\n",
    "def load_model_and_vocabs():\n",
    "    # Charger les vocabulaires\n",
    "    with open(\"input_vocab.pkl\", \"rb\") as f:\n",
    "        input_vocab = pickle.load(f)\n",
    "    with open(\"output_vocab.pkl\", \"rb\") as f:\n",
    "        output_vocab = pickle.load(f)\n",
    "    \n",
    "    # Charger le mod√®le (vous devrez adapter selon votre classe Seq2SeqModel)\n",
    "    # model = Seq2SeqModel(len(input_vocab), len(output_vocab))\n",
    "    # model.load_state_dict(torch.load(\"emotion_seq2seq_lstm_pytorch.pt\"))\n",
    "    \n",
    "    return input_vocab, output_vocab\n",
    "\n",
    "# Exemple d'utilisation\n",
    "if __name__ == \"__main__\":\n",
    "    # Test avec des vocabulaires d'exemple\n",
    "    input_vocab = {\"<pad>\": 0, \"<unk>\": 1, \"anger\": 2, \"[sep]\": 3, \"ce\": 4, \"service\": 5, \"est\": 6, \"scandaleux\": 7}\n",
    "    output_vocab = {\"<pad>\": 0, \"<unk>\": 1, \"<start>\": 2, \"<end>\": 3, \"je\": 4, \"comprends\": 5, \"votre\": 6, \"frustration\": 7}\n",
    "    \n",
    "    input_text = \"anger [SEP] Ce service est scandaleux !\"\n",
    "    reference_output = \"Je comprends votre frustration. Nous allons vous aider au plus vite.\"\n",
    "    \n",
    "    print(\"Input text:\", input_text)\n",
    "    print(\"Reference output:\", reference_output)\n",
    "    print(\"Input vocab:\", input_vocab)\n",
    "    print(\"Output vocab:\", output_vocab)\n",
    "    \n",
    "    # Note: Pour tester compl√®tement, vous devrez charger votre mod√®le entra√Æn√©\n",
    "    # input_vocab, output_vocab = load_model_and_vocabs()\n",
    "    # gen = generate_response_greedy(model, input_text, input_vocab, output_vocab)\n",
    "    # print(\"Generated:\", gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test des fonctions de g√©n√©ration corrig√©es ===\n",
      "\n",
      "1. Chargement des vocabulaires...\n",
      "   ‚úì Input vocab size: 290\n",
      "   ‚úì Output vocab size: 46\n",
      "\n",
      "2. Chargement du mod√®le...\n",
      "   ‚úì Mod√®le charg√© sur cuda\n",
      "\n",
      "3. Test de g√©n√©ration...\n",
      "   Input: anger [SEP] Ce service est scandaleux !\n",
      "   Reference: Je comprends votre frustration. Nous allons vous aider au plus vite.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yann\\AppData\\Local\\Temp\\ipykernel_26096\\3326141997.py:42: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"emotion_seq2seq_lstm_pytorch.pt\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Generated (Greedy): start > merci pour votre message concernant confusion . nous allons vous r√©pondre au mieux . < end > merci pour votre message concernant grief . nous allons vous r√©pondre\n",
      "   Generated (Beam): start > merci pour votre message concernant confusion . nous allons vous r√©pondre au mieux . < end > merci pour votre message concernant grief . nous allons vous r√©pondre\n",
      "\n",
      "4. √âvaluation...\n",
      "   BLEU: 0.0000\n",
      "   ROUGE: {'rouge-1': {'r': 0.36363636363636365, 'p': 0.23529411764705882, 'f': 0.28571428094387763}, 'rouge-2': {'r': 0.1, 'p': 0.05555555555555555, 'f': 0.071428566836735}, 'rouge-l': {'r': 0.36363636363636365, 'p': 0.23529411764705882, 'f': 0.28571428094387763}}\n",
      "\n",
      "‚úÖ Test r√©ussi ! Les fonctions corrig√©es fonctionnent correctement.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Yann\\anaconda3\\envs\\hf_gpu_env\\lib\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "from text_generation_fixed import generate_response_greedy, generate_response_beam, compute_bleu, compute_rouge, compute_perplexity\n",
    "\n",
    "# Classe du mod√®le Seq2Seq (copi√©e de votre notebook)\n",
    "class Seq2SeqModel(nn.Module):\n",
    "    def __init__(self, input_vocab_size, output_vocab_size, embedding_dim=128, hidden_dim=256):\n",
    "        super(Seq2SeqModel, self).__init__()\n",
    "        self.embedding_enc = nn.Embedding(input_vocab_size, embedding_dim)\n",
    "        self.embedding_dec = nn.Embedding(output_vocab_size, embedding_dim)\n",
    "        self.encoder = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.decoder = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        embedded_src = self.embedding_enc(src)\n",
    "        _, (h, c) = self.encoder(embedded_src)\n",
    "\n",
    "        embedded_tgt = self.embedding_dec(tgt)\n",
    "        output, _ = self.decoder(embedded_tgt, (h, c))\n",
    "        logits = self.fc(output)\n",
    "        return logits\n",
    "\n",
    "def test_generation():\n",
    "    print(\"=== Test des fonctions de g√©n√©ration corrig√©es ===\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Charger les vocabulaires\n",
    "        print(\"1. Chargement des vocabulaires...\")\n",
    "        with open(\"input_vocab.pkl\", \"rb\") as f:\n",
    "            input_vocab = pickle.load(f)\n",
    "        with open(\"output_vocab.pkl\", \"rb\") as f:\n",
    "            output_vocab = pickle.load(f)\n",
    "        print(f\"   ‚úì Input vocab size: {len(input_vocab)}\")\n",
    "        print(f\"   ‚úì Output vocab size: {len(output_vocab)}\")\n",
    "        \n",
    "        # Charger le mod√®le\n",
    "        print(\"\\n2. Chargement du mod√®le...\")\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = Seq2SeqModel(len(input_vocab), len(output_vocab)).to(device)\n",
    "        model.load_state_dict(torch.load(\"emotion_seq2seq_lstm_pytorch.pt\"))\n",
    "        print(f\"   ‚úì Mod√®le charg√© sur {device}\")\n",
    "        \n",
    "        # Test de g√©n√©ration\n",
    "        print(\"\\n3. Test de g√©n√©ration...\")\n",
    "        input_text = \"anger [SEP] Ce service est scandaleux !\"\n",
    "        reference_output = \"Je comprends votre frustration. Nous allons vous aider au plus vite.\"\n",
    "        \n",
    "        print(f\"   Input: {input_text}\")\n",
    "        print(f\"   Reference: {reference_output}\")\n",
    "        \n",
    "        # G√©n√©ration greedy\n",
    "        gen_greedy = generate_response_greedy(model, input_text, input_vocab, output_vocab)\n",
    "        print(f\"   Generated (Greedy): {gen_greedy}\")\n",
    "        \n",
    "        # G√©n√©ration beam search\n",
    "        gen_beam = generate_response_beam(model, input_text, input_vocab, output_vocab)\n",
    "        print(f\"   Generated (Beam): {gen_beam}\")\n",
    "        \n",
    "        # √âvaluation\n",
    "        print(\"\\n4. √âvaluation...\")\n",
    "        bleu_score = compute_bleu(reference_output, gen_greedy)\n",
    "        rouge_score = compute_rouge(reference_output, gen_greedy)\n",
    "        \n",
    "        print(f\"   BLEU: {bleu_score:.4f}\")\n",
    "        print(f\"   ROUGE: {rouge_score}\")\n",
    "        \n",
    "        print(\"\\n‚úÖ Test r√©ussi ! Les fonctions corrig√©es fonctionnent correctement.\")\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"‚ùå Erreur: Fichier non trouv√© - {e}\")\n",
    "        print(\"   Assurez-vous que les fichiers input_vocab.pkl, output_vocab.pkl et emotion_seq2seq_lstm_pytorch.pt existent.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur: {e}\")\n",
    "        print(\"   V√©rifiez que tous les fichiers n√©cessaires sont pr√©sents et que le mod√®le a √©t√© correctement entra√Æn√©.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_generation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse des R√©sultats - G√©n√©ration de Texte par √âmotions\n",
    "\n",
    "#### Objectif de l'Exp√©rience\n",
    "D√©velopper un syst√®me de g√©n√©ration de texte automatique pour le support client bas√© sur la d√©tection d'√©motions (28 √©motions GoEmotions) en utilisant un mod√®le Seq2Seq LSTM.\n",
    "\n",
    "#### M√©thodologie Employ√©e\n",
    "\n",
    "##### 1. **Approche Baseline (R√®gles)**\n",
    "- **M√©thode** : Templates pr√©d√©finis par √©motion\n",
    "- **Avantages** : Simple, rapide, pr√©visible\n",
    "- **Inconv√©nients** : Rigide, pas d'adaptation contextuelle\n",
    "\n",
    "##### 2. **Approche Deep Learning (Seq2Seq LSTM)**\n",
    "- **Architecture** : Encoder-Decoder LSTM avec embeddings\n",
    "- **Donn√©es** : 42,000 exemples g√©n√©r√©s (1,500 par √©motion)\n",
    "- **Vocabulaire** : Input (290 tokens) / Output (46 tokens)\n",
    "- **Entra√Ænement** : 10 √©poques, loss convergente\n",
    "\n",
    "#### R√©sultats Obtenus\n",
    "\n",
    "##### **M√©triques d'√âvaluation**\n",
    "- **BLEU Score** : 0.0000 (tr√®s faible)\n",
    "- **ROUGE-1 F1** : 0.286 (mod√©r√©)\n",
    "- **ROUGE-2 F1** : 0.071 (faible)\n",
    "- **ROUGE-L F1** : 0.286 (mod√©r√©)\n",
    "\n",
    "##### **Qualit√© de G√©n√©ration**\n",
    "- **Texte g√©n√©r√©** : \"start > merci pour votre message concernant confusion . nous allons vous r√©pondre au mieux . < end > merci pour votre message concernant grief . nous allons vous r√©pondre\"\n",
    "- **Probl√®mes identifi√©s** :\n",
    "  - R√©p√©tition de patterns\n",
    "  - Tokens sp√©ciaux non filtr√©s (`<start>`, `<end>`)\n",
    "  - Manque de coh√©rence √©motionnelle\n",
    "\n",
    "#### ÔøΩÔøΩ Analyse Critique\n",
    "\n",
    "##### **Points Positifs**\n",
    "1. **Convergence rapide** : Loss descend √† 0.0000 en 10 √©poques\n",
    "2. **Architecture fonctionnelle** : Le mod√®le g√©n√®re du texte coh√©rent syntaxiquement\n",
    "3. **Vocabulaire adapt√©** : Taille raisonnable pour le domaine\n",
    "4. **Fonctions corrig√©es** : G√©n√©ration greedy et beam search op√©rationnelles\n",
    "\n",
    "##### **Points Probl√©matiques**\n",
    "1. **Sous-apprentissage** : Loss trop basse sugg√®re un overfitting\n",
    "2. **Qualit√© g√©n√©rative faible** : BLEU score de 0 indique une mauvaise correspondance\n",
    "3. **Donn√©es synth√©tiques** : 1,500 exemples identiques par √©motion cr√©ent des biais\n",
    "4. **Manque de diversit√©** : Le mod√®le r√©p√®te les m√™mes patterns\n",
    "\n",
    "##### **Limitations Identifi√©es**\n",
    "1. **Donn√©es d'entra√Ænement** : Trop artificielles et r√©p√©titives\n",
    "2. **√âvaluation** : M√©triques classiques peu adapt√©es au support client\n",
    "3. **Contexte √©motionnel** : Le mod√®le ne capture pas vraiment l'√©motion d'entr√©e\n",
    "4. **Post-processing** : Absence de nettoyage des tokens sp√©ciaux\n",
    "\n",
    "##  Recommandations d'Am√©lioration\n",
    "\n",
    "\n",
    "\n",
    "### **1. Optimisation du Mod√®le**\n",
    "```python\n",
    "# Am√©liorations techniques\n",
    "- Augmenter la complexit√© (attention, transformer)\n",
    "- R√©gularisation (dropout, early stopping)\n",
    "- Hyperparam√®tres (learning rate, batch size)\n",
    "- Techniques de g√©n√©ration (top-k, nucleus sampling)\n",
    "```\n",
    "\n",
    "### **4. Post-processing**\n",
    "```python\n",
    "# Nettoyage et am√©lioration\n",
    "- Filtrage des tokens sp√©ciaux\n",
    "- Correction grammaticale\n",
    "- Adaptation au ton de l'entreprise\n",
    "- Validation √©motionnelle\n",
    "```\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Essaie d'am√©lioration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ D√©marrage du pipeline d'am√©lioration du mod√®le\n",
      "üöÄ D√©marrage du pipeline d'am√©lioration du mod√®le\n",
      "\n",
      "üìä 1. Chargement des donn√©es...\n",
      "   ‚úì Dataset r√©duit: 14000 √©chantillons\n",
      "\n",
      "üî§ 2. Construction des vocabulaires...\n",
      "   ‚úì Input vocab: 290 tokens\n",
      "   ‚úì Output vocab: 46 tokens\n",
      "\n",
      "‚öôÔ∏è 3. Tokenisation et encodage...\n",
      "   ‚úì Train: 12600 √©chantillons\n",
      "   ‚úì Validation: 1400 √©chantillons\n",
      "\n",
      "üß† 4. Cr√©ation et entra√Ænement du mod√®le...\n",
      "Entra√Ænement sur cuda\n",
      "Configuration: {'embedding_dim': 128, 'hidden_dim': 256, 'dropout': 0.2, 'learning_rate': 0.001, 'batch_size': 32, 'epochs': 20, 'gradient_clip': 1.0, 'max_len': 30}\n",
      "Meilleur mod√®le sauvegard√© (Val Loss: 0.0087)\n",
      "Epoch 1/20, Train Loss: 0.2426\n",
      "Meilleur mod√®le sauvegard√© (Val Loss: 0.0003)\n",
      "Meilleur mod√®le sauvegard√© (Val Loss: 0.0001)\n",
      "Epoch 6/20, Train Loss: 0.0001\n",
      "Meilleur mod√®le sauvegard√© (Val Loss: 0.0001)\n",
      "Meilleur mod√®le sauvegard√© (Val Loss: 0.0000)\n",
      "Meilleur mod√®le sauvegard√© (Val Loss: 0.0000)\n",
      "Epoch 11/20, Train Loss: 0.0000\n",
      "Early stopping triggered\n",
      "\n",
      "üíæ 5. Sauvegarde...\n",
      "   ‚úì Mod√®le et vocabulaires sauvegard√©s\n",
      "\n",
      "üéØ 6. Test de g√©n√©ration...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Tensors must have same number of dimensions: got 4 and 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 619\u001b[0m\n\u001b[0;32m    613\u001b[0m \u001b[38;5;66;03m# ============================================================================\u001b[39;00m\n\u001b[0;32m    614\u001b[0m \u001b[38;5;66;03m# EX√âCUTION POUR LE NOTEBOOK\u001b[39;00m\n\u001b[0;32m    615\u001b[0m \u001b[38;5;66;03m# ============================================================================\u001b[39;00m\n\u001b[0;32m    616\u001b[0m \n\u001b[0;32m    617\u001b[0m \u001b[38;5;66;03m# Ex√©cuter le pipeline complet\u001b[39;00m\n\u001b[0;32m    618\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müöÄ D√©marrage du pipeline d\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mam√©lioration du mod√®le\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 619\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mrun_improved_model_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    621\u001b[0m \u001b[38;5;66;03m# Afficher un r√©sum√©\u001b[39;00m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m60\u001b[39m)\n",
      "Cell \u001b[1;32mIn[13], line 538\u001b[0m, in \u001b[0;36mrun_improved_model_pipeline\u001b[1;34m()\u001b[0m\n\u001b[0;32m    535\u001b[0m reference_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJe comprends votre frustration. Nous allons vous aider au plus vite.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    537\u001b[0m generated_greedy \u001b[38;5;241m=\u001b[39m generate_response_light(model, input_text, input_vocab, output_vocab, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgreedy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 538\u001b[0m generated_nucleus \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_response_light\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_vocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_vocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnucleus\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    540\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   Input: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_text\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    541\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   Reference: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreference_output\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[13], line 252\u001b[0m, in \u001b[0;36mgenerate_response_light\u001b[1;34m(model, input_text, input_vocab, output_vocab, max_len, method)\u001b[0m\n\u001b[0;32m    249\u001b[0m context_expanded \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# [1, 1, hidden_dim]\u001b[39;00m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;66;03m# Input du decoder (concat√©nation embedding + context)\u001b[39;00m\n\u001b[1;32m--> 252\u001b[0m decoder_input \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedded_tgt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_expanded\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;66;03m# LSTM step\u001b[39;00m\n\u001b[0;32m    255\u001b[0m output, (h, c) \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mdecoder(decoder_input, (h, c))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 4 and 3"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MOD√àLE SEQ2SEQ AM√âLIOR√â - VERSION L√âG√àRE POUR PETIT PC (CORRIG√âE FINALE)\n",
    "# ============================================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pickle\n",
    "import math\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge import Rouge\n",
    "import gc\n",
    "\n",
    "# T√©l√©charger NLTK si n√©cessaire\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "# ============================================================================\n",
    "# 1. ATTENTION MECHANISM SIMPLIFI√â (CORRIG√â)\n",
    "# ============================================================================\n",
    "\n",
    "class SimpleAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.attention = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.v = nn.Linear(hidden_dim, 1, bias=False)\n",
    "    \n",
    "    def forward(self, decoder_hidden, encoder_outputs):\n",
    "        seq_len = encoder_outputs.size(1)\n",
    "        decoder_hidden = decoder_hidden.unsqueeze(1).expand(-1, seq_len, -1)\n",
    "        \n",
    "        energy = torch.tanh(self.attention(torch.cat((decoder_hidden, encoder_outputs), dim=2)))\n",
    "        attention_weights = F.softmax(self.v(energy), dim=1)\n",
    "        context = torch.sum(attention_weights * encoder_outputs, dim=1)\n",
    "        return context\n",
    "\n",
    "class LightSeq2SeqModel(nn.Module):\n",
    "    def __init__(self, input_vocab_size, output_vocab_size, embedding_dim=128, hidden_dim=256, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.embedding_enc = nn.Embedding(input_vocab_size, embedding_dim)\n",
    "        self.embedding_dec = nn.Embedding(output_vocab_size, embedding_dim)\n",
    "        \n",
    "        # Encoder l√©ger (unidirectional)\n",
    "        self.encoder = nn.LSTM(\n",
    "            embedding_dim, hidden_dim, \n",
    "            num_layers=1, \n",
    "            batch_first=True,\n",
    "            dropout=0\n",
    "        )\n",
    "        \n",
    "        # Decoder avec attention\n",
    "        self.decoder = nn.LSTM(\n",
    "            embedding_dim + hidden_dim, hidden_dim,  # 128 + 256 = 384\n",
    "            num_layers=1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.attention = SimpleAttention(hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src, tgt):\n",
    "        batch_size = src.size(0)\n",
    "        tgt_len = tgt.size(1)\n",
    "        \n",
    "        # Encoder\n",
    "        embedded_src = self.dropout(self.embedding_enc(src))\n",
    "        encoder_outputs, (h, c) = self.encoder(embedded_src)\n",
    "        \n",
    "        # Decoder avec attention\n",
    "        embedded_tgt = self.dropout(self.embedding_dec(tgt))\n",
    "        outputs = torch.zeros(batch_size, tgt_len, self.fc.out_features).to(src.device)\n",
    "        \n",
    "        for t in range(tgt_len):\n",
    "            # Attention\n",
    "            context = self.attention(h[-1], encoder_outputs)\n",
    "            \n",
    "            # Input du decoder (concat√©nation embedding + context)\n",
    "            decoder_input = torch.cat((embedded_tgt[:, t:t+1], context.unsqueeze(1)), dim=2)\n",
    "            \n",
    "            # LSTM step\n",
    "            output, (h, c) = self.decoder(decoder_input, (h, c))\n",
    "            \n",
    "            # Output\n",
    "            output = self.dropout(output)\n",
    "            outputs[:, t:t+1] = self.fc(output)\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "# ============================================================================\n",
    "# 2. R√âGULARISATION ET UTILITAIRES\n",
    "# ============================================================================\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0.001):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        \n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        return False\n",
    "\n",
    "def clear_memory():\n",
    "    \"\"\"Nettoyer la m√©moire\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "def reduce_dataset_size(df, max_samples_per_emotion=500):\n",
    "    \"\"\"R√©duire le dataset pour √©conomiser la m√©moire\"\"\"\n",
    "    reduced_data = []\n",
    "    for emotion in df['emotion'].unique():\n",
    "        emotion_data = df[df['emotion'] == emotion]\n",
    "        if len(emotion_data) > max_samples_per_emotion:\n",
    "            emotion_data = emotion_data.sample(n=max_samples_per_emotion, random_state=42)\n",
    "        reduced_data.append(emotion_data)\n",
    "    return pd.concat(reduced_data, ignore_index=True)\n",
    "\n",
    "def prepare_data_for_training(df):\n",
    "    \"\"\"Pr√©parer les donn√©es pour l'entra√Ænement\"\"\"\n",
    "    # Cr√©er les s√©quences d'entr√©e et de sortie AVANT d'optimiser les types\n",
    "    df[\"input_seq\"] = df[\"emotion\"].astype(str) + \" [SEP] \" + df[\"text_input\"].astype(str)\n",
    "    df[\"output_seq\"] = \"<start> \" + df[\"text_output\"].astype(str) + \" <end>\"\n",
    "    \n",
    "    # Maintenant on peut optimiser les types pour les autres colonnes\n",
    "    for col in df.select_dtypes(include=['object']).columns:\n",
    "        if col not in ['input_seq', 'output_seq']:  # Ne pas optimiser les colonnes de s√©quences\n",
    "            df[col] = df[col].astype('category')\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ============================================================================\n",
    "# 3. PR√âPARATION DES DONN√âES\n",
    "# ============================================================================\n",
    "\n",
    "def build_vocab(sentences, min_freq=1):\n",
    "    \"\"\"Construire le vocabulaire\"\"\"\n",
    "    counter = Counter()\n",
    "    for sentence in sentences:\n",
    "        counter.update(word_tokenize(sentence.lower()))\n",
    "    vocab = {\"<pad>\": 0, \"<unk>\": 1}\n",
    "    for word, freq in counter.items():\n",
    "        if freq >= min_freq:\n",
    "            vocab[word] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "def encode_sentence(sentence, vocab):\n",
    "    \"\"\"Encoder une phrase avec le vocabulaire\"\"\"\n",
    "    return [vocab.get(tok, vocab[\"<unk>\"]) for tok in word_tokenize(sentence.lower())]\n",
    "\n",
    "def pad_input(input_seq, max_len=30):\n",
    "    \"\"\"Padding des s√©quences\"\"\"\n",
    "    if len(input_seq) < max_len:\n",
    "        padding = [0] * (max_len - len(input_seq))\n",
    "        input_seq.extend(padding)\n",
    "    else:\n",
    "        input_seq = input_seq[:max_len]\n",
    "    return input_seq\n",
    "\n",
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, inputs, targets):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.targets[idx]\n",
    "\n",
    "# ============================================================================\n",
    "# 4. TECHNIQUES DE G√âN√âRATION (CORRIG√âES FINALES)\n",
    "# ============================================================================\n",
    "\n",
    "def simple_sampling(logits, method='greedy', k=10, p=0.8):\n",
    "    \"\"\"Sampling simplifi√© pour la g√©n√©ration\"\"\"\n",
    "    if method == 'greedy':\n",
    "        return torch.argmax(logits, dim=-1)\n",
    "    elif method == 'top_k':\n",
    "        top_k_logits, top_k_indices = torch.topk(logits, min(k, logits.size(-1)), dim=-1)\n",
    "        probs = F.softmax(top_k_logits, dim=-1)\n",
    "        sampled_indices = torch.multinomial(probs, 1)\n",
    "        return top_k_indices.gather(-1, sampled_indices)\n",
    "    elif method == 'nucleus':\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "        sorted_indices_to_remove = cumulative_probs > p\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "        \n",
    "        indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
    "        logits[indices_to_remove] = float('-inf')\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        return torch.multinomial(probs, 1)\n",
    "\n",
    "def generate_response_light(model, input_text, input_vocab, output_vocab, \n",
    "                          max_len=30, method='nucleus'):\n",
    "    \"\"\"G√©n√©ration de r√©ponse avec le mod√®le l√©ger (CORRIG√âE FINALE)\"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    # Encodage\n",
    "    input_seq = encode_sentence(input_text, input_vocab)\n",
    "    input_seq = pad_input(input_seq, max_len)\n",
    "    input_tensor = torch.LongTensor([input_seq]).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Encoder\n",
    "        embedded_src = model.embedding_enc(input_tensor)\n",
    "        encoder_outputs, (h, c) = model.encoder(embedded_src)\n",
    "        \n",
    "        # Decoder avec attention\n",
    "        start_token = output_vocab.get('<start>', 0)\n",
    "        end_token = output_vocab.get('<end>', 1)\n",
    "        current_token = torch.LongTensor([[start_token]]).to(device)\n",
    "        \n",
    "        generated_tokens = []\n",
    "        \n",
    "        for _ in range(max_len):\n",
    "            # CORRECTION FINALE : Utiliser la m√™me logique que dans forward()\n",
    "            embedded_tgt = model.embedding_dec(current_token)  # [1, 1, embedding_dim]\n",
    "            \n",
    "            # Attention\n",
    "            context = model.attention(h[-1], encoder_outputs)  # [1, hidden_dim]\n",
    "            \n",
    "            # CORRECTION : S'assurer que les dimensions sont coh√©rentes\n",
    "            # embedded_tgt: [1, 1, embedding_dim]\n",
    "            # context: [1, hidden_dim] -> [1, 1, hidden_dim]\n",
    "            context_expanded = context.unsqueeze(1)  # [1, 1, hidden_dim]\n",
    "            \n",
    "            # Input du decoder (concat√©nation embedding + context)\n",
    "            decoder_input = torch.cat((embedded_tgt, context_expanded), dim=2)\n",
    "            \n",
    "            # LSTM step\n",
    "            output, (h, c) = model.decoder(decoder_input, (h, c))\n",
    "            logits = model.fc(output[:, -1, :])\n",
    "            \n",
    "            predicted_token = simple_sampling(logits, method)\n",
    "            predicted_id = predicted_token.item()\n",
    "            \n",
    "            if predicted_id == end_token:\n",
    "                break\n",
    "                \n",
    "            generated_tokens.append(predicted_id)\n",
    "            current_token = predicted_token.unsqueeze(0)\n",
    "    \n",
    "    # D√©codage\n",
    "    inv_vocab = {v: k for k, v in output_vocab.items()}\n",
    "    decoded_words = [inv_vocab.get(tok, '') for tok in generated_tokens]\n",
    "    return ' '.join(decoded_words)\n",
    "\n",
    "# ============================================================================\n",
    "# 5. M√âTRIQUES S3 (BLEU, ROUGE, PERPLEXITY)\n",
    "# ============================================================================\n",
    "\n",
    "def compute_bleu(reference, generated):\n",
    "    \"\"\"Calculer le score BLEU\"\"\"\n",
    "    ref_tokens = reference.lower().split()\n",
    "    gen_tokens = generated.lower().split()\n",
    "    return sentence_bleu([ref_tokens], gen_tokens)\n",
    "\n",
    "def compute_rouge(reference, generated):\n",
    "    \"\"\"Calculer les scores ROUGE\"\"\"\n",
    "    rouge = Rouge()\n",
    "    try:\n",
    "        return rouge.get_scores(generated, reference)[0]\n",
    "    except:\n",
    "        return {'rouge-1': {'f': 0.0}, 'rouge-2': {'f': 0.0}, 'rouge-l': {'f': 0.0}}\n",
    "\n",
    "def compute_perplexity(model, input_seq, target_seq):\n",
    "    \"\"\"Calculer la perplexit√©\"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    input_tensor = torch.LongTensor([input_seq]).to(device)\n",
    "    target_tensor = torch.LongTensor([target_seq]).to(device)\n",
    "    \n",
    "    decoder_input = target_tensor[:, :-1]\n",
    "    target_output = target_tensor[:, 1:]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(input_tensor, decoder_input)\n",
    "        logits = logits.view(-1, logits.shape[-1])\n",
    "        target_output = target_output.view(-1)\n",
    "        loss = F.cross_entropy(logits, target_output, ignore_index=0)\n",
    "    \n",
    "    return math.exp(loss.item())\n",
    "\n",
    "def evaluate_model_s3(model, test_data, input_vocab, output_vocab, max_samples=100):\n",
    "    \"\"\"√âvaluation compl√®te avec m√©triques S3\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    bleu_scores = []\n",
    "    rouge_scores = []\n",
    "    perplexity_scores = []\n",
    "    \n",
    "    # Limiter le nombre d'√©chantillons pour √©viter la surcharge m√©moire\n",
    "    test_samples = test_data.sample(min(len(test_data), max_samples), random_state=42)\n",
    "    \n",
    "    for idx, row in test_samples.iterrows():\n",
    "        input_text = row['input_seq']\n",
    "        reference_output = row['output_seq'].replace('<start> ', '').replace(' <end>', '')\n",
    "        \n",
    "        try:\n",
    "            # G√©n√©ration\n",
    "            generated = generate_response_light(model, input_text, input_vocab, output_vocab)\n",
    "            \n",
    "            # M√©triques\n",
    "            bleu = compute_bleu(reference_output, generated)\n",
    "            rouge = compute_rouge(reference_output, generated)\n",
    "            \n",
    "            # Perplexit√©\n",
    "            input_seq = encode_sentence(input_text, input_vocab)\n",
    "            input_seq = pad_input(input_seq, 30)\n",
    "            reference_seq = encode_sentence(row['output_seq'], output_vocab)\n",
    "            perplexity = compute_perplexity(model, input_seq, reference_seq)\n",
    "            \n",
    "            bleu_scores.append(bleu)\n",
    "            rouge_scores.append(rouge)\n",
    "            perplexity_scores.append(perplexity)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors de l'√©valuation de l'√©chantillon {idx}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Calcul des moyennes\n",
    "    avg_bleu = np.mean(bleu_scores) if bleu_scores else 0.0\n",
    "    avg_rouge_1 = np.mean([r['rouge-1']['f'] for r in rouge_scores]) if rouge_scores else 0.0\n",
    "    avg_rouge_2 = np.mean([r['rouge-2']['f'] for r in rouge_scores]) if rouge_scores else 0.0\n",
    "    avg_rouge_l = np.mean([r['rouge-l']['f'] for r in rouge_scores]) if rouge_scores else 0.0\n",
    "    avg_perplexity = np.mean(perplexity_scores) if perplexity_scores else float('inf')\n",
    "    \n",
    "    return {\n",
    "        'BLEU': avg_bleu,\n",
    "        'ROUGE-1': avg_rouge_1,\n",
    "        'ROUGE-2': avg_rouge_2,\n",
    "        'ROUGE-L': avg_rouge_l,\n",
    "        'Perplexity': avg_perplexity,\n",
    "        'n_samples': len(bleu_scores)\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# 6. PIPELINE D'ENTRA√éNEMENT COMPLET\n",
    "# ============================================================================\n",
    "\n",
    "def train_model_light(model, train_loader, val_loader, config):\n",
    "    \"\"\"Entra√Ænement du mod√®le l√©ger\"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=2)\n",
    "    early_stopping = EarlyStopping(patience=5)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    training_history = {'train_loss': [], 'val_loss': []}\n",
    "    \n",
    "    print(f\"Entra√Ænement sur {device}\")\n",
    "    print(f\"Configuration: {config}\")\n",
    "    \n",
    "    for epoch in range(config['epochs']):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            decoder_input = batch_y[:, :-1]\n",
    "            target = batch_y[:, 1:]\n",
    "            \n",
    "            output = model(batch_x, decoder_input)\n",
    "            output = output.reshape(-1, output.shape[2])\n",
    "            target = target.reshape(-1)\n",
    "            \n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), config['gradient_clip'])\n",
    "            \n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation (moins fr√©quente)\n",
    "        if epoch % 2 == 0:\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for batch_x, batch_y in val_loader:\n",
    "                    batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                    decoder_input = batch_y[:, :-1]\n",
    "                    target = batch_y[:, 1:]\n",
    "                    \n",
    "                    output = model(batch_x, decoder_input)\n",
    "                    output = output.reshape(-1, output.shape[2])\n",
    "                    target = target.reshape(-1)\n",
    "                    \n",
    "                    loss = criterion(output, target)\n",
    "                    val_loss += loss.item()\n",
    "            \n",
    "            avg_train_loss = train_loss / len(train_loader)\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            \n",
    "            training_history['train_loss'].append(avg_train_loss)\n",
    "            training_history['val_loss'].append(avg_val_loss)\n",
    "            \n",
    "            scheduler.step(avg_val_loss)\n",
    "            \n",
    "            # Early stopping\n",
    "            if early_stopping(avg_val_loss):\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "            \n",
    "            # Save best model\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                torch.save(model.state_dict(), \"best_model_light.pt\")\n",
    "                print(f\"Meilleur mod√®le sauvegard√© (Val Loss: {avg_val_loss:.4f})\")\n",
    "        \n",
    "        # Logging\n",
    "        if epoch % 5 == 0:\n",
    "            avg_train_loss = train_loss / len(train_loader)\n",
    "            print(f\"Epoch {epoch+1}/{config['epochs']}, Train Loss: {avg_train_loss:.4f}\")\n",
    "    \n",
    "    return best_val_loss, training_history\n",
    "\n",
    "# ============================================================================\n",
    "# 7. FONCTION PRINCIPALE POUR LE NOTEBOOK\n",
    "# ============================================================================\n",
    "\n",
    "def run_improved_model_pipeline():\n",
    "    \"\"\"Pipeline complet pour le notebook\"\"\"\n",
    "    print(\"üöÄ D√©marrage du pipeline d'am√©lioration du mod√®le\")\n",
    "    \n",
    "    # Configuration l√©g√®re\n",
    "    LIGHT_CONFIG = {\n",
    "        'embedding_dim': 128,\n",
    "        'hidden_dim': 256,\n",
    "        'dropout': 0.2,\n",
    "        'learning_rate': 0.001,\n",
    "        'batch_size': 32,\n",
    "        'epochs': 20,\n",
    "        'gradient_clip': 1.0,\n",
    "        'max_len': 30\n",
    "    }\n",
    "    \n",
    "    # 1. Chargement et pr√©paration des donn√©es\n",
    "    print(\"\\nüìä 1. Chargement des donn√©es...\")\n",
    "    df = pd.read_csv(\"../data/emotion_datasets/emo_reviews.csv\")\n",
    "    df = reduce_dataset_size(df, max_samples_per_emotion=500)  # R√©duire la taille\n",
    "    \n",
    "    # Pr√©paration des donn√©es\n",
    "    df = prepare_data_for_training(df)\n",
    "    \n",
    "    print(f\"   ‚úì Dataset r√©duit: {len(df)} √©chantillons\")\n",
    "    \n",
    "    # 2. Construction des vocabulaires\n",
    "    print(\"\\nüî§ 2. Construction des vocabulaires...\")\n",
    "    input_vocab = build_vocab(df[\"input_seq\"])\n",
    "    output_vocab = build_vocab(df[\"output_seq\"])\n",
    "    \n",
    "    print(f\"   ‚úì Input vocab: {len(input_vocab)} tokens\")\n",
    "    print(f\"   ‚úì Output vocab: {len(output_vocab)} tokens\")\n",
    "    \n",
    "    # 3. Tokenisation et encodage\n",
    "    print(\"\\n‚öôÔ∏è 3. Tokenisation et encodage...\")\n",
    "    MAX_LEN = LIGHT_CONFIG['max_len']\n",
    "    \n",
    "    X = [torch.tensor(encode_sentence(s, input_vocab))[:MAX_LEN] for s in df[\"input_seq\"]]\n",
    "    y = [torch.tensor(encode_sentence(s, output_vocab))[:MAX_LEN] for s in df[\"output_seq\"]]\n",
    "    \n",
    "    X_pad = pad_sequence(X, batch_first=True, padding_value=0)\n",
    "    y_pad = pad_sequence(y, batch_first=True, padding_value=0)\n",
    "    \n",
    "    # 4. Split train/validation\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_pad, y_pad, test_size=0.1, random_state=42)\n",
    "    \n",
    "    train_dataset = EmotionDataset(X_train, y_train)\n",
    "    val_dataset = EmotionDataset(X_val, y_val)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=LIGHT_CONFIG['batch_size'], shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=LIGHT_CONFIG['batch_size'], shuffle=False)\n",
    "    \n",
    "    print(f\"   ‚úì Train: {len(train_dataset)} √©chantillons\")\n",
    "    print(f\"   ‚úì Validation: {len(val_dataset)} √©chantillons\")\n",
    "    \n",
    "    # 5. Cr√©ation et entra√Ænement du mod√®le\n",
    "    print(\"\\nüß† 4. Cr√©ation et entra√Ænement du mod√®le...\")\n",
    "    model = LightSeq2SeqModel(\n",
    "        input_vocab_size=len(input_vocab),\n",
    "        output_vocab_size=len(output_vocab),\n",
    "        embedding_dim=LIGHT_CONFIG['embedding_dim'],\n",
    "        hidden_dim=LIGHT_CONFIG['hidden_dim'],\n",
    "        dropout=LIGHT_CONFIG['dropout']\n",
    "    )\n",
    "    \n",
    "    best_loss, training_history = train_model_light(model, train_loader, val_loader, LIGHT_CONFIG)\n",
    "    \n",
    "    # 6. Sauvegarde du mod√®le et des vocabulaires\n",
    "    print(\"\\nüíæ 5. Sauvegarde...\")\n",
    "    torch.save(model.state_dict(), \"improved_emotion_seq2seq_lstm.pt\")\n",
    "    \n",
    "    with open(\"improved_input_vocab.pkl\", \"wb\") as f:\n",
    "        pickle.dump(input_vocab, f)\n",
    "    with open(\"improved_output_vocab.pkl\", \"wb\") as f:\n",
    "        pickle.dump(output_vocab, f)\n",
    "    \n",
    "    print(\"   ‚úì Mod√®le et vocabulaires sauvegard√©s\")\n",
    "    \n",
    "    # 7. Test de g√©n√©ration\n",
    "    print(\"\\nüéØ 6. Test de g√©n√©ration...\")\n",
    "    input_text = \"anger [SEP] Ce service est scandaleux !\"\n",
    "    reference_output = \"Je comprends votre frustration. Nous allons vous aider au plus vite.\"\n",
    "    \n",
    "    generated_greedy = generate_response_light(model, input_text, input_vocab, output_vocab, method='greedy')\n",
    "    generated_nucleus = generate_response_light(model, input_text, input_vocab, output_vocab, method='nucleus')\n",
    "    \n",
    "    print(f\"   Input: {input_text}\")\n",
    "    print(f\"   Reference: {reference_output}\")\n",
    "    print(f\"   Generated (Greedy): {generated_greedy}\")\n",
    "    print(f\"   Generated (Nucleus): {generated_nucleus}\")\n",
    "    \n",
    "    # 8. √âvaluation avec m√©triques S3\n",
    "    print(\"\\n 7. √âvaluation avec m√©triques S3...\")\n",
    "    test_df = df.sample(min(100, len(df)), random_state=42)  # √âchantillon de test\n",
    "    s3_metrics = evaluate_model_s3(model, test_df, input_vocab, output_vocab, max_samples=50)\n",
    "    \n",
    "    print(\"   M√©triques S3:\")\n",
    "    print(f\"   - BLEU: {s3_metrics['BLEU']:.4f}\")\n",
    "    print(f\"   - ROUGE-1: {s3_metrics['ROUGE-1']:.4f}\")\n",
    "    print(f\"   - ROUGE-2: {s3_metrics['ROUGE-2']:.4f}\")\n",
    "    print(f\"   - ROUGE-L: {s3_metrics['ROUGE-L']:.4f}\")\n",
    "    print(f\"   - Perplexity: {s3_metrics['Perplexity']:.2f}\")\n",
    "    print(f\"   - √âchantillons √©valu√©s: {s3_metrics['n_samples']}\")\n",
    "    \n",
    "    # 9. Nettoyage m√©moire\n",
    "    print(\"\\nüßπ 8. Nettoyage m√©moire...\")\n",
    "    clear_memory()\n",
    "    \n",
    "    print(\"\\n‚úÖ Pipeline termin√© avec succ√®s!\")\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'input_vocab': input_vocab,\n",
    "        'output_vocab': output_vocab,\n",
    "        'config': LIGHT_CONFIG,\n",
    "        'training_history': training_history,\n",
    "        's3_metrics': s3_metrics,\n",
    "        'generated_examples': {\n",
    "            'greedy': generated_greedy,\n",
    "            'nucleus': generated_nucleus\n",
    "        }\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# 8. FONCTIONS DE CHARGEMENT POUR R√âUTILISATION\n",
    "# ============================================================================\n",
    "\n",
    "def load_improved_model():\n",
    "    \"\"\"Charger le mod√®le am√©lior√© sauvegard√©\"\"\"\n",
    "    with open(\"improved_input_vocab.pkl\", \"rb\") as f:\n",
    "        input_vocab = pickle.load(f)\n",
    "    with open(\"improved_output_vocab.pkl\", \"rb\") as f:\n",
    "        output_vocab = pickle.load(f)\n",
    "    \n",
    "    model = LightSeq2SeqModel(\n",
    "        input_vocab_size=len(input_vocab),\n",
    "        output_vocab_size=len(output_vocab),\n",
    "        embedding_dim=128,\n",
    "        hidden_dim=256,\n",
    "        dropout=0.2\n",
    "    )\n",
    "    \n",
    "    model.load_state_dict(torch.load(\"improved_emotion_seq2seq_lstm.pt\"))\n",
    "    return model, input_vocab, output_vocab\n",
    "\n",
    "def test_loaded_model():\n",
    "    \"\"\"Tester le mod√®le charg√©\"\"\"\n",
    "    model, input_vocab, output_vocab = load_improved_model()\n",
    "    model.eval()\n",
    "    \n",
    "    input_text = \"joy [SEP] Je suis tr√®s content de votre service !\"\n",
    "    generated = generate_response_light(model, input_text, input_vocab, output_vocab, method='nucleus')\n",
    "    \n",
    "    print(f\"Test du mod√®le charg√©:\")\n",
    "    print(f\"Input: {input_text}\")\n",
    "    print(f\"Generated: {generated}\")\n",
    "    \n",
    "    return generated\n",
    "\n",
    "# ============================================================================\n",
    "# EX√âCUTION POUR LE NOTEBOOK\n",
    "# ============================================================================\n",
    "\n",
    "# Ex√©cuter le pipeline complet\n",
    "print(\"üöÄ D√©marrage du pipeline d'am√©lioration du mod√®le\")\n",
    "results = run_improved_model_pipeline()\n",
    "\n",
    "# Afficher un r√©sum√©\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä R√âSUM√â DES R√âSULTATS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"BLEU Score: {results['s3_metrics']['BLEU']:.4f}\")\n",
    "print(f\"ROUGE-1 F1: {results['s3_metrics']['ROUGE-1']:.4f}\")\n",
    "print(f\"ROUGE-2 F1: {results['s3_metrics']['ROUGE-2']:.4f}\")\n",
    "print(f\"ROUGE-L F1: {results['s3_metrics']['ROUGE-L']:.4f}\")\n",
    "print(f\"Perplexity: {results['s3_metrics']['Perplexity']:.2f}\")\n",
    "print(f\"Configuration utilis√©e: {results['config']}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Fonctions de g√©n√©ration corrig√©es charg√©es !\n",
      "=== Test complet des fonctions de g√©n√©ration ===\n",
      "\n",
      "1. Chargement des vocabulaires...\n",
      "   ‚úì Input vocab size: 290\n",
      "   ‚úì Output vocab size: 46\n",
      "\n",
      "2. Chargement du mod√®le...\n",
      "   ‚úì Mod√®le charg√© sur cuda\n",
      "\n",
      "3. Test de g√©n√©ration...\n",
      "   Input: anger [SEP] Ce service est scandaleux !\n",
      "   Reference: Je comprends votre frustration. Nous allons vous aider au plus vite.\n",
      "   Generated (Greedy): start > merci pour votre message concernant confusion . nous allons vous r√©pondre au mieux . < end > merci pour votre message concernant grief . nous allons vous r√©pondre\n",
      "   Generated (Beam): start > merci pour votre message concernant confusion . nous allons vous r√©pondre au mieux . < end > merci pour votre message concernant grief . nous allons vous r√©pondre\n",
      "\n",
      "4. √âvaluation...\n",
      "   BLEU: 0.0000\n",
      "   ROUGE: {'rouge-1': {'r': 0.36363636363636365, 'p': 0.23529411764705882, 'f': 0.28571428094387763}, 'rouge-2': {'r': 0.1, 'p': 0.05555555555555555, 'f': 0.071428566836735}, 'rouge-l': {'r': 0.36363636363636365, 'p': 0.23529411764705882, 'f': 0.28571428094387763}}\n",
      "\n",
      "5. Test avec diff√©rentes √©motions...\n",
      "   joy: start > merci pour votre message concernant admiration . nous allons vous r√©pondre au mieux . < end > merci pour votre message concernant grief . nous allons vous r√©pondre\n",
      "   sadness: start > merci pour votre message concernant confusion . nous allons vous r√©pondre au mieux . < end > merci pour votre message concernant grief . nous allons vous r√©pondre\n",
      "   fear: start > merci pour votre message concernant confusion . nous allons vous r√©pondre au mieux . < end > merci pour votre message concernant grief . nous allons vous r√©pondre\n",
      "   anger: start > merci pour votre message concernant confusion . nous allons vous r√©pondre au mieux . < end > merci pour votre message concernant grief . nous allons vous r√©pondre\n",
      "\n",
      "‚úÖ Test complet r√©ussi !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yann\\AppData\\Local\\Temp\\ipykernel_26096\\1754938559.py:162: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"emotion_seq2seq_lstm_pytorch.pt\"))\n",
      "c:\\Users\\Yann\\anaconda3\\envs\\hf_gpu_env\\lib\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge import Rouge\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "# Fonction de padding PyTorch custom corrig√©e\n",
    "def pad_input(input_seq, max_len=30):\n",
    "    if len(input_seq) < max_len:\n",
    "        padding = [0] * (max_len - len(input_seq))\n",
    "        input_seq.extend(padding)\n",
    "    else:\n",
    "        input_seq = input_seq[:max_len]\n",
    "    return input_seq\n",
    "\n",
    "# Fonction d'encodage pour utiliser avec nos vocabulaires\n",
    "def encode_sentence(sentence, vocab):\n",
    "    return [vocab.get(tok, vocab[\"<unk>\"]) for tok in word_tokenize(sentence.lower())]\n",
    "\n",
    "# Fonction Greedy corrig√©e\n",
    "def generate_response_greedy(model, input_text, input_vocab, output_vocab, max_len=30):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    # Encodage avec notre vocabulaire personnalis√©\n",
    "    input_seq = encode_sentence(input_text, input_vocab)\n",
    "    input_seq = pad_input(input_seq, max_len)\n",
    "    input_tensor = torch.LongTensor([input_seq]).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        enc_emb = model.embedding_enc(input_tensor)\n",
    "        _, (h, c) = model.encoder(enc_emb)\n",
    "\n",
    "        # Utilisation des tokens sp√©ciaux de notre vocabulaire\n",
    "        start_token = output_vocab.get('<start>', 0)\n",
    "        end_token = output_vocab.get('<end>', 1)\n",
    "        current_token = torch.LongTensor([[start_token]]).to(device)\n",
    "\n",
    "        generated_tokens = []\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            dec_emb = model.embedding_dec(current_token)\n",
    "            output, (h, c) = model.decoder(dec_emb, (h, c))\n",
    "            logits = model.fc(output[:, -1, :])\n",
    "            predicted_token = torch.argmax(logits, dim=-1)\n",
    "            predicted_id = predicted_token.item()\n",
    "\n",
    "            if predicted_id == end_token:\n",
    "                break\n",
    "\n",
    "            generated_tokens.append(predicted_id)\n",
    "            current_token = predicted_token.unsqueeze(0)\n",
    "\n",
    "    # D√©codage avec notre vocabulaire\n",
    "    inv_vocab = {v: k for k, v in output_vocab.items()}\n",
    "    decoded_words = [inv_vocab.get(tok, '') for tok in generated_tokens]\n",
    "    return ' '.join(decoded_words)\n",
    "\n",
    "# Fonction Beam Search corrig√©e\n",
    "def generate_response_beam(model, input_text, input_vocab, output_vocab, max_len=30, beam_width=3):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    # Encodage avec notre vocabulaire personnalis√©\n",
    "    input_seq = encode_sentence(input_text, input_vocab)\n",
    "    input_seq = pad_input(input_seq, max_len)\n",
    "    input_tensor = torch.LongTensor([input_seq]).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        enc_emb = model.embedding_enc(input_tensor)\n",
    "        _, (h, c) = model.encoder(enc_emb)\n",
    "\n",
    "        # Utilisation des tokens sp√©ciaux de notre vocabulaire\n",
    "        start_token = output_vocab.get('<start>', 0)\n",
    "        end_token = output_vocab.get('<end>', 1)\n",
    "\n",
    "        sequences = [[list(), 0.0, h, c]]\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            all_candidates = []\n",
    "            for seq, score, h, c in sequences:\n",
    "                if seq and seq[-1] == end_token:\n",
    "                    all_candidates.append((seq, score, h, c))\n",
    "                    continue\n",
    "                current_token = torch.LongTensor([[seq[-1]] if seq else [start_token]]).to(device)\n",
    "                dec_emb = model.embedding_dec(current_token)\n",
    "                output, (h_new, c_new) = model.decoder(dec_emb, (h, c))\n",
    "                logits = model.fc(output[:, -1, :])\n",
    "                probs = F.log_softmax(logits, dim=-1)\n",
    "                topk_probs, topk_idxs = torch.topk(probs, beam_width)\n",
    "\n",
    "                for i in range(beam_width):\n",
    "                    candidate = seq + [topk_idxs[0, i].item()]\n",
    "                    candidate_score = score + topk_probs[0, i].item()\n",
    "                    all_candidates.append((candidate, candidate_score, h_new, c_new))\n",
    "\n",
    "            sequences = sorted(all_candidates, key=lambda tup: tup[1], reverse=True)[:beam_width]\n",
    "\n",
    "        best_seq = sequences[0][0]\n",
    "        inv_vocab = {v: k for k, v in output_vocab.items()}\n",
    "        decoded_words = [inv_vocab.get(tok, '') for tok in best_seq if tok != end_token]\n",
    "        return ' '.join(decoded_words)\n",
    "\n",
    "# Fonctions d'√©valuation\n",
    "def compute_bleu(reference, generated):\n",
    "    ref_tokens = reference.lower().split()\n",
    "    gen_tokens = generated.lower().split()\n",
    "    return sentence_bleu([ref_tokens], gen_tokens)\n",
    "\n",
    "# ROUGE\n",
    "rouge = Rouge()\n",
    "def compute_rouge(reference, generated):\n",
    "    return rouge.get_scores(generated, reference)[0]\n",
    "\n",
    "# Perplexit√© (approxim√©e)\n",
    "def compute_perplexity(model, input_seq, target_seq):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    input_tensor = torch.LongTensor([input_seq]).to(device)\n",
    "    target_tensor = torch.LongTensor([target_seq]).to(device)\n",
    "\n",
    "    decoder_input = target_tensor[:, :-1]\n",
    "    target_output = target_tensor[:, 1:]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_tensor, decoder_input)\n",
    "        logits = logits.view(-1, logits.shape[-1])\n",
    "        target_output = target_output.view(-1)\n",
    "        loss = F.cross_entropy(logits, target_output, ignore_index=0)\n",
    "\n",
    "    return math.exp(loss.item())\n",
    "\n",
    "print(\"‚úÖ Fonctions de g√©n√©ration corrig√©es charg√©es !\")\n",
    "\n",
    "# ============================================================================\n",
    "# TEST COMPLET AVEC VOTRE MOD√àLE\n",
    "# ============================================================================\n",
    "\n",
    "# Ajoutez cette cellule pour tester\n",
    "\n",
    "def test_complet():\n",
    "    print(\"=== Test complet des fonctions de g√©n√©ration ===\\n\")\n",
    "    \n",
    "    try:\n",
    "        # 1. Charger les vocabulaires\n",
    "        print(\"1. Chargement des vocabulaires...\")\n",
    "        with open(\"input_vocab.pkl\", \"rb\") as f:\n",
    "            input_vocab = pickle.load(f)\n",
    "        with open(\"output_vocab.pkl\", \"rb\") as f:\n",
    "            output_vocab = pickle.load(f)\n",
    "        print(f\"   ‚úì Input vocab size: {len(input_vocab)}\")\n",
    "        print(f\"   ‚úì Output vocab size: {len(output_vocab)}\")\n",
    "        \n",
    "        # 2. Charger le mod√®le\n",
    "        print(\"\\n2. Chargement du mod√®le...\")\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = Seq2SeqModel(len(input_vocab), len(output_vocab)).to(device)\n",
    "        model.load_state_dict(torch.load(\"emotion_seq2seq_lstm_pytorch.pt\"))\n",
    "        print(f\"   ‚úì Mod√®le charg√© sur {device}\")\n",
    "        \n",
    "        # 3. Test de g√©n√©ration\n",
    "        print(\"\\n3. Test de g√©n√©ration...\")\n",
    "        input_text = \"anger [SEP] Ce service est scandaleux !\"\n",
    "        reference_output = \"Je comprends votre frustration. Nous allons vous aider au plus vite.\"\n",
    "        \n",
    "        print(f\"   Input: {input_text}\")\n",
    "        print(f\"   Reference: {reference_output}\")\n",
    "        \n",
    "        # G√©n√©ration greedy\n",
    "        gen_greedy = generate_response_greedy(model, input_text, input_vocab, output_vocab)\n",
    "        print(f\"   Generated (Greedy): {gen_greedy}\")\n",
    "        \n",
    "        # G√©n√©ration beam search\n",
    "        gen_beam = generate_response_beam(model, input_text, input_vocab, output_vocab)\n",
    "        print(f\"   Generated (Beam): {gen_beam}\")\n",
    "        \n",
    "        # 4. √âvaluation\n",
    "        print(\"\\n4. √âvaluation...\")\n",
    "        bleu_score = compute_bleu(reference_output, gen_greedy)\n",
    "        rouge_score = compute_rouge(reference_output, gen_greedy)\n",
    "        \n",
    "        print(f\"   BLEU: {bleu_score:.4f}\")\n",
    "        print(f\"   ROUGE: {rouge_score}\")\n",
    "        \n",
    "        # 5. Test avec diff√©rentes √©motions\n",
    "        print(\"\\n5. Test avec diff√©rentes √©motions...\")\n",
    "        emotions_test = [\"joy\", \"sadness\", \"fear\", \"anger\"]\n",
    "        \n",
    "        for emotion in emotions_test:\n",
    "            test_input = f\"{emotion} [SEP] Test message\"\n",
    "            gen = generate_response_greedy(model, test_input, input_vocab, output_vocab)\n",
    "            print(f\"   {emotion}: {gen}\")\n",
    "        \n",
    "        print(\"\\n‚úÖ Test complet r√©ussi !\")\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"‚ùå Erreur: Fichier non trouv√© - {e}\")\n",
    "        print(\"   Assurez-vous que les fichiers input_vocab.pkl, output_vocab.pkl et emotion_seq2seq_lstm_pytorch.pt existent.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur: {e}\")\n",
    "        print(\"   V√©rifiez que tous les fichiers n√©cessaires sont pr√©sents.\")\n",
    "\n",
    "# Ex√©cutez cette fonction pour tester\n",
    "test_complet() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autres modeles GPT2 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\yann\\anaconda3\\envs\\hf_gpu_env\\lib\\site-packages (4.40.1)\n",
      "Requirement already satisfied: datasets in c:\\users\\yann\\anaconda3\\envs\\hf_gpu_env\\lib\\site-packages (3.6.0)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp310-cp310-win_amd64.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\yann\\anaconda3\\envs\\hf_gpu_env\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\yann\\anaconda3\\envs\\hf_gpu_env\\lib\\site-packages (from transformers) (0.23.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\yann\\anaconda3\\envs\\hf_gpu_env\\lib\\site-packages (from transformers) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\yann\\anaconda3\\envs\\hf_gpu_env\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\yann\\anaconda3\\envs\\hf_gpu_env\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\yann\\anaconda3\\envs\\hf_gpu_env\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\yann\\anaconda3\\envs\\hf_gpu_env\\lib\\site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\yann\\anaconda3\\envs\\hf_gpu_env\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\yann\\anaconda3\\envs\\hf_gpu_env\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\yann\\anaconda3\\envs\\hf_gpu_env\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\yann\\anaconda3\\envs\\hf_gpu_env\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\yann\\anaconda3\\envs\\hf_gpu_env\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.12.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\yann\\anaconda3\\envs\\hf_gpu_env\\lib\\site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\yann\\anaconda3\\envs\\hf_gpu_env\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\yann\\anaconda3\\envs\\hf_gpu_env\\lib\\site-packages (from datasets) (2.3.0)\n",
      "Requirement already satisfied: xxhash in c:\\users\\yann\\anaconda3\\envs\\hf_gpu_env\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\yann\\anaconda3\\envs\\hf_gpu_env\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Collecting huggingface-hub<1.0,>=0.19.3 (from transformers)\n",
      "  Using cached huggingface_hub-0.33.2-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\yann\\anaconda3\\envs\\hf_gpu_env\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.13)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\yann\\anaconda3\\envs\\hf_gpu_env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\yann\\anaconda3\\envs\\hf_gpu_env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\yann\\anaconda3\\envs\\hf_gpu_env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\yann\\anaconda3\\envs\\hf_gpu_env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\yann\\anaconda3\\envs\\hf_gpu_env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\yann\\anaconda3\\envs\\hf_gpu_env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\yann\\anaconda3\\envs\\hf_gpu_env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\yann\\anaconda3\\envs\\hf_gpu_env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\yann\\anaconda3\\envs\\hf_gpu_env\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\yann\\anaconda3\\envs\\hf_gpu_env\\lib\\site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\yann\\anaconda3\\envs\\hf_gpu_env\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\yann\\anaconda3\\envs\\hf_gpu_env\\lib\\site-packages (from requests->transformers) (2025.6.15)\n",
      "Requirement already satisfied: colorama in c:\\users\\yann\\anaconda3\\envs\\hf_gpu_env\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\yann\\anaconda3\\envs\\hf_gpu_env\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\yann\\anaconda3\\envs\\hf_gpu_env\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\yann\\anaconda3\\envs\\hf_gpu_env\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\yann\\anaconda3\\envs\\hf_gpu_env\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Using cached huggingface_hub-0.33.2-py3-none-any.whl (515 kB)\n",
      "Downloading sentencepiece-0.2.0-cp310-cp310-win_amd64.whl (991 kB)\n",
      "   ---------------------------------------- 0.0/991.5 kB ? eta -:--:--\n",
      "   ------------------------------- -------- 786.4/991.5 kB 5.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 991.5/991.5 kB 3.6 MB/s eta 0:00:00\n",
      "Installing collected packages: sentencepiece, huggingface-hub\n",
      "\n",
      "  Attempting uninstall: huggingface-hub\n",
      "\n",
      "    Found existing installation: huggingface-hub 0.23.1\n",
      "\n",
      "    Uninstalling huggingface-hub-0.23.1:\n",
      "\n",
      "      Successfully uninstalled huggingface-hub-0.23.1\n",
      "\n",
      "   -------------------- ------------------- 1/2 [huggingface-hub]\n",
      "   -------------------- ------------------- 1/2 [huggingface-hub]\n",
      "   -------------------- ------------------- 1/2 [huggingface-hub]\n",
      "   -------------------- ------------------- 1/2 [huggingface-hub]\n",
      "   -------------------- ------------------- 1/2 [huggingface-hub]\n",
      "   -------------------- ------------------- 1/2 [huggingface-hub]\n",
      "   -------------------- ------------------- 1/2 [huggingface-hub]\n",
      "   -------------------- ------------------- 1/2 [huggingface-hub]\n",
      "   -------------------- ------------------- 1/2 [huggingface-hub]\n",
      "   ---------------------------------------- 2/2 [huggingface-hub]\n",
      "\n",
      "Successfully installed huggingface-hub-0.33.2 sentencepiece-0.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sentence-transformers 5.0.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.40.1 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Yann\\anaconda3\\envs\\hf_gpu_env\\lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4178a9cb106b478db7e622c46e9042e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Yann\\anaconda3\\envs\\hf_gpu_env\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Yann\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31dc2e9a88ec4cfa9afaa591387e5b5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8d9c4696cfe4f238f13617c1b2b564c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bce1e91735047ada64871b377acf4d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "334c2d9fe3fc49b3964890b8ae9c62ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f40148f020d4432abb6076b2bddba1b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f53fdc6a9889415dbf9bf97a83c0352d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üó£Ô∏è G√©n√©r√©: Emotion: anger | Message: Ce service est scandaleux! | R√©ponse: te cette pr√®s √† ceux lass√©e | Message: c'est jusqu'il monde : d'un rapport l'appel de mots qui √©taient : si ne √©tait pas que la\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "\n",
    "# 1. Load model & tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Required for batching\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(\"cuda\")\n",
    "\n",
    "# 2. Define prompt\n",
    "emotion = \"anger\"\n",
    "input_text = \"Ce service est scandaleux !\"\n",
    "prompt = f\"Emotion: {emotion} | Message: {input_text} | R√©ponse:\"\n",
    "\n",
    "# 3. Tokenize input\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# 4. Generate\n",
    "output = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=50,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    temperature=0.9,\n",
    "    num_return_sequences=1\n",
    ")\n",
    "\n",
    "# 5. Decode\n",
    "response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\"üó£Ô∏è G√©n√©r√©:\", response.replace(prompt, \"\").strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers nltk rouge-score --quiet\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Yann\\anaconda3\\envs\\hf_gpu_env\\lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                                    output  \\\n",
      "0                                                   Emotion: anger | Message: Ce service est scandaleux! | R√©ponse: √©tait en d√©clarement, ces compte dans les autres et √©t√© parle √©t√© parle √† la t√©l√©phonic de l'√©ducation des √©t√© parle a   \n",
      "1                                                                                            Emotion: anger | Message: Ce service est scandaleux! | R√©ponse: dans l'universit√© de l'honneur et l'honneur des √©lectories de l'√©lectories en   \n",
      "2                                                                  Emotion: anger | Message: Ce service est scandaleux! | R√©ponse: Le jeunesse des seux pour une service en √©quip√© du service de nombre sur la service dans cette service!   \n",
      "3                                                                                                       Emotion: anger | Message: Ce service est scandaleux! | R√©ponse: ce ce ce qu'est pas de ce qu'√† cette s'accouvoir d'une est en fait   \n",
      "4                                                                                                              Emotion: anger | Message: Ce service est scandaleux! | R√©ponse: √©lle vueux l√† de l√†, √† s'il vueux dans un mieux √©tait des s   \n",
      "5                                                            Emotion: anger | Message: Ce service est scandaleux! | R√©ponse: ¬´Le vive un fois ¬ª (1) √† cette fois, √† ces sont enfants de l'attend√©, √† l'attend√©, que vous lui √† ces sont en   \n",
      "6                                                               Emotion: anger | Message: Ce service est scandaleux! | R√©ponse: dans l'universelle de l'esprit d'universelle √† l'esprit! | | | | | | | | | | | | | | | | | | | | | | | | |   \n",
      "7                                                   Emotion: anger | Message: Ce service est scandaleux! | R√©ponse: √©pisode dans le monde | Subject: Re: [Ce service est scandaleux] | | | | | | | | | | | | | | | | | | | | | | | | | | |   \n",
      "8                                                                                                          Emotion: anger | Message: Ce service est scandaleux! | R√©ponse: http://mail.jazz.com/jazz-lounge/jazz-lounge-taste-soup-and-che   \n",
      "9                                                        Emotion: anger | Message: Ce service est scandaleux! | R√©ponse: mais de l'attentat de vivre des m√™me de ce m√™me √† la vie de l'attentat de vivre des m√™me de ce m√™me √† la vie de l   \n",
      "10                                                                                                 Emotion: anger | Message: Ce service est scandaleux! | R√©ponse: P√©d√©ration de la cosa de la cosa de la cosa de la cosa! | | | | | | | |   \n",
      "11                                                                   Emotion: anger | Message: Ce service est scandaleux! | R√©ponse: [Ce service est scandaleux] | Message: Ce service est scandaleux est scandaleux | Message: Ce service   \n",
      "12              Emotion: anger | Message: Ce service est scandaleux! | R√©ponse: \"I am sorry for your loss, but I am not going to forgive you.\" | Reply: ~~~~~\\n\\nMikael\\n\\nMember\\n\\njoin:2005-04-03 Mikael Member Re: I am sorry for your   \n",
      "13                                                                                                    Emotion: anger | Message: Ce service est scandaleux! | R√©ponse: √©pendant √† leur √©t√© de la r√®gle.\\n\\nIn your case, you're a good guy.   \n",
      "14  Emotion: anger | Message: Ce service est scandaleux! | R√©ponse: Ce est environnement pour la service! | Message: C'est la service! | Message: C'est la service! | Message: C'est la service! | Message: C'est la service! | Message: C   \n",
      "15                                                                                            Emotion: anger | Message: Ce service est scandaleux! | R√©ponse: √©t√© √† l'affaire des m√©decins | Message: √©crit pas de l'affaire des m√©decins!   \n",
      "16             Emotion: anger | Message: Ce service est scandaleux! | R√©ponse: te si je me c'est, c'est la vie! | Resume: c'est la vie!\\n\\nAfter we had had a little discussion we went on our way, making sure that we had a good deal of   \n",
      "17                                         Emotion: anger | Message: Ce service est scandaleux! | R√©ponse: I think you've got it. | Reply: 30 Dec 2010, 11:33:28 [12] | Reply: 29 Dec 2010, 12:35:27 [13] | Reply: 28 Dec 2010, 12:34:35 [   \n",
      "18                                                                                                   Emotion: anger | Message: Ce service est scandaleux! | R√©ponse: C√©saire de la bibliqua, la biblibilit√©.  ~~ ~~ ~~\\n\\nA little after 5   \n",
      "19                        Emotion: anger | Message: Ce service est scandaleux! | R√©ponse:\\n\\n[#2] : The 'trend' of the \"fantasy\" (and not-so-fantasy-like 'fantasy') is the first phase of 'emotionality': there has to be a point of view   \n",
      "20                                                                                            Emotion: anger | Message: Ce service est scandaleux! | R√©ponse: 'Je vivant d√©j√† vous se ret√™te des pr√©sentations de ces pr√©sents. | Vermaire   \n",
      "21                                Emotion: anger | Message: Ce service est scandaleux! | R√©ponse: [1] [5] |\\n\\nThe last page of my paper is available here\\n\\nThe next post is in a longer version which I've added to the list of topics.   \n",
      "22                                                                                             Emotion: anger | Message: Ce service est scandaleux! | R√©ponse: Piquet √† t√™te pas √Ætre est ses ennui pas √† faire que son faecre ou des neuf   \n",
      "23                                                 Emotion: anger | Message: Ce service est scandaleux! | R√©ponse: ce service est scandaleux! | Conferenc√©s: √©me poussin!\\n\\nRe: B. S. B., The Royal Service | [SIP/BMC: KIC 29010101 ] (K   \n",
      "\n",
      "    temperature  top_k  top_p  max_new_tokens    bleu  rouge-1  rouge-2  \\\n",
      "0           0.9     20   0.95              50  0.0214   0.1600   0.0417   \n",
      "1           0.7     20   0.95              30  0.0149   0.1000   0.0000   \n",
      "2           0.9     50   0.95              30  0.0114   0.0909   0.0000   \n",
      "3           0.9     50   0.80              30  0.0112   0.0952   0.0000   \n",
      "4           1.2     20   0.80              30  0.0112   0.0513   0.0000   \n",
      "5           0.7     50   0.95              50  0.0084   0.0435   0.0000   \n",
      "6           0.9     20   0.80              50  0.0070   0.1143   0.0000   \n",
      "7           0.7     20   0.95              50  0.0063   0.0571   0.0000   \n",
      "8           0.7     50   0.95              30  0.0000   0.0000   0.0000   \n",
      "9           0.7     50   0.80              50  0.0000   0.0000   0.0000   \n",
      "10          0.7     50   0.80              30  0.0000   0.0500   0.0000   \n",
      "11          0.7     20   0.80              30  0.0000   0.0000   0.0000   \n",
      "12          0.7     20   0.80              50  0.0000   0.0000   0.0000   \n",
      "13          0.9     20   0.95              30  0.0000   0.0000   0.0000   \n",
      "14          0.9     50   0.80              50  0.0000   0.0000   0.0000   \n",
      "15          0.9     20   0.80              30  0.0000   0.0000   0.0000   \n",
      "16          0.9     50   0.95              50  0.0000   0.0328   0.0000   \n",
      "17          1.2     20   0.80              50  0.0000   0.0000   0.0000   \n",
      "18          1.2     20   0.95              30  0.0000   0.0000   0.0000   \n",
      "19          1.2     20   0.95              50  0.0000   0.0000   0.0000   \n",
      "20          1.2     50   0.80              30  0.0000   0.0976   0.0000   \n",
      "21          1.2     50   0.80              50  0.0000   0.0000   0.0000   \n",
      "22          1.2     50   0.95              30  0.0000   0.0000   0.0000   \n",
      "23          1.2     50   0.95              50  0.0000   0.0000   0.0000   \n",
      "\n",
      "    rouge-L  perplexity  \n",
      "0    0.1200     33.2226  \n",
      "1    0.0500     39.2486  \n",
      "2    0.0909     78.9210  \n",
      "3    0.0476     83.5893  \n",
      "4    0.0513     71.6970  \n",
      "5    0.0435     31.9104  \n",
      "6    0.1143     13.4356  \n",
      "7    0.0571     12.8105  \n",
      "8    0.0000     46.8526  \n",
      "9    0.0000     16.5168  \n",
      "10   0.0500     34.4234  \n",
      "11   0.0000     21.5124  \n",
      "12   0.0000     17.8372  \n",
      "13   0.0000     64.5148  \n",
      "14   0.0000     13.3734  \n",
      "15   0.0000     39.5195  \n",
      "16   0.0328     33.8519  \n",
      "17   0.0000     19.7497  \n",
      "18   0.0000    104.0261  \n",
      "19   0.0000     47.7955  \n",
      "20   0.0976     73.1169  \n",
      "21   0.0000     55.7357  \n",
      "22   0.0000    121.4807  \n",
      "23   0.0000     75.5174  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "# === Chargement du mod√®le ===\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(\"cuda\")\n",
    "\n",
    "# === R√©f√©rence & prompt ===\n",
    "emotion = \"anger\"\n",
    "input_text = \"Ce service est scandaleux !\"\n",
    "prompt = f\"Emotion: {emotion} | Message: {input_text} | R√©ponse:\"\n",
    "reference = \"Je comprends votre frustration. Nous prenons en charge votre demande dans les plus brefs d√©lais.\"\n",
    "\n",
    "# === Hyperparam√®tres √† tester ===\n",
    "temperatures = [0.7, 0.9, 1.2]\n",
    "top_ks = [20, 50]\n",
    "top_ps = [0.8, 0.95]\n",
    "max_tokens_list = [30, 50]\n",
    "\n",
    "# === Scorers ===\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "smoothie = SmoothingFunction().method4\n",
    "\n",
    "# === Benchmarking ===\n",
    "results = []\n",
    "\n",
    "for temp in temperatures:\n",
    "    for top_k in top_ks:\n",
    "        for top_p in top_ps:\n",
    "            for max_tok in max_tokens_list:\n",
    "                inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "                output_ids = model.generate(\n",
    "                    **inputs,\n",
    "                    do_sample=True,\n",
    "                    top_k=top_k,\n",
    "                    top_p=top_p,\n",
    "                    temperature=temp,\n",
    "                    max_new_tokens=max_tok,\n",
    "                    num_return_sequences=1\n",
    "                )\n",
    "\n",
    "                output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True).replace(prompt, \"\").strip()\n",
    "\n",
    "                # BLEU\n",
    "                bleu = sentence_bleu([reference.split()], output_text.split(), smoothing_function=smoothie)\n",
    "\n",
    "                # ROUGE\n",
    "                rouge = scorer.score(reference, output_text)\n",
    "\n",
    "                # Perplexity\n",
    "                encodings = tokenizer(output_text, return_tensors=\"pt\")\n",
    "                input_ids = encodings.input_ids.to(\"cuda\")\n",
    "                with torch.no_grad():\n",
    "                    loss = model(input_ids, labels=input_ids).loss\n",
    "                perplexity = math.exp(loss.item()) if loss.item() < 100 else float('inf')\n",
    "\n",
    "                results.append({\n",
    "                    \"output\": output_text,\n",
    "                    \"temperature\": temp,\n",
    "                    \"top_k\": top_k,\n",
    "                    \"top_p\": top_p,\n",
    "                    \"max_new_tokens\": max_tok,\n",
    "                    \"bleu\": round(bleu, 4),\n",
    "                    \"rouge-1\": round(rouge['rouge1'].fmeasure, 4),\n",
    "                    \"rouge-2\": round(rouge['rouge2'].fmeasure, 4),\n",
    "                    \"rouge-L\": round(rouge['rougeL'].fmeasure, 4),\n",
    "                    \"perplexity\": round(perplexity, 4)\n",
    "                })\n",
    "\n",
    "# === R√©sultats sous forme de DataFrame ===\n",
    "df_results = pd.DataFrame(results)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "print(df_results.sort_values(by=\"bleu\", ascending=False).reset_index(drop=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    temperature  top_k  top_p  max_new_tokens    bleu  rouge-1  rouge-2  \\\n",
      "0           0.7     20   0.80              30  0.0000   0.0000   0.0000   \n",
      "1           0.7     20   0.80              50  0.0000   0.0000   0.0000   \n",
      "2           0.7     20   0.95              30  0.0149   0.1000   0.0000   \n",
      "3           0.7     20   0.95              50  0.0063   0.0571   0.0000   \n",
      "4           0.7     50   0.80              30  0.0000   0.0500   0.0000   \n",
      "5           0.7     50   0.80              50  0.0000   0.0000   0.0000   \n",
      "6           0.7     50   0.95              30  0.0000   0.0000   0.0000   \n",
      "7           0.7     50   0.95              50  0.0084   0.0435   0.0000   \n",
      "8           0.9     20   0.80              30  0.0000   0.0000   0.0000   \n",
      "9           0.9     20   0.80              50  0.0070   0.1143   0.0000   \n",
      "10          0.9     20   0.95              30  0.0000   0.0000   0.0000   \n",
      "11          0.9     20   0.95              50  0.0214   0.1600   0.0417   \n",
      "12          0.9     50   0.80              30  0.0112   0.0952   0.0000   \n",
      "13          0.9     50   0.80              50  0.0000   0.0000   0.0000   \n",
      "14          0.9     50   0.95              30  0.0114   0.0909   0.0000   \n",
      "15          0.9     50   0.95              50  0.0000   0.0328   0.0000   \n",
      "16          1.2     20   0.80              30  0.0112   0.0513   0.0000   \n",
      "17          1.2     20   0.80              50  0.0000   0.0000   0.0000   \n",
      "18          1.2     20   0.95              30  0.0000   0.0000   0.0000   \n",
      "19          1.2     20   0.95              50  0.0000   0.0000   0.0000   \n",
      "20          1.2     50   0.80              30  0.0000   0.0976   0.0000   \n",
      "21          1.2     50   0.80              50  0.0000   0.0000   0.0000   \n",
      "22          1.2     50   0.95              30  0.0000   0.0000   0.0000   \n",
      "23          1.2     50   0.95              50  0.0000   0.0000   0.0000   \n",
      "\n",
      "    rouge-L  perplexity  \\\n",
      "0    0.0000     21.5124   \n",
      "1    0.0000     17.8372   \n",
      "2    0.0500     39.2486   \n",
      "3    0.0571     12.8105   \n",
      "4    0.0500     34.4234   \n",
      "5    0.0000     16.5168   \n",
      "6    0.0000     46.8526   \n",
      "7    0.0435     31.9104   \n",
      "8    0.0000     39.5195   \n",
      "9    0.1143     13.4356   \n",
      "10   0.0000     64.5148   \n",
      "11   0.1200     33.2226   \n",
      "12   0.0476     83.5893   \n",
      "13   0.0000     13.3734   \n",
      "14   0.0909     78.9210   \n",
      "15   0.0328     33.8519   \n",
      "16   0.0513     71.6970   \n",
      "17   0.0000     19.7497   \n",
      "18   0.0000    104.0261   \n",
      "19   0.0000     47.7955   \n",
      "20   0.0976     73.1169   \n",
      "21   0.0000     55.7357   \n",
      "22   0.0000    121.4807   \n",
      "23   0.0000     75.5174   \n",
      "\n",
      "                                                                                                                                                                                                                                    output  \n",
      "0                                                                    Emotion: anger | Message: Ce service est scandaleux! | R√©ponse: [Ce service est scandaleux] | Message: Ce service est scandaleux est scandaleux | Message: Ce service  \n",
      "1               Emotion: anger | Message: Ce service est scandaleux! | R√©ponse: \"I am sorry for your loss, but I am not going to forgive you.\" | Reply: ~~~~~\\n\\nMikael\\n\\nMember\\n\\njoin:2005-04-03 Mikael Member Re: I am sorry for your  \n",
      "2                                                                                            Emotion: anger | Message: Ce service est scandaleux! | R√©ponse: dans l'universit√© de l'honneur et l'honneur des √©lectories de l'√©lectories en  \n",
      "3                                                   Emotion: anger | Message: Ce service est scandaleux! | R√©ponse: √©pisode dans le monde | Subject: Re: [Ce service est scandaleux] | | | | | | | | | | | | | | | | | | | | | | | | | | |  \n",
      "4                                                                                                  Emotion: anger | Message: Ce service est scandaleux! | R√©ponse: P√©d√©ration de la cosa de la cosa de la cosa de la cosa! | | | | | | | |  \n",
      "5                                                        Emotion: anger | Message: Ce service est scandaleux! | R√©ponse: mais de l'attentat de vivre des m√™me de ce m√™me √† la vie de l'attentat de vivre des m√™me de ce m√™me √† la vie de l  \n",
      "6                                                                                                          Emotion: anger | Message: Ce service est scandaleux! | R√©ponse: http://mail.jazz.com/jazz-lounge/jazz-lounge-taste-soup-and-che  \n",
      "7                                                            Emotion: anger | Message: Ce service est scandaleux! | R√©ponse: ¬´Le vive un fois ¬ª (1) √† cette fois, √† ces sont enfants de l'attend√©, √† l'attend√©, que vous lui √† ces sont en  \n",
      "8                                                                                             Emotion: anger | Message: Ce service est scandaleux! | R√©ponse: √©t√© √† l'affaire des m√©decins | Message: √©crit pas de l'affaire des m√©decins!  \n",
      "9                                                               Emotion: anger | Message: Ce service est scandaleux! | R√©ponse: dans l'universelle de l'esprit d'universelle √† l'esprit! | | | | | | | | | | | | | | | | | | | | | | | | |  \n",
      "10                                                                                                    Emotion: anger | Message: Ce service est scandaleux! | R√©ponse: √©pendant √† leur √©t√© de la r√®gle.\\n\\nIn your case, you're a good guy.  \n",
      "11                                                  Emotion: anger | Message: Ce service est scandaleux! | R√©ponse: √©tait en d√©clarement, ces compte dans les autres et √©t√© parle √©t√© parle √† la t√©l√©phonic de l'√©ducation des √©t√© parle a  \n",
      "12                                                                                                      Emotion: anger | Message: Ce service est scandaleux! | R√©ponse: ce ce ce qu'est pas de ce qu'√† cette s'accouvoir d'une est en fait  \n",
      "13  Emotion: anger | Message: Ce service est scandaleux! | R√©ponse: Ce est environnement pour la service! | Message: C'est la service! | Message: C'est la service! | Message: C'est la service! | Message: C'est la service! | Message: C  \n",
      "14                                                                 Emotion: anger | Message: Ce service est scandaleux! | R√©ponse: Le jeunesse des seux pour une service en √©quip√© du service de nombre sur la service dans cette service!  \n",
      "15             Emotion: anger | Message: Ce service est scandaleux! | R√©ponse: te si je me c'est, c'est la vie! | Resume: c'est la vie!\\n\\nAfter we had had a little discussion we went on our way, making sure that we had a good deal of  \n",
      "16                                                                                                             Emotion: anger | Message: Ce service est scandaleux! | R√©ponse: √©lle vueux l√† de l√†, √† s'il vueux dans un mieux √©tait des s  \n",
      "17                                         Emotion: anger | Message: Ce service est scandaleux! | R√©ponse: I think you've got it. | Reply: 30 Dec 2010, 11:33:28 [12] | Reply: 29 Dec 2010, 12:35:27 [13] | Reply: 28 Dec 2010, 12:34:35 [  \n",
      "18                                                                                                   Emotion: anger | Message: Ce service est scandaleux! | R√©ponse: C√©saire de la bibliqua, la biblibilit√©.  ~~ ~~ ~~\\n\\nA little after 5  \n",
      "19                        Emotion: anger | Message: Ce service est scandaleux! | R√©ponse:\\n\\n[#2] : The 'trend' of the \"fantasy\" (and not-so-fantasy-like 'fantasy') is the first phase of 'emotionality': there has to be a point of view  \n",
      "20                                                                                            Emotion: anger | Message: Ce service est scandaleux! | R√©ponse: 'Je vivant d√©j√† vous se ret√™te des pr√©sentations de ces pr√©sents. | Vermaire  \n",
      "21                                Emotion: anger | Message: Ce service est scandaleux! | R√©ponse: [1] [5] |\\n\\nThe last page of my paper is available here\\n\\nThe next post is in a longer version which I've added to the list of topics.  \n",
      "22                                                                                             Emotion: anger | Message: Ce service est scandaleux! | R√©ponse: Piquet √† t√™te pas √Ætre est ses ennui pas √† faire que son faecre ou des neuf  \n",
      "23                                                 Emotion: anger | Message: Ce service est scandaleux! | R√©ponse: ce service est scandaleux! | Conferenc√©s: √©me poussin!\\n\\nRe: B. S. B., The Royal Service | [SIP/BMC: KIC 29010101 ] (K  \n"
     ]
    }
   ],
   "source": [
    "print(df_results[[\"temperature\", \"top_k\", \"top_p\", \"max_new_tokens\", \"bleu\", \"rouge-1\", \"rouge-2\", \"rouge-L\", \"perplexity\", \"output\"]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse du Benchmark de G√©n√©ration GPT-2\n",
    "\n",
    "## Contexte\n",
    "\n",
    "- **Prompt test√© :** `Emotion: anger | Message: Ce service est scandaleux !`\n",
    "- **Mod√®le utilis√© :** GPT2\n",
    "- **Objectif :** G√©n√©rer une r√©ponse empathique en fonction de l'√©motion via prompting\n",
    "- **M√©triques analys√©es :** BLEU, ROUGE-1, ROUGE-2, avec variations sur `temperature`, `top_k`, `top_p`, `max_new_tokens`\n",
    "\n",
    "## Observations Cl√©s\n",
    "\n",
    "| Param√®tre          | Impact |\n",
    "|--------------------|--------|\n",
    "| **Temperature**    | Des temp√©ratures entre `0.8` et `1.0` semblent g√©n√©rer un contenu plus vari√©, avec de meilleurs scores BLEU et ROUGE. En revanche, √† `1.2`, les g√©n√©rations deviennent plus erratiques voire non coh√©rentes. |\n",
    "| **top_k / top_p**  | Des valeurs `top_k = 50` et `top_p = 0.95` donnent de meilleurs r√©sultats, surtout combin√©es avec des temp√©ratures mod√©r√©es. Des `top_k = 20` limitent parfois trop la diversit√©. |\n",
    "| **max_new_tokens** | Des s√©quences de 50 tokens permettent des r√©ponses plus compl√®tes, ce qui am√©liore l√©g√®rement les scores ROUGE. |\n",
    "\n",
    "## Meilleures Configurations Rep√©r√©es\n",
    "\n",
    "- `temperature = 0.9`, `top_k = 50`, `top_p = 0.95`, `max_new_tokens = 50`\n",
    "  - **ROUGE-1 ‚âà 0.16**\n",
    "  - **BLEU ‚âà 0.021**\n",
    "\n",
    "C‚Äôest le meilleur score observ√©, mais cela reste faible. Le mod√®le GPT-2 vanilla a du mal √† g√©n√©rer des r√©ponses adapt√©es sans fine-tuning.\n",
    "\n",
    "## Limites Identifi√©es\n",
    "\n",
    "- **BLEU = 0.0 dans la majorit√© des cas** : Le mod√®le n‚Äôaligne pas bien ses r√©ponses avec la r√©f√©rence, m√™me s‚Äôil reste parfois dans le th√®me.\n",
    "- **R√©ponses incoh√©rentes** : certaines g√©n√©rations contiennent des phrases en anglais ou hors sujet, malgr√© un prompting en fran√ßais clair.\n",
    "- **R√©ponses trop g√©n√©riques ou recycl√©es** : manque de sp√©cialisation du mod√®le pour la t√¢che cibl√©e.\n",
    "\n",
    "## Recommandations\n",
    "\n",
    "- Fine-tuner GPT-2 ou utiliser un mod√®le T5/BART sur ton propre CSV pour am√©liorer la pertinence.\n",
    "- Utiliser une strat√©gie de reranking ou filtrage s√©mantique post-g√©n√©ration pour am√©liorer la qualit√© finale.\n",
    "- Ajouter la perplexit√© moyenne √† l‚Äôanalyse pour identifier les sorties les plus fiables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning de GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_gpt2_dataset(df):\n",
    "    return [\n",
    "        f\"Emotion: {row['emotion']} | Message: {row['text_input']} | R√©ponse: {row['text_output']}\"\n",
    "        for _, row in df.iterrows()\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Yann\\anaconda3\\envs\\hf_gpu_env\\lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Yann\\anaconda3\\envs\\hf_gpu_env\\lib\\site-packages\\transformers\\data\\datasets\\language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ü§ó Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n",
      "c:\\Users\\Yann\\anaconda3\\envs\\hf_gpu_env\\lib\\site-packages\\accelerate\\accelerator.py:469: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72a036cbe1ce412497df64cafc193e1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25719 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0345, 'grad_norm': 7.854506015777588, 'learning_rate': 4.981142346125433e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4673, 'grad_norm': 7.5281853675842285, 'learning_rate': 4.9617014658423736e-05, 'epoch': 0.02}\n",
      "{'loss': 0.285, 'grad_norm': 4.004764080047607, 'learning_rate': 4.942260585559315e-05, 'epoch': 0.03}\n",
      "{'loss': 0.2132, 'grad_norm': 5.3351335525512695, 'learning_rate': 4.922819705276255e-05, 'epoch': 0.05}\n",
      "{'loss': 0.172, 'grad_norm': 4.28413200378418, 'learning_rate': 4.9033788249931964e-05, 'epoch': 0.06}\n",
      "{'loss': 0.1403, 'grad_norm': 3.492551326751709, 'learning_rate': 4.883937944710137e-05, 'epoch': 0.07}\n",
      "{'loss': 0.1226, 'grad_norm': 3.7754299640655518, 'learning_rate': 4.8644970644270774e-05, 'epoch': 0.08}\n",
      "{'loss': 0.1097, 'grad_norm': 2.952648401260376, 'learning_rate': 4.8450561841440185e-05, 'epoch': 0.09}\n",
      "{'loss': 0.1096, 'grad_norm': 2.2080702781677246, 'learning_rate': 4.825615303860959e-05, 'epoch': 0.1}\n",
      "{'loss': 0.0982, 'grad_norm': 3.959505081176758, 'learning_rate': 4.8061744235779e-05, 'epoch': 0.12}\n",
      "{'loss': 0.0976, 'grad_norm': 3.187751293182373, 'learning_rate': 4.7867335432948406e-05, 'epoch': 0.13}\n",
      "{'loss': 0.0932, 'grad_norm': 2.531838893890381, 'learning_rate': 4.767292663011781e-05, 'epoch': 0.14}\n",
      "{'loss': 0.0925, 'grad_norm': 2.3562910556793213, 'learning_rate': 4.747851782728722e-05, 'epoch': 0.15}\n",
      "{'loss': 0.0893, 'grad_norm': 3.9998562335968018, 'learning_rate': 4.728410902445663e-05, 'epoch': 0.16}\n",
      "{'loss': 0.0887, 'grad_norm': 2.1515493392944336, 'learning_rate': 4.708970022162604e-05, 'epoch': 0.17}\n",
      "{'loss': 0.0823, 'grad_norm': 2.0277516841888428, 'learning_rate': 4.6895291418795444e-05, 'epoch': 0.19}\n",
      "{'loss': 0.0898, 'grad_norm': 2.591653347015381, 'learning_rate': 4.6700882615964856e-05, 'epoch': 0.2}\n",
      "{'loss': 0.1187, 'grad_norm': 2.61722993850708, 'learning_rate': 4.6508417901162564e-05, 'epoch': 0.21}\n",
      "{'loss': 0.0805, 'grad_norm': 2.038360357284546, 'learning_rate': 4.6314009098331976e-05, 'epoch': 0.22}\n",
      "{'loss': 0.0816, 'grad_norm': 3.5894007682800293, 'learning_rate': 4.611960029550138e-05, 'epoch': 0.23}\n",
      "{'loss': 0.0788, 'grad_norm': 1.5246126651763916, 'learning_rate': 4.5925191492670785e-05, 'epoch': 0.24}\n",
      "{'loss': 0.0812, 'grad_norm': 1.5698148012161255, 'learning_rate': 4.57307826898402e-05, 'epoch': 0.26}\n",
      "{'loss': 0.0774, 'grad_norm': 2.9107720851898193, 'learning_rate': 4.55363738870096e-05, 'epoch': 0.27}\n",
      "{'loss': 0.0781, 'grad_norm': 1.952919602394104, 'learning_rate': 4.534196508417901e-05, 'epoch': 0.28}\n",
      "{'loss': 0.0774, 'grad_norm': 3.7357194423675537, 'learning_rate': 4.5147556281348425e-05, 'epoch': 0.29}\n",
      "{'loss': 0.075, 'grad_norm': 1.841217041015625, 'learning_rate': 4.495314747851783e-05, 'epoch': 0.3}\n",
      "{'loss': 0.073, 'grad_norm': 3.0490524768829346, 'learning_rate': 4.475873867568724e-05, 'epoch': 0.31}\n",
      "{'loss': 0.0754, 'grad_norm': 2.238063097000122, 'learning_rate': 4.4564329872856646e-05, 'epoch': 0.33}\n",
      "{'loss': 0.0737, 'grad_norm': 2.425481081008911, 'learning_rate': 4.436992107002606e-05, 'epoch': 0.34}\n",
      "{'loss': 0.0729, 'grad_norm': 1.8912039995193481, 'learning_rate': 4.417551226719546e-05, 'epoch': 0.35}\n",
      "{'loss': 0.0889, 'grad_norm': 2.8604683876037598, 'learning_rate': 4.398304755239318e-05, 'epoch': 0.36}\n",
      "{'loss': 0.0768, 'grad_norm': 1.6340761184692383, 'learning_rate': 4.378863874956258e-05, 'epoch': 0.37}\n",
      "{'loss': 0.0683, 'grad_norm': 2.722515821456909, 'learning_rate': 4.359422994673199e-05, 'epoch': 0.38}\n",
      "{'loss': 0.0684, 'grad_norm': 1.309323787689209, 'learning_rate': 4.33998211439014e-05, 'epoch': 0.4}\n",
      "{'loss': 0.0681, 'grad_norm': 1.111762285232544, 'learning_rate': 4.3205412341070804e-05, 'epoch': 0.41}\n",
      "{'loss': 0.0709, 'grad_norm': 2.5449182987213135, 'learning_rate': 4.3011003538240215e-05, 'epoch': 0.42}\n",
      "{'loss': 0.0653, 'grad_norm': 4.273569107055664, 'learning_rate': 4.281659473540962e-05, 'epoch': 0.43}\n",
      "{'loss': 0.0653, 'grad_norm': 1.75690495967865, 'learning_rate': 4.2622185932579025e-05, 'epoch': 0.44}\n",
      "{'loss': 0.0681, 'grad_norm': 2.555504322052002, 'learning_rate': 4.242777712974844e-05, 'epoch': 0.45}\n",
      "{'loss': 0.0668, 'grad_norm': 1.725535273551941, 'learning_rate': 4.223336832691784e-05, 'epoch': 0.47}\n",
      "{'loss': 0.0661, 'grad_norm': 1.3622159957885742, 'learning_rate': 4.203895952408725e-05, 'epoch': 0.48}\n",
      "{'loss': 0.0669, 'grad_norm': 1.7655938863754272, 'learning_rate': 4.184455072125666e-05, 'epoch': 0.49}\n",
      "{'loss': 0.068, 'grad_norm': 2.6725425720214844, 'learning_rate': 4.165014191842607e-05, 'epoch': 0.5}\n",
      "{'loss': 0.0644, 'grad_norm': 3.085331678390503, 'learning_rate': 4.1455733115595474e-05, 'epoch': 0.51}\n",
      "{'loss': 0.0646, 'grad_norm': 1.6167628765106201, 'learning_rate': 4.126132431276488e-05, 'epoch': 0.52}\n",
      "{'loss': 0.0657, 'grad_norm': 1.9285541772842407, 'learning_rate': 4.106691550993429e-05, 'epoch': 0.54}\n",
      "{'loss': 0.0644, 'grad_norm': 2.9362497329711914, 'learning_rate': 4.0872506707103696e-05, 'epoch': 0.55}\n",
      "{'loss': 0.064, 'grad_norm': 1.840004801750183, 'learning_rate': 4.067809790427311e-05, 'epoch': 0.56}\n",
      "{'loss': 0.0668, 'grad_norm': 1.9830418825149536, 'learning_rate': 4.048368910144252e-05, 'epoch': 0.57}\n",
      "{'loss': 0.0647, 'grad_norm': 1.9066660404205322, 'learning_rate': 4.0289280298611924e-05, 'epoch': 0.58}\n",
      "{'loss': 0.0597, 'grad_norm': 1.3824156522750854, 'learning_rate': 4.0094871495781335e-05, 'epoch': 0.59}\n",
      "{'loss': 0.0644, 'grad_norm': 1.7068933248519897, 'learning_rate': 3.990046269295074e-05, 'epoch': 0.61}\n",
      "{'loss': 0.0632, 'grad_norm': 1.452319622039795, 'learning_rate': 3.9706053890120145e-05, 'epoch': 0.62}\n",
      "{'loss': 0.0608, 'grad_norm': 1.4649626016616821, 'learning_rate': 3.951164508728956e-05, 'epoch': 0.63}\n",
      "{'loss': 0.0622, 'grad_norm': 1.7518153190612793, 'learning_rate': 3.931723628445896e-05, 'epoch': 0.64}\n",
      "{'loss': 0.0614, 'grad_norm': 1.3118796348571777, 'learning_rate': 3.912282748162837e-05, 'epoch': 0.65}\n",
      "{'loss': 0.067, 'grad_norm': 1.0159807205200195, 'learning_rate': 3.892841867879778e-05, 'epoch': 0.66}\n",
      "{'loss': 0.064, 'grad_norm': 1.3405325412750244, 'learning_rate': 3.873400987596719e-05, 'epoch': 0.68}\n",
      "{'loss': 0.0607, 'grad_norm': 1.1736301183700562, 'learning_rate': 3.8539601073136594e-05, 'epoch': 0.69}\n",
      "{'loss': 0.0631, 'grad_norm': 3.1304354667663574, 'learning_rate': 3.8345192270306e-05, 'epoch': 0.7}\n",
      "{'loss': 0.061, 'grad_norm': 1.4237881898880005, 'learning_rate': 3.815078346747541e-05, 'epoch': 0.71}\n",
      "{'loss': 0.0639, 'grad_norm': 1.7242541313171387, 'learning_rate': 3.7956374664644816e-05, 'epoch': 0.72}\n",
      "{'loss': 0.0595, 'grad_norm': 1.4228875637054443, 'learning_rate': 3.776196586181423e-05, 'epoch': 0.73}\n",
      "{'loss': 0.0611, 'grad_norm': 1.3549169301986694, 'learning_rate': 3.756755705898363e-05, 'epoch': 0.75}\n",
      "{'loss': 0.0585, 'grad_norm': 1.9929876327514648, 'learning_rate': 3.737314825615304e-05, 'epoch': 0.76}\n",
      "{'loss': 0.0603, 'grad_norm': 0.7792428135871887, 'learning_rate': 3.717873945332245e-05, 'epoch': 0.77}\n",
      "{'loss': 0.062, 'grad_norm': 1.7730575799942017, 'learning_rate': 3.6984330650491854e-05, 'epoch': 0.78}\n",
      "{'loss': 0.06, 'grad_norm': 2.049724578857422, 'learning_rate': 3.6789921847661265e-05, 'epoch': 0.79}\n",
      "{'loss': 0.0595, 'grad_norm': 2.8822858333587646, 'learning_rate': 3.659551304483067e-05, 'epoch': 0.8}\n",
      "{'loss': 0.0572, 'grad_norm': 1.9522734880447388, 'learning_rate': 3.6401104242000075e-05, 'epoch': 0.82}\n",
      "{'loss': 0.0593, 'grad_norm': 1.0064674615859985, 'learning_rate': 3.6206695439169486e-05, 'epoch': 0.83}\n",
      "{'loss': 0.0602, 'grad_norm': 1.9523173570632935, 'learning_rate': 3.601228663633889e-05, 'epoch': 0.84}\n",
      "{'loss': 0.0579, 'grad_norm': 1.5972061157226562, 'learning_rate': 3.58178778335083e-05, 'epoch': 0.85}\n",
      "{'loss': 0.0589, 'grad_norm': 1.887126088142395, 'learning_rate': 3.562346903067771e-05, 'epoch': 0.86}\n",
      "{'loss': 0.0571, 'grad_norm': 0.8401422500610352, 'learning_rate': 3.542906022784712e-05, 'epoch': 0.87}\n",
      "{'loss': 0.0579, 'grad_norm': 1.493455171585083, 'learning_rate': 3.5234651425016524e-05, 'epoch': 0.89}\n",
      "{'loss': 0.0579, 'grad_norm': 1.0734508037567139, 'learning_rate': 3.5040242622185936e-05, 'epoch': 0.9}\n",
      "{'loss': 0.0598, 'grad_norm': 1.2907670736312866, 'learning_rate': 3.484583381935535e-05, 'epoch': 0.91}\n",
      "{'loss': 0.0564, 'grad_norm': 2.118103265762329, 'learning_rate': 3.465142501652475e-05, 'epoch': 0.92}\n",
      "{'loss': 0.0561, 'grad_norm': 1.1457515954971313, 'learning_rate': 3.445701621369416e-05, 'epoch': 0.93}\n",
      "{'loss': 0.0576, 'grad_norm': 2.514720916748047, 'learning_rate': 3.426260741086357e-05, 'epoch': 0.94}\n",
      "{'loss': 0.0562, 'grad_norm': 1.3976253271102905, 'learning_rate': 3.4068198608032974e-05, 'epoch': 0.96}\n",
      "{'loss': 0.0598, 'grad_norm': 0.8844670653343201, 'learning_rate': 3.3873789805202385e-05, 'epoch': 0.97}\n",
      "{'loss': 0.0572, 'grad_norm': 1.0645008087158203, 'learning_rate': 3.367938100237179e-05, 'epoch': 0.98}\n",
      "{'loss': 0.0577, 'grad_norm': 1.9224255084991455, 'learning_rate': 3.3484972199541195e-05, 'epoch': 0.99}\n",
      "{'loss': 0.0568, 'grad_norm': 1.0376193523406982, 'learning_rate': 3.3290563396710606e-05, 'epoch': 1.0}\n",
      "{'loss': 0.0556, 'grad_norm': 1.6600476503372192, 'learning_rate': 3.309615459388001e-05, 'epoch': 1.01}\n",
      "{'loss': 0.0565, 'grad_norm': 1.4869905710220337, 'learning_rate': 3.290174579104942e-05, 'epoch': 1.03}\n",
      "{'loss': 0.0559, 'grad_norm': 1.1708415746688843, 'learning_rate': 3.270733698821883e-05, 'epoch': 1.04}\n",
      "{'loss': 0.0557, 'grad_norm': 1.1933374404907227, 'learning_rate': 3.251292818538824e-05, 'epoch': 1.05}\n",
      "{'loss': 0.0552, 'grad_norm': 2.3513424396514893, 'learning_rate': 3.2318519382557644e-05, 'epoch': 1.06}\n",
      "{'loss': 0.0568, 'grad_norm': 1.7763906717300415, 'learning_rate': 3.212411057972705e-05, 'epoch': 1.07}\n",
      "{'loss': 0.057, 'grad_norm': 1.9173383712768555, 'learning_rate': 3.192970177689646e-05, 'epoch': 1.08}\n",
      "{'loss': 0.0548, 'grad_norm': 1.4175372123718262, 'learning_rate': 3.1735292974065865e-05, 'epoch': 1.1}\n",
      "{'loss': 0.0538, 'grad_norm': 1.2221784591674805, 'learning_rate': 3.154088417123528e-05, 'epoch': 1.11}\n",
      "{'loss': 0.0556, 'grad_norm': 1.6504167318344116, 'learning_rate': 3.134647536840468e-05, 'epoch': 1.12}\n",
      "{'loss': 0.0534, 'grad_norm': 1.3020572662353516, 'learning_rate': 3.115206656557409e-05, 'epoch': 1.13}\n",
      "{'loss': 0.0551, 'grad_norm': 1.391627550125122, 'learning_rate': 3.09576577627435e-05, 'epoch': 1.14}\n",
      "{'loss': 0.0548, 'grad_norm': 1.3041454553604126, 'learning_rate': 3.07632489599129e-05, 'epoch': 1.15}\n",
      "{'loss': 0.0549, 'grad_norm': 1.398481011390686, 'learning_rate': 3.0568840157082315e-05, 'epoch': 1.17}\n",
      "{'loss': 0.0538, 'grad_norm': 2.1750714778900146, 'learning_rate': 3.037443135425172e-05, 'epoch': 1.18}\n",
      "{'loss': 0.0547, 'grad_norm': 1.190281867980957, 'learning_rate': 3.0180022551421128e-05, 'epoch': 1.19}\n",
      "{'loss': 0.0526, 'grad_norm': 1.3117340803146362, 'learning_rate': 2.9985613748590536e-05, 'epoch': 1.2}\n",
      "{'loss': 0.0564, 'grad_norm': 1.8261572122573853, 'learning_rate': 2.9791204945759944e-05, 'epoch': 1.21}\n",
      "{'loss': 0.0551, 'grad_norm': 1.0246143341064453, 'learning_rate': 2.959679614292935e-05, 'epoch': 1.22}\n",
      "{'loss': 0.0548, 'grad_norm': 0.584760308265686, 'learning_rate': 2.9402387340098764e-05, 'epoch': 1.24}\n",
      "{'loss': 0.0545, 'grad_norm': 1.0097988843917847, 'learning_rate': 2.9207978537268172e-05, 'epoch': 1.25}\n",
      "{'loss': 0.0534, 'grad_norm': 1.331218957901001, 'learning_rate': 2.901356973443758e-05, 'epoch': 1.26}\n",
      "{'loss': 0.0549, 'grad_norm': 1.8346269130706787, 'learning_rate': 2.8819160931606985e-05, 'epoch': 1.27}\n",
      "{'loss': 0.0552, 'grad_norm': 1.23416268825531, 'learning_rate': 2.8624752128776394e-05, 'epoch': 1.28}\n",
      "{'loss': 0.0562, 'grad_norm': 1.0781999826431274, 'learning_rate': 2.8430343325945802e-05, 'epoch': 1.29}\n",
      "{'loss': 0.0526, 'grad_norm': 1.2201578617095947, 'learning_rate': 2.823593452311521e-05, 'epoch': 1.31}\n",
      "{'loss': 0.0537, 'grad_norm': 0.9313327670097351, 'learning_rate': 2.804152572028462e-05, 'epoch': 1.32}\n",
      "{'loss': 0.052, 'grad_norm': 1.9057046175003052, 'learning_rate': 2.7847116917454023e-05, 'epoch': 1.33}\n",
      "{'loss': 0.0544, 'grad_norm': 0.6163673996925354, 'learning_rate': 2.765270811462343e-05, 'epoch': 1.34}\n",
      "{'loss': 0.0545, 'grad_norm': 1.6436306238174438, 'learning_rate': 2.745829931179284e-05, 'epoch': 1.35}\n",
      "{'loss': 0.0517, 'grad_norm': 1.093610405921936, 'learning_rate': 2.7263890508962248e-05, 'epoch': 1.36}\n",
      "{'loss': 0.0527, 'grad_norm': 1.0376194715499878, 'learning_rate': 2.7069481706131656e-05, 'epoch': 1.38}\n",
      "{'loss': 0.0534, 'grad_norm': 0.9482207298278809, 'learning_rate': 2.6875072903301064e-05, 'epoch': 1.39}\n",
      "{'loss': 0.0534, 'grad_norm': 1.0140258073806763, 'learning_rate': 2.668066410047047e-05, 'epoch': 1.4}\n",
      "{'loss': 0.0515, 'grad_norm': 0.8591196537017822, 'learning_rate': 2.6486255297639877e-05, 'epoch': 1.41}\n",
      "{'loss': 0.0526, 'grad_norm': 2.00996470451355, 'learning_rate': 2.6291846494809286e-05, 'epoch': 1.42}\n",
      "{'loss': 0.0545, 'grad_norm': 1.6050498485565186, 'learning_rate': 2.6097437691978694e-05, 'epoch': 1.43}\n",
      "{'loss': 0.0532, 'grad_norm': 1.0998523235321045, 'learning_rate': 2.5903028889148102e-05, 'epoch': 1.45}\n",
      "{'loss': 0.0518, 'grad_norm': 0.8028086423873901, 'learning_rate': 2.5708620086317507e-05, 'epoch': 1.46}\n",
      "{'loss': 0.0519, 'grad_norm': 1.0128793716430664, 'learning_rate': 2.5514211283486915e-05, 'epoch': 1.47}\n",
      "{'loss': 0.0538, 'grad_norm': 1.8063836097717285, 'learning_rate': 2.5319802480656323e-05, 'epoch': 1.48}\n",
      "{'loss': 0.0535, 'grad_norm': 0.8815935850143433, 'learning_rate': 2.512539367782573e-05, 'epoch': 1.49}\n",
      "{'loss': 0.0521, 'grad_norm': 1.8288514614105225, 'learning_rate': 2.4930984874995143e-05, 'epoch': 1.5}\n",
      "{'loss': 0.0524, 'grad_norm': 1.5715510845184326, 'learning_rate': 2.4736576072164548e-05, 'epoch': 1.52}\n",
      "{'loss': 0.0512, 'grad_norm': 1.2564777135849, 'learning_rate': 2.4544111357362263e-05, 'epoch': 1.53}\n",
      "{'loss': 0.0538, 'grad_norm': 1.1443507671356201, 'learning_rate': 2.434970255453167e-05, 'epoch': 1.54}\n",
      "{'loss': 0.0529, 'grad_norm': 1.4490230083465576, 'learning_rate': 2.4155293751701076e-05, 'epoch': 1.55}\n",
      "{'loss': 0.0527, 'grad_norm': 1.3369696140289307, 'learning_rate': 2.3960884948870484e-05, 'epoch': 1.56}\n",
      "{'loss': 0.0533, 'grad_norm': 1.6700383424758911, 'learning_rate': 2.3766476146039893e-05, 'epoch': 1.57}\n",
      "{'loss': 0.0519, 'grad_norm': 0.8253697156906128, 'learning_rate': 2.35720673432093e-05, 'epoch': 1.59}\n",
      "{'loss': 0.0529, 'grad_norm': 0.7201516628265381, 'learning_rate': 2.337765854037871e-05, 'epoch': 1.6}\n",
      "{'loss': 0.053, 'grad_norm': 1.1329739093780518, 'learning_rate': 2.3183249737548117e-05, 'epoch': 1.61}\n",
      "{'loss': 0.0527, 'grad_norm': 1.3769454956054688, 'learning_rate': 2.2988840934717525e-05, 'epoch': 1.62}\n",
      "{'loss': 0.053, 'grad_norm': 1.0387358665466309, 'learning_rate': 2.2794432131886934e-05, 'epoch': 1.63}\n",
      "{'loss': 0.0524, 'grad_norm': 1.4102883338928223, 'learning_rate': 2.2600023329056342e-05, 'epoch': 1.64}\n",
      "{'loss': 0.051, 'grad_norm': 1.6075797080993652, 'learning_rate': 2.240561452622575e-05, 'epoch': 1.66}\n",
      "{'loss': 0.0524, 'grad_norm': 0.9084166288375854, 'learning_rate': 2.2211205723395155e-05, 'epoch': 1.67}\n",
      "{'loss': 0.0511, 'grad_norm': 1.3292839527130127, 'learning_rate': 2.2016796920564563e-05, 'epoch': 1.68}\n",
      "{'loss': 0.0517, 'grad_norm': 1.817433476448059, 'learning_rate': 2.182238811773397e-05, 'epoch': 1.69}\n",
      "{'loss': 0.0525, 'grad_norm': 0.8852408528327942, 'learning_rate': 2.162797931490338e-05, 'epoch': 1.7}\n",
      "{'loss': 0.0527, 'grad_norm': 0.5990745425224304, 'learning_rate': 2.1433570512072788e-05, 'epoch': 1.71}\n",
      "{'loss': 0.0502, 'grad_norm': 0.9810763001441956, 'learning_rate': 2.1239161709242196e-05, 'epoch': 1.73}\n",
      "{'loss': 0.0522, 'grad_norm': 2.0555474758148193, 'learning_rate': 2.10447529064116e-05, 'epoch': 1.74}\n",
      "{'loss': 0.0506, 'grad_norm': 1.8084958791732788, 'learning_rate': 2.085034410358101e-05, 'epoch': 1.75}\n",
      "{'loss': 0.0524, 'grad_norm': 1.6194511651992798, 'learning_rate': 2.0657879388778724e-05, 'epoch': 1.76}\n",
      "{'loss': 0.0516, 'grad_norm': 0.9577648043632507, 'learning_rate': 2.0463470585948132e-05, 'epoch': 1.77}\n",
      "{'loss': 0.0512, 'grad_norm': 1.2693102359771729, 'learning_rate': 2.026906178311754e-05, 'epoch': 1.78}\n",
      "{'loss': 0.0525, 'grad_norm': 2.191636800765991, 'learning_rate': 2.007465298028695e-05, 'epoch': 1.8}\n",
      "{'loss': 0.0513, 'grad_norm': 1.3423948287963867, 'learning_rate': 1.9880244177456357e-05, 'epoch': 1.81}\n",
      "{'loss': 0.0504, 'grad_norm': 1.7729498147964478, 'learning_rate': 1.9685835374625762e-05, 'epoch': 1.82}\n",
      "{'loss': 0.0512, 'grad_norm': 1.0591449737548828, 'learning_rate': 1.949142657179517e-05, 'epoch': 1.83}\n",
      "{'loss': 0.0492, 'grad_norm': 1.4684959650039673, 'learning_rate': 1.929701776896458e-05, 'epoch': 1.84}\n",
      "{'loss': 0.0512, 'grad_norm': 1.88214910030365, 'learning_rate': 1.9102608966133987e-05, 'epoch': 1.85}\n",
      "{'loss': 0.0524, 'grad_norm': 2.0904557704925537, 'learning_rate': 1.8908200163303395e-05, 'epoch': 1.87}\n",
      "{'loss': 0.0521, 'grad_norm': 1.065442681312561, 'learning_rate': 1.8713791360472803e-05, 'epoch': 1.88}\n",
      "{'loss': 0.0518, 'grad_norm': 1.0293408632278442, 'learning_rate': 1.851938255764221e-05, 'epoch': 1.89}\n",
      "{'loss': 0.0495, 'grad_norm': 1.6529312133789062, 'learning_rate': 1.832497375481162e-05, 'epoch': 1.9}\n",
      "{'loss': 0.0503, 'grad_norm': 1.4164530038833618, 'learning_rate': 1.8130564951981028e-05, 'epoch': 1.91}\n",
      "{'loss': 0.0496, 'grad_norm': 1.0654821395874023, 'learning_rate': 1.7936156149150436e-05, 'epoch': 1.92}\n",
      "{'loss': 0.0513, 'grad_norm': 1.721459150314331, 'learning_rate': 1.774174734631984e-05, 'epoch': 1.94}\n",
      "{'loss': 0.05, 'grad_norm': 1.2775012254714966, 'learning_rate': 1.754733854348925e-05, 'epoch': 1.95}\n",
      "{'loss': 0.0506, 'grad_norm': 0.8669699430465698, 'learning_rate': 1.7352929740658657e-05, 'epoch': 1.96}\n",
      "{'loss': 0.0508, 'grad_norm': 1.1435071229934692, 'learning_rate': 1.7158520937828065e-05, 'epoch': 1.97}\n",
      "{'loss': 0.0495, 'grad_norm': 1.5652343034744263, 'learning_rate': 1.6964112134997474e-05, 'epoch': 1.98}\n",
      "{'loss': 0.0498, 'grad_norm': 1.0554221868515015, 'learning_rate': 1.677164742019519e-05, 'epoch': 1.99}\n",
      "{'loss': 0.0505, 'grad_norm': 1.2998919486999512, 'learning_rate': 1.6577238617364597e-05, 'epoch': 2.01}\n",
      "{'loss': 0.048, 'grad_norm': 1.1963670253753662, 'learning_rate': 1.6382829814534005e-05, 'epoch': 2.02}\n",
      "{'loss': 0.0488, 'grad_norm': 1.6951287984848022, 'learning_rate': 1.618842101170341e-05, 'epoch': 2.03}\n",
      "{'loss': 0.0504, 'grad_norm': 1.2702364921569824, 'learning_rate': 1.5994012208872818e-05, 'epoch': 2.04}\n",
      "{'loss': 0.0509, 'grad_norm': 0.9764886498451233, 'learning_rate': 1.5799603406042226e-05, 'epoch': 2.05}\n",
      "{'loss': 0.0504, 'grad_norm': 1.0999716520309448, 'learning_rate': 1.5605194603211635e-05, 'epoch': 2.06}\n",
      "{'loss': 0.0506, 'grad_norm': 1.1286057233810425, 'learning_rate': 1.5410785800381043e-05, 'epoch': 2.08}\n",
      "{'loss': 0.0499, 'grad_norm': 1.1809114217758179, 'learning_rate': 1.521637699755045e-05, 'epoch': 2.09}\n",
      "{'loss': 0.0475, 'grad_norm': 1.2323821783065796, 'learning_rate': 1.5021968194719858e-05, 'epoch': 2.1}\n",
      "{'loss': 0.0495, 'grad_norm': 1.8255901336669922, 'learning_rate': 1.4827559391889264e-05, 'epoch': 2.11}\n",
      "{'loss': 0.0484, 'grad_norm': 1.5396671295166016, 'learning_rate': 1.4633150589058672e-05, 'epoch': 2.12}\n",
      "{'loss': 0.0496, 'grad_norm': 1.6423994302749634, 'learning_rate': 1.443874178622808e-05, 'epoch': 2.13}\n",
      "{'loss': 0.0476, 'grad_norm': 0.9879516959190369, 'learning_rate': 1.4244332983397487e-05, 'epoch': 2.15}\n",
      "{'loss': 0.0515, 'grad_norm': 2.528609037399292, 'learning_rate': 1.4049924180566899e-05, 'epoch': 2.16}\n",
      "{'loss': 0.0502, 'grad_norm': 1.882240653038025, 'learning_rate': 1.3855515377736305e-05, 'epoch': 2.17}\n",
      "{'loss': 0.0504, 'grad_norm': 0.8534610867500305, 'learning_rate': 1.3661106574905714e-05, 'epoch': 2.18}\n",
      "{'loss': 0.0503, 'grad_norm': 1.4805322885513306, 'learning_rate': 1.346669777207512e-05, 'epoch': 2.19}\n",
      "{'loss': 0.0477, 'grad_norm': 1.469130277633667, 'learning_rate': 1.3272288969244528e-05, 'epoch': 2.2}\n",
      "{'loss': 0.0474, 'grad_norm': 0.8329635858535767, 'learning_rate': 1.3077880166413937e-05, 'epoch': 2.22}\n",
      "{'loss': 0.0488, 'grad_norm': 1.4509550333023071, 'learning_rate': 1.2883471363583343e-05, 'epoch': 2.23}\n",
      "{'loss': 0.0493, 'grad_norm': 1.5808881521224976, 'learning_rate': 1.2691006648781056e-05, 'epoch': 2.24}\n",
      "{'loss': 0.0492, 'grad_norm': 1.3725183010101318, 'learning_rate': 1.2496597845950465e-05, 'epoch': 2.25}\n",
      "{'loss': 0.0496, 'grad_norm': 1.1063460111618042, 'learning_rate': 1.2302189043119873e-05, 'epoch': 2.26}\n",
      "{'loss': 0.0489, 'grad_norm': 1.2621517181396484, 'learning_rate': 1.2107780240289281e-05, 'epoch': 2.27}\n",
      "{'loss': 0.05, 'grad_norm': 0.9256197810173035, 'learning_rate': 1.1913371437458688e-05, 'epoch': 2.29}\n",
      "{'loss': 0.0496, 'grad_norm': 1.7121801376342773, 'learning_rate': 1.1718962634628096e-05, 'epoch': 2.3}\n",
      "{'loss': 0.0478, 'grad_norm': 0.867753803730011, 'learning_rate': 1.1524553831797506e-05, 'epoch': 2.31}\n",
      "{'loss': 0.0471, 'grad_norm': 1.495627760887146, 'learning_rate': 1.1330145028966912e-05, 'epoch': 2.32}\n",
      "{'loss': 0.0502, 'grad_norm': 1.6070208549499512, 'learning_rate': 1.113573622613632e-05, 'epoch': 2.33}\n",
      "{'loss': 0.0477, 'grad_norm': 1.365918755531311, 'learning_rate': 1.0941327423305727e-05, 'epoch': 2.34}\n",
      "{'loss': 0.0492, 'grad_norm': 0.9579719305038452, 'learning_rate': 1.0746918620475135e-05, 'epoch': 2.36}\n",
      "{'loss': 0.0497, 'grad_norm': 1.3406320810317993, 'learning_rate': 1.0552509817644543e-05, 'epoch': 2.37}\n",
      "{'loss': 0.0484, 'grad_norm': 1.0322941541671753, 'learning_rate': 1.035810101481395e-05, 'epoch': 2.38}\n",
      "{'loss': 0.0484, 'grad_norm': 1.3015949726104736, 'learning_rate': 1.016369221198336e-05, 'epoch': 2.39}\n",
      "{'loss': 0.0481, 'grad_norm': 1.1998475790023804, 'learning_rate': 9.969283409152766e-06, 'epoch': 2.4}\n",
      "{'loss': 0.0494, 'grad_norm': 1.1165655851364136, 'learning_rate': 9.774874606322175e-06, 'epoch': 2.41}\n",
      "{'loss': 0.0471, 'grad_norm': 1.2427302598953247, 'learning_rate': 9.580465803491583e-06, 'epoch': 2.43}\n",
      "{'loss': 0.0481, 'grad_norm': 0.874218225479126, 'learning_rate': 9.38605700066099e-06, 'epoch': 2.44}\n",
      "{'loss': 0.0499, 'grad_norm': 1.5505502223968506, 'learning_rate': 9.191648197830398e-06, 'epoch': 2.45}\n",
      "{'loss': 0.0487, 'grad_norm': 0.6606749296188354, 'learning_rate': 8.997239394999806e-06, 'epoch': 2.46}\n",
      "{'loss': 0.0479, 'grad_norm': 1.0985466241836548, 'learning_rate': 8.80477468019752e-06, 'epoch': 2.47}\n",
      "{'loss': 0.048, 'grad_norm': 1.0359662771224976, 'learning_rate': 8.610365877366927e-06, 'epoch': 2.48}\n",
      "{'loss': 0.0474, 'grad_norm': 1.04714834690094, 'learning_rate': 8.415957074536334e-06, 'epoch': 2.5}\n",
      "{'loss': 0.0491, 'grad_norm': 1.1467620134353638, 'learning_rate': 8.221548271705744e-06, 'epoch': 2.51}\n",
      "{'loss': 0.0474, 'grad_norm': 1.229055404663086, 'learning_rate': 8.027139468875152e-06, 'epoch': 2.52}\n",
      "{'loss': 0.0481, 'grad_norm': 1.504244089126587, 'learning_rate': 7.832730666044559e-06, 'epoch': 2.53}\n",
      "{'loss': 0.0505, 'grad_norm': 0.9919617176055908, 'learning_rate': 7.638321863213967e-06, 'epoch': 2.54}\n",
      "{'loss': 0.0473, 'grad_norm': 1.9009045362472534, 'learning_rate': 7.443913060383374e-06, 'epoch': 2.55}\n",
      "{'loss': 0.0491, 'grad_norm': 1.5124047994613647, 'learning_rate': 7.249504257552782e-06, 'epoch': 2.57}\n",
      "{'loss': 0.0478, 'grad_norm': 1.0900987386703491, 'learning_rate': 7.05509545472219e-06, 'epoch': 2.58}\n",
      "{'loss': 0.0492, 'grad_norm': 1.1474870443344116, 'learning_rate': 6.860686651891599e-06, 'epoch': 2.59}\n",
      "{'loss': 0.05, 'grad_norm': 1.6597506999969482, 'learning_rate': 6.666277849061006e-06, 'epoch': 2.6}\n",
      "{'loss': 0.0487, 'grad_norm': 0.8675825595855713, 'learning_rate': 6.471869046230414e-06, 'epoch': 2.61}\n",
      "{'loss': 0.0473, 'grad_norm': 0.8413164615631104, 'learning_rate': 6.277460243399821e-06, 'epoch': 2.62}\n",
      "{'loss': 0.049, 'grad_norm': 2.7607104778289795, 'learning_rate': 6.083051440569229e-06, 'epoch': 2.64}\n",
      "{'loss': 0.0493, 'grad_norm': 1.4606393575668335, 'learning_rate': 5.8886426377386375e-06, 'epoch': 2.65}\n",
      "{'loss': 0.0481, 'grad_norm': 1.367985486984253, 'learning_rate': 5.694233834908045e-06, 'epoch': 2.66}\n",
      "{'loss': 0.0477, 'grad_norm': 0.7548201680183411, 'learning_rate': 5.499825032077452e-06, 'epoch': 2.67}\n",
      "{'loss': 0.0466, 'grad_norm': 1.2368475198745728, 'learning_rate': 5.3054162292468605e-06, 'epoch': 2.68}\n",
      "{'loss': 0.0487, 'grad_norm': 1.0968105792999268, 'learning_rate': 5.111007426416269e-06, 'epoch': 2.69}\n",
      "{'loss': 0.0478, 'grad_norm': 1.1380828619003296, 'learning_rate': 4.918542711613982e-06, 'epoch': 2.71}\n",
      "{'loss': 0.0478, 'grad_norm': 1.360062837600708, 'learning_rate': 4.72413390878339e-06, 'epoch': 2.72}\n",
      "{'loss': 0.0504, 'grad_norm': 1.0851000547409058, 'learning_rate': 4.529725105952798e-06, 'epoch': 2.73}\n",
      "{'loss': 0.0475, 'grad_norm': 0.9728887677192688, 'learning_rate': 4.335316303122206e-06, 'epoch': 2.74}\n",
      "{'loss': 0.0477, 'grad_norm': 1.0699453353881836, 'learning_rate': 4.140907500291613e-06, 'epoch': 2.75}\n",
      "{'loss': 0.0487, 'grad_norm': 1.584311842918396, 'learning_rate': 3.948442785489327e-06, 'epoch': 2.76}\n",
      "{'loss': 0.0483, 'grad_norm': 0.9300485849380493, 'learning_rate': 3.7540339826587348e-06, 'epoch': 2.78}\n",
      "{'loss': 0.0475, 'grad_norm': 0.5457674264907837, 'learning_rate': 3.559625179828143e-06, 'epoch': 2.79}\n",
      "{'loss': 0.0485, 'grad_norm': 1.015019416809082, 'learning_rate': 3.365216376997551e-06, 'epoch': 2.8}\n",
      "{'loss': 0.0482, 'grad_norm': 0.6068149209022522, 'learning_rate': 3.170807574166958e-06, 'epoch': 2.81}\n",
      "{'loss': 0.0486, 'grad_norm': 0.9959906339645386, 'learning_rate': 2.9763987713363664e-06, 'epoch': 2.82}\n",
      "{'loss': 0.0477, 'grad_norm': 2.0360302925109863, 'learning_rate': 2.7819899685057738e-06, 'epoch': 2.83}\n",
      "{'loss': 0.0488, 'grad_norm': 0.9892419576644897, 'learning_rate': 2.587581165675182e-06, 'epoch': 2.85}\n",
      "{'loss': 0.0508, 'grad_norm': 1.6862386465072632, 'learning_rate': 2.3931723628445894e-06, 'epoch': 2.86}\n",
      "{'loss': 0.0496, 'grad_norm': 0.5751855373382568, 'learning_rate': 2.1987635600139976e-06, 'epoch': 2.87}\n",
      "{'loss': 0.0468, 'grad_norm': 1.6767202615737915, 'learning_rate': 2.0043547571834054e-06, 'epoch': 2.88}\n",
      "{'loss': 0.0478, 'grad_norm': 1.2651889324188232, 'learning_rate': 1.8099459543528132e-06, 'epoch': 2.89}\n",
      "{'loss': 0.0489, 'grad_norm': 0.8230139017105103, 'learning_rate': 1.615537151522221e-06, 'epoch': 2.9}\n",
      "{'loss': 0.0468, 'grad_norm': 1.9257560968399048, 'learning_rate': 1.4211283486916288e-06, 'epoch': 2.92}\n",
      "{'loss': 0.0485, 'grad_norm': 1.4053417444229126, 'learning_rate': 1.2267195458610366e-06, 'epoch': 2.93}\n",
      "{'loss': 0.0479, 'grad_norm': 1.4491097927093506, 'learning_rate': 1.0323107430304444e-06, 'epoch': 2.94}\n",
      "{'loss': 0.0478, 'grad_norm': 1.016250491142273, 'learning_rate': 8.379019401998523e-07, 'epoch': 2.95}\n",
      "{'loss': 0.0493, 'grad_norm': 2.288825035095215, 'learning_rate': 6.434931373692601e-07, 'epoch': 2.96}\n",
      "{'loss': 0.0474, 'grad_norm': 1.2153470516204834, 'learning_rate': 4.490843345386679e-07, 'epoch': 2.97}\n",
      "{'loss': 0.0472, 'grad_norm': 1.6245694160461426, 'learning_rate': 2.5467553170807575e-07, 'epoch': 2.99}\n",
      "{'loss': 0.0488, 'grad_norm': 0.7892409563064575, 'learning_rate': 6.026672887748358e-08, 'epoch': 3.0}\n",
      "{'train_runtime': 1580.7354, 'train_samples_per_second': 32.539, 'train_steps_per_second': 16.27, 'train_loss': 0.06469992753705626, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('gpt2-finetuned\\\\tokenizer_config.json',\n",
       " 'gpt2-finetuned\\\\special_tokens_map.json',\n",
       " 'gpt2-finetuned\\\\vocab.json',\n",
       " 'gpt2-finetuned\\\\merges.txt',\n",
       " 'gpt2-finetuned\\\\added_tokens.json')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, TextDataset, DataCollatorForLanguageModeling\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# Chargement des donn√©es\n",
    "df = pd.read_csv(\"../data/emotion_datasets/emo_reviews.csv\")\n",
    "texts = prepare_gpt2_dataset(df)\n",
    "\n",
    "# Sauvegarde dans un fichier temporaire\n",
    "with open(\"train.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(texts))\n",
    "\n",
    "# Tokenizer + dataset\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "def load_dataset(file_path, tokenizer):\n",
    "    return TextDataset(\n",
    "        tokenizer=tokenizer,\n",
    "        file_path=file_path,\n",
    "        block_size=128\n",
    "    )\n",
    "\n",
    "dataset = load_dataset(\"train.txt\", tokenizer)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Training\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-finetuned\",\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=100,\n",
    "    fp16=torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "model.save_pretrained(\"gpt2-finetuned\")\n",
    "tokenizer.save_pretrained(\"gpt2-finetuned\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Load fine-tuned model\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2-finetuned\").to(\"cuda\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-finetuned\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 2. Sample data (prompt + reference)\n",
    "samples = [\n",
    "    {\"emotion\": \"anger\", \"input\": \"Ce service est scandaleux !\", \"reference\": \"Je comprends votre frustration. Nous allons vous aider au plus vite.\"},\n",
    "    {\"emotion\": \"joy\", \"input\": \"Merci pour votre r√©activit√© exceptionnelle !\", \"reference\": \"Merci beaucoup pour votre message positif. Cela nous encourage.\"},\n",
    "    {\"emotion\": \"sadness\", \"input\": \"Je suis tr√®s d√©√ßu par votre r√©ponse.\", \"reference\": \"Nous sommes d√©sol√©s pour cette exp√©rience. Nous allons revoir cela.\"},\n",
    "    {\"emotion\": \"fear\", \"input\": \"J'ai peur de ne jamais recevoir ma commande.\", \"reference\": \"Nous comprenons votre inqui√©tude. Nous faisons le n√©cessaire rapidement.\"},\n",
    "    {\"emotion\": \"confusion\", \"input\": \"Je ne comprends pas votre politique de retour.\", \"reference\": \"Nous allons vous expliquer cela clairement. Voici comment proc√©der.\"},\n",
    "]\n",
    "\n",
    "# 3. Prepare for evaluation\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "smoothie = SmoothingFunction().method4\n",
    "results = []\n",
    "\n",
    "# 4. Loop over samples\n",
    "for s in samples:\n",
    "    prompt = f\"Emotion: {s['emotion']} | Message: {s['input']} | R√©ponse:\"\n",
    "    reference = s[\"reference\"]\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=50,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        temperature=0.9\n",
    "    )\n",
    "\n",
    "    generated = tokenizer.decode(output[0], skip_special_tokens=True).replace(prompt, \"\").strip()\n",
    "\n",
    "    # Metrics\n",
    "    bleu = sentence_bleu([reference.split()], generated.split(), smoothing_function=smoothie)\n",
    "    rouge = scorer.score(reference, generated)\n",
    "    enc = tokenizer(generated, return_tensors=\"pt\").to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        loss = model(**enc, labels=enc[\"input_ids\"]).loss\n",
    "    perplexity = math.exp(loss.item()) if loss.item() < 100 else float(\"inf\")\n",
    "\n",
    "    results.append({\n",
    "        \"emotion\": s[\"emotion\"],\n",
    "        \"input\": s[\"input\"],\n",
    "        \"reference\": reference,\n",
    "        \"generated\": generated,\n",
    "        \"bleu\": round(bleu, 4),\n",
    "        \"rouge-1\": round(rouge['rouge1'].fmeasure, 4),\n",
    "        \"rouge-2\": round(rouge['rouge2'].fmeasure, 4),\n",
    "        \"rouge-L\": round(rouge['rougeL'].fmeasure, 4),\n",
    "        \"perplexity\": round(perplexity, 4)\n",
    "    })\n",
    "\n",
    "# 5. Display\n",
    "df = pd.DataFrame(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>bleu</th>\n",
       "      <th>rouge-1</th>\n",
       "      <th>rouge-2</th>\n",
       "      <th>rouge-L</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>generated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>confusion</td>\n",
       "      <td>0.0545</td>\n",
       "      <td>0.1765</td>\n",
       "      <td>0.1250</td>\n",
       "      <td>0.1765</td>\n",
       "      <td>1.1352</td>\n",
       "      <td>Merci pour votre message concernant confusion. Nous allons vous r√©pondre au mieux.\\nEmotion: confusion | Message: Je suis compl√®tement perdu. | R√©ponse: Merci</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>anger</td>\n",
       "      <td>0.0442</td>\n",
       "      <td>0.2727</td>\n",
       "      <td>0.0952</td>\n",
       "      <td>0.2273</td>\n",
       "      <td>10.7181</td>\n",
       "      <td>Emotion: anger | Message: Ce service est scandaleux! | R√©ponse: Merci pour votre message concernant anger. Nous allons vous r√©pondre au mieux.\\nEmotion: anger | Message: Je suis furieux contre votre service. | R√©ponse:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>joy</td>\n",
       "      <td>0.0399</td>\n",
       "      <td>0.2222</td>\n",
       "      <td>0.0930</td>\n",
       "      <td>0.2222</td>\n",
       "      <td>8.1961</td>\n",
       "      <td>Emotion: joy | Message: Merci pour votre r√©activit√© exceptionnelle! | R√©ponse: Merci pour votre message concernant joy. Nous allons vous r√©pondre au mieux.\\nEmotion: joy | Message: Je suis tr√®s heureux de cette exp√©rience.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sadness</td>\n",
       "      <td>0.0277</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.0526</td>\n",
       "      <td>0.1500</td>\n",
       "      <td>1.0901</td>\n",
       "      <td>Merci pour votre message concernant sadness. Nous allons vous r√©pondre au mieux.\\nEmotion: sadness | Message: Je suis d√©√ßu et triste de cette issue. | R√©p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fear</td>\n",
       "      <td>0.0133</td>\n",
       "      <td>0.1111</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1111</td>\n",
       "      <td>1.1002</td>\n",
       "      <td>Merci pour votre message concernant fear. Nous allons vous r√©pondre au mieux.\\nEmotion: fear | Message: Je suis inquiet pour ma s√©curit√©. | R√©ponse:</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     emotion    bleu  rouge-1  rouge-2  rouge-L  perplexity  \\\n",
       "0  confusion  0.0545   0.1765   0.1250   0.1765      1.1352   \n",
       "1      anger  0.0442   0.2727   0.0952   0.2273     10.7181   \n",
       "2        joy  0.0399   0.2222   0.0930   0.2222      8.1961   \n",
       "3    sadness  0.0277   0.2500   0.0526   0.1500      1.0901   \n",
       "4       fear  0.0133   0.1111   0.0000   0.1111      1.1002   \n",
       "\n",
       "                                                                                                                                                                                                                        generated  \n",
       "0                                                                  Merci pour votre message concernant confusion. Nous allons vous r√©pondre au mieux.\\nEmotion: confusion | Message: Je suis compl√®tement perdu. | R√©ponse: Merci  \n",
       "1      Emotion: anger | Message: Ce service est scandaleux! | R√©ponse: Merci pour votre message concernant anger. Nous allons vous r√©pondre au mieux.\\nEmotion: anger | Message: Je suis furieux contre votre service. | R√©ponse:  \n",
       "2  Emotion: joy | Message: Merci pour votre r√©activit√© exceptionnelle! | R√©ponse: Merci pour votre message concernant joy. Nous allons vous r√©pondre au mieux.\\nEmotion: joy | Message: Je suis tr√®s heureux de cette exp√©rience.  \n",
       "3                                                                      Merci pour votre message concernant sadness. Nous allons vous r√©pondre au mieux.\\nEmotion: sadness | Message: Je suis d√©√ßu et triste de cette issue. | R√©p  \n",
       "4                                                                            Merci pour votre message concernant fear. Nous allons vous r√©pondre au mieux.\\nEmotion: fear | Message: Je suis inquiet pour ma s√©curit√©. | R√©ponse:  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_sorted = df.sort_values(by=\"bleu\", ascending=False).reset_index(drop=True)\n",
    "display(df_sorted[[\"emotion\", \"bleu\", \"rouge-1\", \"rouge-2\", \"rouge-L\", \"perplexity\", \"generated\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Conclusion : Fine-tuning GPT-2 vs Prompt-based GPT-2\n",
    "\n",
    "Apr√®s avoir compar√© les performances de **GPT-2 vanilla** (prompting uniquement) avec celles de **GPT-2 fine-tun√©** sur notre corpus √©motionnel :\n",
    "\n",
    "### R√©sultats\n",
    "\n",
    "- **BLEU** : Le score BLEU maximal est pass√© de ~0.02 √† ~0.05, indiquant une meilleure correspondance n-grammes avec les r√©ponses attendues.\n",
    "- **ROUGE-1/2/L** : Tous les scores ROUGE sont significativement am√©lior√©s apr√®s fine-tuning, avec un ROUGE-1 atteignant 0.27 (contre ~0.16 pr√©c√©demment).\n",
    "- **Perplexit√©** : Elle chute fortement (~1‚Äì10 apr√®s fine-tuning vs >20 avant), montrant que le mod√®le a appris une distribution linguistique plus fluide et pr√©visible.\n",
    "- **Qualit√© des r√©ponses** : Les r√©ponses fine-tun√©es sont plus coh√©rentes, contextualis√©es, et surtout mieux align√©es √©motionnellement.\n",
    "\n",
    "### Interpr√©tation\n",
    "\n",
    "Le fine-tuning permet au mod√®le de s‚Äôadapter pr√©cis√©ment au style, au ton et √† la structure des r√©ponses de support client √©motionnel. Il d√©passe largement la g√©n√©ration bas√©e sur le prompt seul, qui reste trop g√©n√©rique ou hors-sujet.\n",
    "\n",
    "### Recommandation\n",
    "\n",
    "**Adopter le mod√®le fine-tun√©** pour toute t√¢che de g√©n√©ration de texte √©motionnel sp√©cifique, ou explorer le fine-tuning de mod√®les encore plus performants comme **T5** ou **BART**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T5 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments, DataCollatorForSeq2Seq\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# 1. Chargement du CSV\n",
    "df = pd.read_csv(\"../data/emotion_datasets/emo_reviews.csv\")\n",
    "\n",
    "# 2. Formatage pour T5 (prompt de type: \"emotion: anger message: xxx\")\n",
    "df[\"input_text\"] = \"emotion: \" + df[\"emotion\"] + \" message: \" + df[\"text_input\"]\n",
    "df[\"target_text\"] = df[\"text_output\"]\n",
    "\n",
    "# 3. Dataset HuggingFace\n",
    "dataset = Dataset.from_pandas(df[[\"input_text\", \"target_text\"]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Yann\\anaconda3\\envs\\hf_gpu_env\\lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2faea3b947d468cabf9c12cf72220d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Yann\\anaconda3\\envs\\hf_gpu_env\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Yann\\.cache\\huggingface\\hub\\models--t5-small. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89924b3bf0b84d3caa57aa35249e6578",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cedc81202c184596ba159097bee4a482",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94894f9567a14412a97ba0c22c34c185",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/42000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "def tokenize(batch):\n",
    "    inputs = tokenizer(batch[\"input_text\"], padding=\"max_length\", truncation=True, max_length=64)\n",
    "    targets = tokenizer(batch[\"target_text\"], padding=\"max_length\", truncation=True, max_length=64)\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize, batched=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Entra√Ænement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acc0648c9066463bb5bbf208fad3bde4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b7ba562d6674b0ba2af6646938a1be4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac217e24708b4fc7ad7aab1768598cad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Yann\\anaconda3\\envs\\hf_gpu_env\\lib\\site-packages\\accelerate\\accelerator.py:469: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a701ef4d0c864040bc0c4cbd37974111",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.5674, 'grad_norm': 5.943992614746094, 'learning_rate': 4.9926984126984124e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5968, 'grad_norm': 6.198870658874512, 'learning_rate': 4.9847619047619046e-05, 'epoch': 0.01}\n",
      "{'loss': 0.1136, 'grad_norm': 0.41801565885543823, 'learning_rate': 4.976825396825397e-05, 'epoch': 0.01}\n",
      "{'loss': 0.0179, 'grad_norm': 0.24424144625663757, 'learning_rate': 4.968888888888889e-05, 'epoch': 0.02}\n",
      "{'loss': 0.008, 'grad_norm': 0.028020648285746574, 'learning_rate': 4.960952380952381e-05, 'epoch': 0.02}\n",
      "{'loss': 0.0056, 'grad_norm': 0.024435386061668396, 'learning_rate': 4.953015873015873e-05, 'epoch': 0.03}\n",
      "{'loss': 0.0052, 'grad_norm': 0.02321653626859188, 'learning_rate': 4.9450793650793654e-05, 'epoch': 0.03}\n",
      "{'loss': 0.0023, 'grad_norm': 0.061207570135593414, 'learning_rate': 4.9371428571428575e-05, 'epoch': 0.04}\n",
      "{'loss': 0.0022, 'grad_norm': 0.021738845854997635, 'learning_rate': 4.9292063492063497e-05, 'epoch': 0.04}\n",
      "{'loss': 0.0021, 'grad_norm': 0.010227644816040993, 'learning_rate': 4.921269841269842e-05, 'epoch': 0.05}\n",
      "{'loss': 0.0013, 'grad_norm': 0.024407071992754936, 'learning_rate': 4.913333333333334e-05, 'epoch': 0.05}\n",
      "{'loss': 0.0016, 'grad_norm': 0.030688857659697533, 'learning_rate': 4.905396825396826e-05, 'epoch': 0.06}\n",
      "{'loss': 0.0011, 'grad_norm': 0.7344925999641418, 'learning_rate': 4.8974603174603176e-05, 'epoch': 0.06}\n",
      "{'loss': 0.0009, 'grad_norm': 0.05213882401585579, 'learning_rate': 4.88952380952381e-05, 'epoch': 0.07}\n",
      "{'loss': 0.0009, 'grad_norm': 0.013287224806845188, 'learning_rate': 4.881587301587302e-05, 'epoch': 0.07}\n",
      "{'loss': 0.0009, 'grad_norm': 0.07711287587881088, 'learning_rate': 4.873650793650794e-05, 'epoch': 0.08}\n",
      "{'loss': 0.001, 'grad_norm': 0.013492990285158157, 'learning_rate': 4.865714285714286e-05, 'epoch': 0.08}\n",
      "{'loss': 0.0011, 'grad_norm': 0.052751462906599045, 'learning_rate': 4.8577777777777776e-05, 'epoch': 0.09}\n",
      "{'loss': 0.0006, 'grad_norm': 0.004812977276742458, 'learning_rate': 4.84984126984127e-05, 'epoch': 0.09}\n",
      "{'loss': 0.0006, 'grad_norm': 0.0047946106642484665, 'learning_rate': 4.841904761904762e-05, 'epoch': 0.1}\n",
      "{'loss': 0.0005, 'grad_norm': 0.010554754175245762, 'learning_rate': 4.833968253968254e-05, 'epoch': 0.1}\n",
      "{'loss': 0.0004, 'grad_norm': 0.003956511616706848, 'learning_rate': 4.826031746031746e-05, 'epoch': 0.1}\n",
      "{'loss': 0.0003, 'grad_norm': 0.05824141576886177, 'learning_rate': 4.8180952380952384e-05, 'epoch': 0.11}\n",
      "{'loss': 0.0003, 'grad_norm': 0.004845956340432167, 'learning_rate': 4.8101587301587305e-05, 'epoch': 0.11}\n",
      "{'loss': 0.0003, 'grad_norm': 0.008174716494977474, 'learning_rate': 4.802222222222223e-05, 'epoch': 0.12}\n",
      "{'loss': 0.0004, 'grad_norm': 0.012496169656515121, 'learning_rate': 4.794285714285714e-05, 'epoch': 0.12}\n",
      "{'loss': 0.0004, 'grad_norm': 0.05969266965985298, 'learning_rate': 4.786349206349206e-05, 'epoch': 0.13}\n",
      "{'loss': 0.0002, 'grad_norm': 0.006760935764759779, 'learning_rate': 4.7784126984126985e-05, 'epoch': 0.13}\n",
      "{'loss': 0.0005, 'grad_norm': 0.001861664466559887, 'learning_rate': 4.7704761904761906e-05, 'epoch': 0.14}\n",
      "{'loss': 0.0007, 'grad_norm': 0.007662463467568159, 'learning_rate': 4.762539682539683e-05, 'epoch': 0.14}\n",
      "{'loss': 0.0012, 'grad_norm': 0.005158627405762672, 'learning_rate': 4.754603174603175e-05, 'epoch': 0.15}\n",
      "{'loss': 0.0003, 'grad_norm': 0.006374089512974024, 'learning_rate': 4.746666666666667e-05, 'epoch': 0.15}\n",
      "{'loss': 0.001, 'grad_norm': 0.00292950845323503, 'learning_rate': 4.738730158730159e-05, 'epoch': 0.16}\n",
      "{'loss': 0.0002, 'grad_norm': 0.001597995520569384, 'learning_rate': 4.7307936507936514e-05, 'epoch': 0.16}\n",
      "{'loss': 0.0002, 'grad_norm': 0.0015322774415835738, 'learning_rate': 4.7228571428571435e-05, 'epoch': 0.17}\n",
      "{'loss': 0.0001, 'grad_norm': 0.005040706135332584, 'learning_rate': 4.714920634920636e-05, 'epoch': 0.17}\n",
      "{'loss': 0.0002, 'grad_norm': 0.0006603373913094401, 'learning_rate': 4.706984126984127e-05, 'epoch': 0.18}\n",
      "{'loss': 0.0002, 'grad_norm': 0.04020530357956886, 'learning_rate': 4.699047619047619e-05, 'epoch': 0.18}\n",
      "{'loss': 0.0002, 'grad_norm': 0.0018080733716487885, 'learning_rate': 4.6911111111111114e-05, 'epoch': 0.19}\n",
      "{'loss': 0.0001, 'grad_norm': 0.000880492094438523, 'learning_rate': 4.683174603174603e-05, 'epoch': 0.19}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0034311346244066954, 'learning_rate': 4.675238095238095e-05, 'epoch': 0.2}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0043405755423009396, 'learning_rate': 4.667301587301587e-05, 'epoch': 0.2}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0010574525222182274, 'learning_rate': 4.6593650793650794e-05, 'epoch': 0.2}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0017744733486324549, 'learning_rate': 4.6514285714285715e-05, 'epoch': 0.21}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0029372945427894592, 'learning_rate': 4.643492063492064e-05, 'epoch': 0.21}\n",
      "{'loss': 0.0002, 'grad_norm': 0.0013802263420075178, 'learning_rate': 4.635555555555556e-05, 'epoch': 0.22}\n",
      "{'loss': 0.0004, 'grad_norm': 0.003935320768505335, 'learning_rate': 4.627619047619048e-05, 'epoch': 0.22}\n",
      "{'loss': 0.0001, 'grad_norm': 0.007879693061113358, 'learning_rate': 4.61968253968254e-05, 'epoch': 0.23}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0019219988025724888, 'learning_rate': 4.611746031746032e-05, 'epoch': 0.23}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0005670451791957021, 'learning_rate': 4.6038095238095244e-05, 'epoch': 0.24}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0016351398080587387, 'learning_rate': 4.595873015873016e-05, 'epoch': 0.24}\n",
      "{'loss': 0.0001, 'grad_norm': 0.001384149887599051, 'learning_rate': 4.587936507936508e-05, 'epoch': 0.25}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0020581777207553387, 'learning_rate': 4.58e-05, 'epoch': 0.25}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0006226941477507353, 'learning_rate': 4.5720634920634923e-05, 'epoch': 0.26}\n",
      "{'loss': 0.0002, 'grad_norm': 0.0008991258218884468, 'learning_rate': 4.5641269841269845e-05, 'epoch': 0.26}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0018565417267382145, 'learning_rate': 4.5561904761904766e-05, 'epoch': 0.27}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0008516940288245678, 'learning_rate': 4.548253968253968e-05, 'epoch': 0.27}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0006265753181651235, 'learning_rate': 4.54031746031746e-05, 'epoch': 0.28}\n",
      "{'loss': 0.0002, 'grad_norm': 0.00046782303252257407, 'learning_rate': 4.5323809523809524e-05, 'epoch': 0.28}\n",
      "{'loss': 0.0001, 'grad_norm': 0.000617757614236325, 'learning_rate': 4.5244444444444446e-05, 'epoch': 0.29}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0010004332289099693, 'learning_rate': 4.516507936507937e-05, 'epoch': 0.29}\n",
      "{'loss': 0.0001, 'grad_norm': 0.005406643729656935, 'learning_rate': 4.508571428571429e-05, 'epoch': 0.3}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0005017880466766655, 'learning_rate': 4.500634920634921e-05, 'epoch': 0.3}\n",
      "{'loss': 0.0, 'grad_norm': 0.0004475651658140123, 'learning_rate': 4.492698412698413e-05, 'epoch': 0.3}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0024637007154524326, 'learning_rate': 4.4847619047619046e-05, 'epoch': 0.31}\n",
      "{'loss': 0.0, 'grad_norm': 0.00048733415314927697, 'learning_rate': 4.476825396825397e-05, 'epoch': 0.31}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0005047989543527365, 'learning_rate': 4.468888888888889e-05, 'epoch': 0.32}\n",
      "{'loss': 0.0001, 'grad_norm': 0.000307620910461992, 'learning_rate': 4.460952380952381e-05, 'epoch': 0.32}\n",
      "{'loss': 0.0001, 'grad_norm': 0.00024200865300372243, 'learning_rate': 4.453015873015873e-05, 'epoch': 0.33}\n",
      "{'loss': 0.0, 'grad_norm': 0.000353987910784781, 'learning_rate': 4.4450793650793654e-05, 'epoch': 0.33}\n",
      "{'loss': 0.0, 'grad_norm': 0.00022570692817680538, 'learning_rate': 4.4371428571428575e-05, 'epoch': 0.34}\n",
      "{'loss': 0.0, 'grad_norm': 0.0002980642602778971, 'learning_rate': 4.42920634920635e-05, 'epoch': 0.34}\n",
      "{'loss': 0.0, 'grad_norm': 0.0018516721902415156, 'learning_rate': 4.421269841269842e-05, 'epoch': 0.35}\n",
      "{'loss': 0.0, 'grad_norm': 0.00044633910874836147, 'learning_rate': 4.413333333333334e-05, 'epoch': 0.35}\n",
      "{'loss': 0.0, 'grad_norm': 0.0004437240422703326, 'learning_rate': 4.405396825396826e-05, 'epoch': 0.36}\n",
      "{'loss': 0.0, 'grad_norm': 0.0005332389264367521, 'learning_rate': 4.3974603174603176e-05, 'epoch': 0.36}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0016005475772544742, 'learning_rate': 4.38952380952381e-05, 'epoch': 0.37}\n",
      "{'loss': 0.0, 'grad_norm': 0.0005237254081293941, 'learning_rate': 4.381587301587301e-05, 'epoch': 0.37}\n",
      "{'loss': 0.0, 'grad_norm': 0.0012816744856536388, 'learning_rate': 4.3736507936507934e-05, 'epoch': 0.38}\n",
      "{'loss': 0.0, 'grad_norm': 0.011345384642481804, 'learning_rate': 4.3657142857142855e-05, 'epoch': 0.38}\n",
      "{'loss': 0.0003, 'grad_norm': 0.00014075626677367836, 'learning_rate': 4.357777777777778e-05, 'epoch': 0.39}\n",
      "{'loss': 0.0001, 'grad_norm': 0.002337393118068576, 'learning_rate': 4.34984126984127e-05, 'epoch': 0.39}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0006625905516557395, 'learning_rate': 4.341904761904762e-05, 'epoch': 0.4}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0027911856304854155, 'learning_rate': 4.333968253968254e-05, 'epoch': 0.4}\n",
      "{'loss': 0.0, 'grad_norm': 0.00027642204076983035, 'learning_rate': 4.326031746031746e-05, 'epoch': 0.4}\n",
      "{'loss': 0.0, 'grad_norm': 0.00046081069740466774, 'learning_rate': 4.3180952380952384e-05, 'epoch': 0.41}\n",
      "{'loss': 0.0, 'grad_norm': 0.0004084677784703672, 'learning_rate': 4.3101587301587306e-05, 'epoch': 0.41}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0007137068314477801, 'learning_rate': 4.302222222222223e-05, 'epoch': 0.42}\n",
      "{'loss': 0.0, 'grad_norm': 0.00023898751533124596, 'learning_rate': 4.294285714285715e-05, 'epoch': 0.42}\n",
      "{'loss': 0.0, 'grad_norm': 0.00116393540520221, 'learning_rate': 4.2863492063492064e-05, 'epoch': 0.43}\n",
      "{'loss': 0.0, 'grad_norm': 0.00043600230128504336, 'learning_rate': 4.2784126984126985e-05, 'epoch': 0.43}\n",
      "{'loss': 0.0, 'grad_norm': 0.0003666780539788306, 'learning_rate': 4.270476190476191e-05, 'epoch': 0.44}\n",
      "{'loss': 0.0, 'grad_norm': 0.0007042628712952137, 'learning_rate': 4.262539682539683e-05, 'epoch': 0.44}\n",
      "{'loss': 0.0, 'grad_norm': 0.00026400695787742734, 'learning_rate': 4.254603174603175e-05, 'epoch': 0.45}\n",
      "{'loss': 0.0, 'grad_norm': 0.0012406331952661276, 'learning_rate': 4.246666666666667e-05, 'epoch': 0.45}\n",
      "{'loss': 0.0, 'grad_norm': 0.030567357316613197, 'learning_rate': 4.2387301587301586e-05, 'epoch': 0.46}\n",
      "{'loss': 0.0, 'grad_norm': 0.00805357750505209, 'learning_rate': 4.230793650793651e-05, 'epoch': 0.46}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0008000796078704298, 'learning_rate': 4.222857142857143e-05, 'epoch': 0.47}\n",
      "{'loss': 0.0, 'grad_norm': 0.00017257063882425427, 'learning_rate': 4.214920634920635e-05, 'epoch': 0.47}\n",
      "{'loss': 0.0, 'grad_norm': 0.00017284456407651305, 'learning_rate': 4.206984126984127e-05, 'epoch': 0.48}\n",
      "{'loss': 0.0, 'grad_norm': 0.000138477684231475, 'learning_rate': 4.1990476190476193e-05, 'epoch': 0.48}\n",
      "{'loss': 0.0, 'grad_norm': 0.0010160940000787377, 'learning_rate': 4.1911111111111115e-05, 'epoch': 0.49}\n",
      "{'loss': 0.0, 'grad_norm': 0.00027294392930343747, 'learning_rate': 4.183174603174603e-05, 'epoch': 0.49}\n",
      "{'loss': 0.0, 'grad_norm': 9.437740663997829e-05, 'learning_rate': 4.175238095238095e-05, 'epoch': 0.5}\n",
      "{'loss': 0.0, 'grad_norm': 0.0008672979311086237, 'learning_rate': 4.167301587301587e-05, 'epoch': 0.5}\n",
      "{'loss': 0.0, 'grad_norm': 0.00037190367584116757, 'learning_rate': 4.1593650793650794e-05, 'epoch': 0.5}\n",
      "{'loss': 0.0, 'grad_norm': 0.00011801970686065033, 'learning_rate': 4.1514285714285716e-05, 'epoch': 0.51}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0002572065859567374, 'learning_rate': 4.143492063492064e-05, 'epoch': 0.51}\n",
      "{'loss': 0.0, 'grad_norm': 0.0010111179435625672, 'learning_rate': 4.135555555555556e-05, 'epoch': 0.52}\n",
      "{'loss': 0.0, 'grad_norm': 9.228711132891476e-05, 'learning_rate': 4.127619047619048e-05, 'epoch': 0.52}\n",
      "{'loss': 0.0, 'grad_norm': 7.079844363033772e-05, 'learning_rate': 4.11968253968254e-05, 'epoch': 0.53}\n",
      "{'loss': 0.0, 'grad_norm': 0.00567658944055438, 'learning_rate': 4.111746031746032e-05, 'epoch': 0.53}\n",
      "{'loss': 0.0, 'grad_norm': 7.620297401444986e-05, 'learning_rate': 4.1038095238095245e-05, 'epoch': 0.54}\n",
      "{'loss': 0.0, 'grad_norm': 0.00031274158391170204, 'learning_rate': 4.0958730158730166e-05, 'epoch': 0.54}\n",
      "{'loss': 0.0002, 'grad_norm': 0.000481855939142406, 'learning_rate': 4.087936507936508e-05, 'epoch': 0.55}\n",
      "{'loss': 0.0, 'grad_norm': 0.00014912657206878066, 'learning_rate': 4.08e-05, 'epoch': 0.55}\n",
      "{'loss': 0.0, 'grad_norm': 0.000331924733472988, 'learning_rate': 4.072063492063492e-05, 'epoch': 0.56}\n",
      "{'loss': 0.0, 'grad_norm': 0.00043832711526192725, 'learning_rate': 4.064126984126984e-05, 'epoch': 0.56}\n",
      "{'loss': 0.0, 'grad_norm': 0.0002755206951405853, 'learning_rate': 4.056190476190476e-05, 'epoch': 0.57}\n",
      "{'loss': 0.0, 'grad_norm': 0.0002956732932943851, 'learning_rate': 4.048253968253968e-05, 'epoch': 0.57}\n",
      "{'loss': 0.0, 'grad_norm': 0.0003476231067907065, 'learning_rate': 4.04031746031746e-05, 'epoch': 0.58}\n",
      "{'loss': 0.0, 'grad_norm': 0.00016803287144284695, 'learning_rate': 4.0323809523809525e-05, 'epoch': 0.58}\n",
      "{'loss': 0.0, 'grad_norm': 0.0007108052377589047, 'learning_rate': 4.0244444444444446e-05, 'epoch': 0.59}\n",
      "{'loss': 0.0, 'grad_norm': 0.00020702346228063107, 'learning_rate': 4.016507936507937e-05, 'epoch': 0.59}\n",
      "{'loss': 0.0, 'grad_norm': 0.00012329952733125538, 'learning_rate': 4.008571428571429e-05, 'epoch': 0.6}\n",
      "{'loss': 0.0001, 'grad_norm': 0.00010895968443946913, 'learning_rate': 4.000634920634921e-05, 'epoch': 0.6}\n",
      "{'loss': 0.0, 'grad_norm': 0.00033715603058226407, 'learning_rate': 3.992698412698413e-05, 'epoch': 0.6}\n",
      "{'loss': 0.0, 'grad_norm': 0.0001913968299049884, 'learning_rate': 3.9847619047619054e-05, 'epoch': 0.61}\n",
      "{'loss': 0.0, 'grad_norm': 9.62837366387248e-05, 'learning_rate': 3.976825396825397e-05, 'epoch': 0.61}\n",
      "{'loss': 0.0, 'grad_norm': 0.000235848143347539, 'learning_rate': 3.968888888888889e-05, 'epoch': 0.62}\n",
      "{'loss': 0.0, 'grad_norm': 0.00044241046998649836, 'learning_rate': 3.960952380952381e-05, 'epoch': 0.62}\n",
      "{'loss': 0.0, 'grad_norm': 0.0001662283466430381, 'learning_rate': 3.953015873015873e-05, 'epoch': 0.63}\n",
      "{'loss': 0.0, 'grad_norm': 7.192938210209832e-05, 'learning_rate': 3.9450793650793654e-05, 'epoch': 0.63}\n",
      "{'loss': 0.0, 'grad_norm': 0.00023705266357865185, 'learning_rate': 3.9371428571428576e-05, 'epoch': 0.64}\n",
      "{'loss': 0.0, 'grad_norm': 8.543027070118114e-05, 'learning_rate': 3.92920634920635e-05, 'epoch': 0.64}\n",
      "{'loss': 0.0, 'grad_norm': 0.00025199929950758815, 'learning_rate': 3.921269841269841e-05, 'epoch': 0.65}\n",
      "{'loss': 0.0, 'grad_norm': 0.00068164017284289, 'learning_rate': 3.9133333333333334e-05, 'epoch': 0.65}\n",
      "{'loss': 0.0, 'grad_norm': 0.00013656307419296354, 'learning_rate': 3.9053968253968255e-05, 'epoch': 0.66}\n",
      "{'loss': 0.0, 'grad_norm': 0.0002933189971372485, 'learning_rate': 3.897460317460318e-05, 'epoch': 0.66}\n",
      "{'loss': 0.0, 'grad_norm': 0.00013137751375325024, 'learning_rate': 3.88952380952381e-05, 'epoch': 0.67}\n",
      "{'loss': 0.0, 'grad_norm': 0.0001093845785362646, 'learning_rate': 3.881587301587302e-05, 'epoch': 0.67}\n",
      "{'loss': 0.0, 'grad_norm': 0.0003678914508782327, 'learning_rate': 3.8736507936507934e-05, 'epoch': 0.68}\n",
      "{'loss': 0.0, 'grad_norm': 7.980447844602168e-05, 'learning_rate': 3.8657142857142856e-05, 'epoch': 0.68}\n",
      "{'loss': 0.0, 'grad_norm': 0.00033595151035115123, 'learning_rate': 3.857777777777778e-05, 'epoch': 0.69}\n",
      "{'loss': 0.0, 'grad_norm': 0.00014984302106313407, 'learning_rate': 3.84984126984127e-05, 'epoch': 0.69}\n",
      "{'loss': 0.0, 'grad_norm': 6.661481893388554e-05, 'learning_rate': 3.841904761904762e-05, 'epoch': 0.7}\n",
      "{'loss': 0.0, 'grad_norm': 0.00034080553450621665, 'learning_rate': 3.833968253968254e-05, 'epoch': 0.7}\n",
      "{'loss': 0.0, 'grad_norm': 0.000452135456725955, 'learning_rate': 3.826031746031746e-05, 'epoch': 0.7}\n",
      "{'loss': 0.0, 'grad_norm': 7.302218728000298e-05, 'learning_rate': 3.8180952380952385e-05, 'epoch': 0.71}\n",
      "{'loss': 0.0, 'grad_norm': 0.00011351880675647408, 'learning_rate': 3.8101587301587306e-05, 'epoch': 0.71}\n",
      "{'loss': 0.0, 'grad_norm': 5.668379890266806e-05, 'learning_rate': 3.802222222222223e-05, 'epoch': 0.72}\n",
      "{'loss': 0.0, 'grad_norm': 5.507333480636589e-05, 'learning_rate': 3.794285714285715e-05, 'epoch': 0.72}\n",
      "{'loss': 0.0, 'grad_norm': 7.870215631555766e-05, 'learning_rate': 3.786349206349207e-05, 'epoch': 0.73}\n",
      "{'loss': 0.0, 'grad_norm': 4.5853292249375954e-05, 'learning_rate': 3.7784126984126986e-05, 'epoch': 0.73}\n",
      "{'loss': 0.0, 'grad_norm': 0.00018404277216177434, 'learning_rate': 3.770476190476191e-05, 'epoch': 0.74}\n",
      "{'loss': 0.0003, 'grad_norm': 7.439383625751361e-05, 'learning_rate': 3.762698412698413e-05, 'epoch': 0.74}\n",
      "{'loss': 0.0, 'grad_norm': 6.460112490458414e-05, 'learning_rate': 3.7547619047619045e-05, 'epoch': 0.75}\n",
      "{'loss': 0.0, 'grad_norm': 3.265248233219609e-05, 'learning_rate': 3.746825396825397e-05, 'epoch': 0.75}\n",
      "{'loss': 0.0, 'grad_norm': 2.2670590624329634e-05, 'learning_rate': 3.738888888888889e-05, 'epoch': 0.76}\n",
      "{'loss': 0.0, 'grad_norm': 3.362853385624476e-05, 'learning_rate': 3.730952380952381e-05, 'epoch': 0.76}\n",
      "{'loss': 0.0, 'grad_norm': 6.320677493931726e-05, 'learning_rate': 3.723015873015873e-05, 'epoch': 0.77}\n",
      "{'loss': 0.0, 'grad_norm': 0.00017555222439114004, 'learning_rate': 3.715079365079365e-05, 'epoch': 0.77}\n",
      "{'loss': 0.0, 'grad_norm': 6.822155410191044e-05, 'learning_rate': 3.7071428571428574e-05, 'epoch': 0.78}\n",
      "{'loss': 0.0, 'grad_norm': 0.00013220272376202047, 'learning_rate': 3.6992063492063496e-05, 'epoch': 0.78}\n",
      "{'loss': 0.0, 'grad_norm': 0.00022351248480845243, 'learning_rate': 3.691269841269842e-05, 'epoch': 0.79}\n",
      "{'loss': 0.0, 'grad_norm': 8.04521914687939e-05, 'learning_rate': 3.683333333333334e-05, 'epoch': 0.79}\n",
      "{'loss': 0.0, 'grad_norm': 0.00010332519741496071, 'learning_rate': 3.675396825396826e-05, 'epoch': 0.8}\n",
      "{'loss': 0.0, 'grad_norm': 0.0003523457853589207, 'learning_rate': 3.6674603174603175e-05, 'epoch': 0.8}\n",
      "{'loss': 0.0, 'grad_norm': 4.8963232984533533e-05, 'learning_rate': 3.6595238095238096e-05, 'epoch': 0.8}\n",
      "{'loss': 0.0, 'grad_norm': 6.0167123592691496e-05, 'learning_rate': 3.651587301587302e-05, 'epoch': 0.81}\n",
      "{'loss': 0.0, 'grad_norm': 6.022190427756868e-05, 'learning_rate': 3.643650793650794e-05, 'epoch': 0.81}\n",
      "{'loss': 0.0, 'grad_norm': 0.0002945904270745814, 'learning_rate': 3.6357142857142854e-05, 'epoch': 0.82}\n",
      "{'loss': 0.0, 'grad_norm': 5.1657760195666924e-05, 'learning_rate': 3.6277777777777776e-05, 'epoch': 0.82}\n",
      "{'loss': 0.0, 'grad_norm': 0.000370457157259807, 'learning_rate': 3.61984126984127e-05, 'epoch': 0.83}\n",
      "{'loss': 0.0, 'grad_norm': 4.364171763882041e-05, 'learning_rate': 3.611904761904762e-05, 'epoch': 0.83}\n",
      "{'loss': 0.0, 'grad_norm': 0.0001446750684408471, 'learning_rate': 3.603968253968254e-05, 'epoch': 0.84}\n",
      "{'loss': 0.0, 'grad_norm': 6.880040746182203e-05, 'learning_rate': 3.596031746031746e-05, 'epoch': 0.84}\n",
      "{'loss': 0.0, 'grad_norm': 7.014916627667844e-05, 'learning_rate': 3.588095238095238e-05, 'epoch': 0.85}\n",
      "{'loss': 0.0, 'grad_norm': 9.256159682990983e-05, 'learning_rate': 3.5801587301587305e-05, 'epoch': 0.85}\n",
      "{'loss': 0.0, 'grad_norm': 0.0010718924459069967, 'learning_rate': 3.5722222222222226e-05, 'epoch': 0.86}\n",
      "{'loss': 0.0, 'grad_norm': 5.8898011047858745e-05, 'learning_rate': 3.564285714285715e-05, 'epoch': 0.86}\n",
      "{'loss': 0.0, 'grad_norm': 3.046225720026996e-05, 'learning_rate': 3.556349206349206e-05, 'epoch': 0.87}\n",
      "{'loss': 0.0, 'grad_norm': 4.195066139800474e-05, 'learning_rate': 3.5484126984126984e-05, 'epoch': 0.87}\n",
      "{'loss': 0.0, 'grad_norm': 3.731206015800126e-05, 'learning_rate': 3.5404761904761905e-05, 'epoch': 0.88}\n",
      "{'loss': 0.0, 'grad_norm': 4.450957567314617e-05, 'learning_rate': 3.532539682539683e-05, 'epoch': 0.88}\n",
      "{'loss': 0.0, 'grad_norm': 6.169574771774933e-05, 'learning_rate': 3.524603174603175e-05, 'epoch': 0.89}\n",
      "{'loss': 0.0, 'grad_norm': 2.79776613751892e-05, 'learning_rate': 3.516666666666667e-05, 'epoch': 0.89}\n",
      "{'loss': 0.0, 'grad_norm': 0.00031485166982747614, 'learning_rate': 3.508730158730159e-05, 'epoch': 0.9}\n",
      "{'loss': 0.0, 'grad_norm': 9.518521255813539e-05, 'learning_rate': 3.500793650793651e-05, 'epoch': 0.9}\n",
      "{'loss': 0.0, 'grad_norm': 2.8617743737413548e-05, 'learning_rate': 3.4928571428571434e-05, 'epoch': 0.9}\n",
      "{'loss': 0.0, 'grad_norm': 4.103291576029733e-05, 'learning_rate': 3.4849206349206356e-05, 'epoch': 0.91}\n",
      "{'loss': 0.0, 'grad_norm': 2.8625421691685915e-05, 'learning_rate': 3.476984126984127e-05, 'epoch': 0.91}\n",
      "{'loss': 0.0, 'grad_norm': 3.981158806709573e-05, 'learning_rate': 3.469047619047619e-05, 'epoch': 0.92}\n",
      "{'loss': 0.0, 'grad_norm': 5.977210821583867e-05, 'learning_rate': 3.4611111111111114e-05, 'epoch': 0.92}\n",
      "{'loss': 0.0, 'grad_norm': 4.295297549106181e-05, 'learning_rate': 3.453174603174603e-05, 'epoch': 0.93}\n",
      "{'loss': 0.0, 'grad_norm': 1.5200930647552013e-05, 'learning_rate': 3.445238095238095e-05, 'epoch': 0.93}\n",
      "{'loss': 0.0, 'grad_norm': 1.7942955309990793e-05, 'learning_rate': 3.437301587301587e-05, 'epoch': 0.94}\n",
      "{'loss': 0.0, 'grad_norm': 4.754670590045862e-05, 'learning_rate': 3.429365079365079e-05, 'epoch': 0.94}\n",
      "{'loss': 0.0, 'grad_norm': 4.61168892798014e-05, 'learning_rate': 3.4214285714285714e-05, 'epoch': 0.95}\n",
      "{'loss': 0.0, 'grad_norm': 4.31426233262755e-05, 'learning_rate': 3.4134920634920636e-05, 'epoch': 0.95}\n",
      "{'loss': 0.0, 'grad_norm': 6.285253766691312e-05, 'learning_rate': 3.405555555555556e-05, 'epoch': 0.96}\n",
      "{'loss': 0.0, 'grad_norm': 0.0011531513882800937, 'learning_rate': 3.397619047619048e-05, 'epoch': 0.96}\n",
      "{'loss': 0.0, 'grad_norm': 0.0004572603793349117, 'learning_rate': 3.38968253968254e-05, 'epoch': 0.97}\n",
      "{'loss': 0.0, 'grad_norm': 3.984509748988785e-05, 'learning_rate': 3.381746031746032e-05, 'epoch': 0.97}\n",
      "{'loss': 0.0, 'grad_norm': 8.992151560960338e-05, 'learning_rate': 3.3738095238095243e-05, 'epoch': 0.98}\n",
      "{'loss': 0.0, 'grad_norm': 2.214966298197396e-05, 'learning_rate': 3.3658730158730165e-05, 'epoch': 0.98}\n",
      "{'loss': 0.0, 'grad_norm': 0.00064679334172979, 'learning_rate': 3.357936507936508e-05, 'epoch': 0.99}\n",
      "{'loss': 0.0, 'grad_norm': 2.4714554456295446e-05, 'learning_rate': 3.35e-05, 'epoch': 0.99}\n",
      "{'loss': 0.0, 'grad_norm': 0.00011969198385486379, 'learning_rate': 3.342063492063492e-05, 'epoch': 1.0}\n",
      "{'loss': 0.0, 'grad_norm': 4.551539313979447e-05, 'learning_rate': 3.3341269841269844e-05, 'epoch': 1.0}\n",
      "{'loss': 0.0, 'grad_norm': 3.035864392586518e-05, 'learning_rate': 3.326190476190476e-05, 'epoch': 1.0}\n",
      "{'loss': 0.0, 'grad_norm': 2.146975020878017e-05, 'learning_rate': 3.318253968253968e-05, 'epoch': 1.01}\n",
      "{'loss': 0.0, 'grad_norm': 5.0829399697249755e-05, 'learning_rate': 3.31031746031746e-05, 'epoch': 1.01}\n",
      "{'loss': 0.0, 'grad_norm': 1.648170109547209e-05, 'learning_rate': 3.302380952380952e-05, 'epoch': 1.02}\n",
      "{'loss': 0.0, 'grad_norm': 1.5473171515623108e-05, 'learning_rate': 3.2944444444444445e-05, 'epoch': 1.02}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0002583511231932789, 'learning_rate': 3.286666666666667e-05, 'epoch': 1.03}\n",
      "{'loss': 0.0, 'grad_norm': 0.00022796647681389004, 'learning_rate': 3.278730158730159e-05, 'epoch': 1.03}\n",
      "{'loss': 0.0, 'grad_norm': 1.9622690160758793e-05, 'learning_rate': 3.270793650793651e-05, 'epoch': 1.04}\n",
      "{'loss': 0.0, 'grad_norm': 3.992006168118678e-05, 'learning_rate': 3.262857142857143e-05, 'epoch': 1.04}\n",
      "{'loss': 0.0, 'grad_norm': 1.866069396783132e-05, 'learning_rate': 3.2549206349206354e-05, 'epoch': 1.05}\n",
      "{'loss': 0.0, 'grad_norm': 1.8485909095034003e-05, 'learning_rate': 3.2469841269841276e-05, 'epoch': 1.05}\n",
      "{'loss': 0.0, 'grad_norm': 6.511698302347213e-05, 'learning_rate': 3.239047619047619e-05, 'epoch': 1.06}\n",
      "{'loss': 0.0, 'grad_norm': 1.6950893041212112e-05, 'learning_rate': 3.231111111111111e-05, 'epoch': 1.06}\n",
      "{'loss': 0.0, 'grad_norm': 2.1163032215554267e-05, 'learning_rate': 3.2231746031746033e-05, 'epoch': 1.07}\n",
      "{'loss': 0.0, 'grad_norm': 8.103758773359004e-06, 'learning_rate': 3.2152380952380955e-05, 'epoch': 1.07}\n",
      "{'loss': 0.0, 'grad_norm': 1.2337940461293329e-05, 'learning_rate': 3.2073015873015876e-05, 'epoch': 1.08}\n",
      "{'loss': 0.0004, 'grad_norm': 6.672146992059425e-05, 'learning_rate': 3.19936507936508e-05, 'epoch': 1.08}\n",
      "{'loss': 0.0, 'grad_norm': 0.0001592149055795744, 'learning_rate': 3.191428571428571e-05, 'epoch': 1.09}\n",
      "{'loss': 0.0, 'grad_norm': 0.0008157910779118538, 'learning_rate': 3.1834920634920634e-05, 'epoch': 1.09}\n",
      "{'loss': 0.0, 'grad_norm': 3.283569458290003e-05, 'learning_rate': 3.1755555555555556e-05, 'epoch': 1.1}\n",
      "{'loss': 0.0, 'grad_norm': 4.148424704908393e-05, 'learning_rate': 3.167619047619048e-05, 'epoch': 1.1}\n",
      "{'loss': 0.0, 'grad_norm': 2.3357608370133676e-05, 'learning_rate': 3.15968253968254e-05, 'epoch': 1.1}\n",
      "{'loss': 0.0, 'grad_norm': 4.658731631934643e-05, 'learning_rate': 3.151746031746032e-05, 'epoch': 1.11}\n",
      "{'loss': 0.0, 'grad_norm': 9.447424235986546e-05, 'learning_rate': 3.143809523809524e-05, 'epoch': 1.11}\n",
      "{'loss': 0.0, 'grad_norm': 4.797476140083745e-05, 'learning_rate': 3.1358730158730156e-05, 'epoch': 1.12}\n",
      "{'loss': 0.0, 'grad_norm': 2.8256705263629556e-05, 'learning_rate': 3.127936507936508e-05, 'epoch': 1.12}\n",
      "{'loss': 0.0, 'grad_norm': 0.0015393892535939813, 'learning_rate': 3.12e-05, 'epoch': 1.13}\n",
      "{'loss': 0.0, 'grad_norm': 1.0233606190013234e-05, 'learning_rate': 3.112063492063492e-05, 'epoch': 1.13}\n",
      "{'loss': 0.0, 'grad_norm': 1.0521696822252125e-05, 'learning_rate': 3.104126984126984e-05, 'epoch': 1.14}\n",
      "{'loss': 0.0, 'grad_norm': 9.100217721424997e-05, 'learning_rate': 3.0961904761904764e-05, 'epoch': 1.14}\n",
      "{'loss': 0.0, 'grad_norm': 8.036720828386024e-05, 'learning_rate': 3.0882539682539685e-05, 'epoch': 1.15}\n",
      "{'loss': 0.0, 'grad_norm': 9.381047129863873e-05, 'learning_rate': 3.080317460317461e-05, 'epoch': 1.15}\n",
      "{'loss': 0.0, 'grad_norm': 1.907906334963627e-05, 'learning_rate': 3.072380952380953e-05, 'epoch': 1.16}\n",
      "{'loss': 0.0, 'grad_norm': 9.54364295466803e-05, 'learning_rate': 3.064444444444445e-05, 'epoch': 1.16}\n",
      "{'loss': 0.0, 'grad_norm': 1.7630138245294802e-05, 'learning_rate': 3.056507936507937e-05, 'epoch': 1.17}\n",
      "{'loss': 0.0, 'grad_norm': 2.8735437808791175e-05, 'learning_rate': 3.048571428571429e-05, 'epoch': 1.17}\n",
      "{'loss': 0.0, 'grad_norm': 2.3696311473031528e-05, 'learning_rate': 3.040634920634921e-05, 'epoch': 1.18}\n",
      "{'loss': 0.0, 'grad_norm': 2.8421913157217205e-05, 'learning_rate': 3.0326984126984126e-05, 'epoch': 1.18}\n",
      "{'loss': 0.0, 'grad_norm': 7.240078411996365e-05, 'learning_rate': 3.0247619047619047e-05, 'epoch': 1.19}\n",
      "{'loss': 0.0, 'grad_norm': 2.9012884624535218e-05, 'learning_rate': 3.016825396825397e-05, 'epoch': 1.19}\n",
      "{'loss': 0.0, 'grad_norm': 7.87133103585802e-05, 'learning_rate': 3.008888888888889e-05, 'epoch': 1.2}\n",
      "{'loss': 0.0, 'grad_norm': 5.39059783477569e-06, 'learning_rate': 3.000952380952381e-05, 'epoch': 1.2}\n",
      "{'loss': 0.0, 'grad_norm': 1.6980044165393338e-05, 'learning_rate': 2.993015873015873e-05, 'epoch': 1.2}\n",
      "{'loss': 0.0, 'grad_norm': 1.3915183444623835e-05, 'learning_rate': 2.985079365079365e-05, 'epoch': 1.21}\n",
      "{'loss': 0.0, 'grad_norm': 8.859446097631007e-05, 'learning_rate': 2.9771428571428573e-05, 'epoch': 1.21}\n",
      "{'loss': 0.0, 'grad_norm': 1.8651606296771206e-05, 'learning_rate': 2.9692063492063494e-05, 'epoch': 1.22}\n",
      "{'loss': 0.0, 'grad_norm': 1.6148065697052516e-05, 'learning_rate': 2.9612698412698413e-05, 'epoch': 1.22}\n",
      "{'loss': 0.0, 'grad_norm': 1.5519493899773806e-05, 'learning_rate': 2.9533333333333334e-05, 'epoch': 1.23}\n",
      "{'loss': 0.0, 'grad_norm': 1.6392346879001707e-05, 'learning_rate': 2.9453968253968256e-05, 'epoch': 1.23}\n",
      "{'loss': 0.0, 'grad_norm': 2.5574889150448143e-05, 'learning_rate': 2.9374603174603177e-05, 'epoch': 1.24}\n",
      "{'loss': 0.0, 'grad_norm': 0.00012672827870119363, 'learning_rate': 2.92952380952381e-05, 'epoch': 1.24}\n",
      "{'loss': 0.0, 'grad_norm': 2.2394160623662174e-05, 'learning_rate': 2.921587301587302e-05, 'epoch': 1.25}\n",
      "{'loss': 0.0, 'grad_norm': 0.0007482781074941158, 'learning_rate': 2.9136507936507938e-05, 'epoch': 1.25}\n",
      "{'loss': 0.0, 'grad_norm': 2.1496747649507597e-05, 'learning_rate': 2.905714285714286e-05, 'epoch': 1.26}\n",
      "{'loss': 0.0, 'grad_norm': 8.113499643513933e-05, 'learning_rate': 2.897777777777778e-05, 'epoch': 1.26}\n",
      "{'loss': 0.0, 'grad_norm': 1.379768582410179e-05, 'learning_rate': 2.8898412698412703e-05, 'epoch': 1.27}\n",
      "{'loss': 0.0, 'grad_norm': 1.3545534784498159e-05, 'learning_rate': 2.8819047619047617e-05, 'epoch': 1.27}\n",
      "{'loss': 0.0, 'grad_norm': 2.3043925466481596e-05, 'learning_rate': 2.873968253968254e-05, 'epoch': 1.28}\n",
      "{'loss': 0.0, 'grad_norm': 1.0269847734889481e-05, 'learning_rate': 2.866031746031746e-05, 'epoch': 1.28}\n",
      "{'loss': 0.0, 'grad_norm': 1.3254176337795798e-05, 'learning_rate': 2.8580952380952382e-05, 'epoch': 1.29}\n",
      "{'loss': 0.0005, 'grad_norm': 2.3333164790528826e-05, 'learning_rate': 2.8503174603174605e-05, 'epoch': 1.29}\n",
      "{'loss': 0.0, 'grad_norm': 5.142752797837602e-06, 'learning_rate': 2.8423809523809523e-05, 'epoch': 1.3}\n",
      "{'loss': 0.0, 'grad_norm': 5.044874706072733e-05, 'learning_rate': 2.8344444444444445e-05, 'epoch': 1.3}\n",
      "{'loss': 0.0, 'grad_norm': 2.005303031182848e-05, 'learning_rate': 2.8265079365079366e-05, 'epoch': 1.3}\n",
      "{'loss': 0.0001, 'grad_norm': 2.715219306992367e-05, 'learning_rate': 2.8185714285714288e-05, 'epoch': 1.31}\n",
      "{'loss': 0.0, 'grad_norm': 1.2031979167659301e-05, 'learning_rate': 2.810634920634921e-05, 'epoch': 1.31}\n",
      "{'loss': 0.0, 'grad_norm': 0.0001551569002913311, 'learning_rate': 2.802698412698413e-05, 'epoch': 1.32}\n",
      "{'loss': 0.0, 'grad_norm': 4.8486261221114546e-05, 'learning_rate': 2.794761904761905e-05, 'epoch': 1.32}\n",
      "{'loss': 0.0, 'grad_norm': 2.0330397092038766e-05, 'learning_rate': 2.786825396825397e-05, 'epoch': 1.33}\n",
      "{'loss': 0.0, 'grad_norm': 1.629081816645339e-05, 'learning_rate': 2.7788888888888892e-05, 'epoch': 1.33}\n",
      "{'loss': 0.0, 'grad_norm': 1.909634556795936e-05, 'learning_rate': 2.7709523809523813e-05, 'epoch': 1.34}\n",
      "{'loss': 0.0, 'grad_norm': 3.0314149626065046e-05, 'learning_rate': 2.7630158730158735e-05, 'epoch': 1.34}\n",
      "{'loss': 0.0, 'grad_norm': 5.081984636490233e-05, 'learning_rate': 2.7550793650793657e-05, 'epoch': 1.35}\n",
      "{'loss': 0.0, 'grad_norm': 2.573867459432222e-05, 'learning_rate': 2.747142857142857e-05, 'epoch': 1.35}\n",
      "{'loss': 0.0, 'grad_norm': 4.4576034269994125e-05, 'learning_rate': 2.7392063492063493e-05, 'epoch': 1.36}\n",
      "{'loss': 0.0, 'grad_norm': 0.02671959437429905, 'learning_rate': 2.731269841269841e-05, 'epoch': 1.36}\n",
      "{'loss': 0.0, 'grad_norm': 2.2319845811580308e-05, 'learning_rate': 2.7233333333333332e-05, 'epoch': 1.37}\n",
      "{'loss': 0.0, 'grad_norm': 8.353314478881657e-05, 'learning_rate': 2.7153968253968254e-05, 'epoch': 1.37}\n",
      "{'loss': 0.0, 'grad_norm': 1.2023350791423582e-05, 'learning_rate': 2.7074603174603175e-05, 'epoch': 1.38}\n",
      "{'loss': 0.0, 'grad_norm': 1.3058269360044505e-05, 'learning_rate': 2.6995238095238097e-05, 'epoch': 1.38}\n",
      "{'loss': 0.0, 'grad_norm': 0.0001102682072087191, 'learning_rate': 2.6915873015873015e-05, 'epoch': 1.39}\n",
      "{'loss': 0.0, 'grad_norm': 4.362485924502835e-05, 'learning_rate': 2.6836507936507936e-05, 'epoch': 1.39}\n",
      "{'loss': 0.0, 'grad_norm': 0.000178805275936611, 'learning_rate': 2.6757142857142858e-05, 'epoch': 1.4}\n",
      "{'loss': 0.0, 'grad_norm': 2.7889122065971605e-05, 'learning_rate': 2.667777777777778e-05, 'epoch': 1.4}\n",
      "{'loss': 0.0, 'grad_norm': 0.00043764387373812497, 'learning_rate': 2.65984126984127e-05, 'epoch': 1.4}\n",
      "{'loss': 0.0, 'grad_norm': 1.1561008250282612e-05, 'learning_rate': 2.6519047619047622e-05, 'epoch': 1.41}\n",
      "{'loss': 0.0, 'grad_norm': 0.000709748303052038, 'learning_rate': 2.643968253968254e-05, 'epoch': 1.41}\n",
      "{'loss': 0.0, 'grad_norm': 3.9235386793734506e-05, 'learning_rate': 2.6360317460317462e-05, 'epoch': 1.42}\n",
      "{'loss': 0.0, 'grad_norm': 3.726227441802621e-05, 'learning_rate': 2.6280952380952384e-05, 'epoch': 1.42}\n",
      "{'loss': 0.0, 'grad_norm': 8.70738676894689e-06, 'learning_rate': 2.6201587301587305e-05, 'epoch': 1.43}\n",
      "{'loss': 0.0, 'grad_norm': 0.00012235130998305976, 'learning_rate': 2.6122222222222227e-05, 'epoch': 1.43}\n",
      "{'loss': 0.0, 'grad_norm': 7.270611968124285e-05, 'learning_rate': 2.6042857142857148e-05, 'epoch': 1.44}\n",
      "{'loss': 0.0, 'grad_norm': 2.4449191187159158e-05, 'learning_rate': 2.5963492063492063e-05, 'epoch': 1.44}\n",
      "{'loss': 0.0, 'grad_norm': 3.52314964402467e-05, 'learning_rate': 2.5884126984126984e-05, 'epoch': 1.45}\n",
      "{'loss': 0.0, 'grad_norm': 1.3532026059692726e-05, 'learning_rate': 2.5804761904761902e-05, 'epoch': 1.45}\n",
      "{'loss': 0.0, 'grad_norm': 5.979886191198602e-05, 'learning_rate': 2.5725396825396824e-05, 'epoch': 1.46}\n",
      "{'loss': 0.0, 'grad_norm': 0.000170415427419357, 'learning_rate': 2.5646031746031745e-05, 'epoch': 1.46}\n",
      "{'loss': 0.0, 'grad_norm': 5.8486650232225657e-05, 'learning_rate': 2.5566666666666667e-05, 'epoch': 1.47}\n",
      "{'loss': 0.0, 'grad_norm': 1.243736369360704e-05, 'learning_rate': 2.548730158730159e-05, 'epoch': 1.47}\n",
      "{'loss': 0.0, 'grad_norm': 2.188650796597358e-05, 'learning_rate': 2.540793650793651e-05, 'epoch': 1.48}\n",
      "{'loss': 0.0, 'grad_norm': 1.0796411515912041e-05, 'learning_rate': 2.5328571428571428e-05, 'epoch': 1.48}\n",
      "{'loss': 0.0, 'grad_norm': 3.48974353983067e-05, 'learning_rate': 2.524920634920635e-05, 'epoch': 1.49}\n",
      "{'loss': 0.0, 'grad_norm': 1.11761573862168e-05, 'learning_rate': 2.516984126984127e-05, 'epoch': 1.49}\n",
      "{'loss': 0.0, 'grad_norm': 8.792975677351933e-06, 'learning_rate': 2.5090476190476193e-05, 'epoch': 1.5}\n",
      "{'loss': 0.0, 'grad_norm': 5.686357690137811e-05, 'learning_rate': 2.5011111111111114e-05, 'epoch': 1.5}\n",
      "{'loss': 0.0, 'grad_norm': 1.1131310202472378e-05, 'learning_rate': 2.4931746031746032e-05, 'epoch': 1.5}\n",
      "{'loss': 0.0, 'grad_norm': 2.8728129109367728e-05, 'learning_rate': 2.4852380952380954e-05, 'epoch': 1.51}\n",
      "{'loss': 0.0, 'grad_norm': 2.286740163981449e-05, 'learning_rate': 2.4773015873015872e-05, 'epoch': 1.51}\n",
      "{'loss': 0.0, 'grad_norm': 2.041019979515113e-05, 'learning_rate': 2.4693650793650793e-05, 'epoch': 1.52}\n",
      "{'loss': 0.0, 'grad_norm': 1.943600545928348e-05, 'learning_rate': 2.4614285714285715e-05, 'epoch': 1.52}\n",
      "{'loss': 0.0, 'grad_norm': 0.0002840090310201049, 'learning_rate': 2.4534920634920636e-05, 'epoch': 1.53}\n",
      "{'loss': 0.0, 'grad_norm': 9.263184438168537e-06, 'learning_rate': 2.4455555555555558e-05, 'epoch': 1.53}\n",
      "{'loss': 0.0, 'grad_norm': 7.119643123587593e-05, 'learning_rate': 2.4376190476190476e-05, 'epoch': 1.54}\n",
      "{'loss': 0.0, 'grad_norm': 3.4909160604001954e-05, 'learning_rate': 2.4296825396825397e-05, 'epoch': 1.54}\n",
      "{'loss': 0.0, 'grad_norm': 9.933205546985846e-06, 'learning_rate': 2.421746031746032e-05, 'epoch': 1.55}\n",
      "{'loss': 0.0, 'grad_norm': 0.00014509369793813676, 'learning_rate': 2.413809523809524e-05, 'epoch': 1.55}\n",
      "{'loss': 0.0, 'grad_norm': 3.5878583730664104e-05, 'learning_rate': 2.4058730158730162e-05, 'epoch': 1.56}\n",
      "{'loss': 0.0, 'grad_norm': 3.855728937196545e-05, 'learning_rate': 2.397936507936508e-05, 'epoch': 1.56}\n",
      "{'loss': 0.0, 'grad_norm': 1.2152271665399894e-05, 'learning_rate': 2.39e-05, 'epoch': 1.57}\n",
      "{'loss': 0.0, 'grad_norm': 6.279956869548187e-05, 'learning_rate': 2.382063492063492e-05, 'epoch': 1.57}\n",
      "{'loss': 0.0, 'grad_norm': 1.4581833966076374e-05, 'learning_rate': 2.374126984126984e-05, 'epoch': 1.58}\n",
      "{'loss': 0.0, 'grad_norm': 7.275993993971497e-05, 'learning_rate': 2.3661904761904763e-05, 'epoch': 1.58}\n",
      "{'loss': 0.0, 'grad_norm': 4.0612329030409455e-05, 'learning_rate': 2.3582539682539684e-05, 'epoch': 1.59}\n",
      "{'loss': 0.0, 'grad_norm': 2.056889934465289e-05, 'learning_rate': 2.3503174603174606e-05, 'epoch': 1.59}\n",
      "{'loss': 0.0, 'grad_norm': 0.00015705358237028122, 'learning_rate': 2.3423809523809527e-05, 'epoch': 1.6}\n",
      "{'loss': 0.0, 'grad_norm': 1.4293258573161438e-05, 'learning_rate': 2.3344444444444445e-05, 'epoch': 1.6}\n",
      "{'loss': 0.0, 'grad_norm': 1.1991306564596016e-05, 'learning_rate': 2.3265079365079367e-05, 'epoch': 1.6}\n",
      "{'loss': 0.0, 'grad_norm': 7.861591257096734e-06, 'learning_rate': 2.3185714285714285e-05, 'epoch': 1.61}\n",
      "{'loss': 0.0, 'grad_norm': 6.791521445848048e-05, 'learning_rate': 2.3106349206349206e-05, 'epoch': 1.61}\n",
      "{'loss': 0.0, 'grad_norm': 5.8598270697984844e-05, 'learning_rate': 2.3026984126984128e-05, 'epoch': 1.62}\n",
      "{'loss': 0.0, 'grad_norm': 9.095112545765005e-06, 'learning_rate': 2.294761904761905e-05, 'epoch': 1.62}\n",
      "{'loss': 0.0, 'grad_norm': 0.00010820091847563162, 'learning_rate': 2.286825396825397e-05, 'epoch': 1.63}\n",
      "{'loss': 0.0, 'grad_norm': 8.419858204433694e-06, 'learning_rate': 2.278888888888889e-05, 'epoch': 1.63}\n",
      "{'loss': 0.0, 'grad_norm': 1.2743108527502045e-05, 'learning_rate': 2.270952380952381e-05, 'epoch': 1.64}\n",
      "{'loss': 0.0, 'grad_norm': 5.477769718709169e-06, 'learning_rate': 2.2630158730158732e-05, 'epoch': 1.64}\n",
      "{'loss': 0.0, 'grad_norm': 1.1550406270544045e-05, 'learning_rate': 2.2550793650793654e-05, 'epoch': 1.65}\n",
      "{'loss': 0.0, 'grad_norm': 3.801615457632579e-05, 'learning_rate': 2.2471428571428575e-05, 'epoch': 1.65}\n",
      "{'loss': 0.0, 'grad_norm': 1.4832324268354569e-05, 'learning_rate': 2.2392063492063493e-05, 'epoch': 1.66}\n",
      "{'loss': 0.0, 'grad_norm': 2.793559178826399e-05, 'learning_rate': 2.231269841269841e-05, 'epoch': 1.66}\n",
      "{'loss': 0.0, 'grad_norm': 4.597869974531932e-06, 'learning_rate': 2.2233333333333333e-05, 'epoch': 1.67}\n",
      "{'loss': 0.0, 'grad_norm': 2.485100776539184e-05, 'learning_rate': 2.2153968253968254e-05, 'epoch': 1.67}\n",
      "{'loss': 0.0, 'grad_norm': 7.3634632826724555e-06, 'learning_rate': 2.2074603174603176e-05, 'epoch': 1.68}\n",
      "{'loss': 0.0, 'grad_norm': 6.589867098227842e-06, 'learning_rate': 2.1995238095238097e-05, 'epoch': 1.68}\n",
      "{'loss': 0.0, 'grad_norm': 5.587257419392699e-06, 'learning_rate': 2.191587301587302e-05, 'epoch': 1.69}\n",
      "{'loss': 0.0, 'grad_norm': 3.49096953868866e-05, 'learning_rate': 2.1836507936507937e-05, 'epoch': 1.69}\n",
      "{'loss': 0.0, 'grad_norm': 2.975540337502025e-05, 'learning_rate': 2.175714285714286e-05, 'epoch': 1.7}\n",
      "{'loss': 0.0, 'grad_norm': 1.0840441973414272e-05, 'learning_rate': 2.167777777777778e-05, 'epoch': 1.7}\n",
      "{'loss': 0.0, 'grad_norm': 3.3450734918005764e-05, 'learning_rate': 2.1598412698412698e-05, 'epoch': 1.7}\n",
      "{'loss': 0.0, 'grad_norm': 4.426357008924242e-06, 'learning_rate': 2.151904761904762e-05, 'epoch': 1.71}\n",
      "{'loss': 0.0, 'grad_norm': 0.0003995794104412198, 'learning_rate': 2.143968253968254e-05, 'epoch': 1.71}\n",
      "{'loss': 0.0, 'grad_norm': 8.453967893728986e-06, 'learning_rate': 2.1360317460317463e-05, 'epoch': 1.72}\n",
      "{'loss': 0.0, 'grad_norm': 4.656314558815211e-05, 'learning_rate': 2.128095238095238e-05, 'epoch': 1.72}\n",
      "{'loss': 0.0, 'grad_norm': 4.670088401326211e-06, 'learning_rate': 2.1201587301587302e-05, 'epoch': 1.73}\n",
      "{'loss': 0.0, 'grad_norm': 6.8332233240653295e-06, 'learning_rate': 2.1122222222222224e-05, 'epoch': 1.73}\n",
      "{'loss': 0.0, 'grad_norm': 2.3525744836661033e-05, 'learning_rate': 2.1042857142857145e-05, 'epoch': 1.74}\n",
      "{'loss': 0.0, 'grad_norm': 5.8071942476090044e-05, 'learning_rate': 2.0963492063492067e-05, 'epoch': 1.74}\n",
      "{'loss': 0.0, 'grad_norm': 2.0255029085092247e-05, 'learning_rate': 2.0884126984126985e-05, 'epoch': 1.75}\n",
      "{'loss': 0.0, 'grad_norm': 1.9796934793703258e-05, 'learning_rate': 2.0804761904761906e-05, 'epoch': 1.75}\n",
      "{'loss': 0.0, 'grad_norm': 1.8721762899076566e-05, 'learning_rate': 2.0725396825396824e-05, 'epoch': 1.76}\n",
      "{'loss': 0.0, 'grad_norm': 0.00736728310585022, 'learning_rate': 2.0646031746031746e-05, 'epoch': 1.76}\n",
      "{'loss': 0.0, 'grad_norm': 1.222964419866912e-05, 'learning_rate': 2.0566666666666667e-05, 'epoch': 1.77}\n",
      "{'loss': 0.0, 'grad_norm': 1.427598726877477e-05, 'learning_rate': 2.048730158730159e-05, 'epoch': 1.77}\n",
      "{'loss': 0.0, 'grad_norm': 5.0398980420141015e-06, 'learning_rate': 2.040793650793651e-05, 'epoch': 1.78}\n",
      "{'loss': 0.0, 'grad_norm': 7.273982646438526e-06, 'learning_rate': 2.032857142857143e-05, 'epoch': 1.78}\n",
      "{'loss': 0.0, 'grad_norm': 3.903082415490644e-06, 'learning_rate': 2.024920634920635e-05, 'epoch': 1.79}\n",
      "{'loss': 0.0, 'grad_norm': 1.821285331971012e-05, 'learning_rate': 2.016984126984127e-05, 'epoch': 1.79}\n",
      "{'loss': 0.0, 'grad_norm': 1.8042799638351426e-05, 'learning_rate': 2.009047619047619e-05, 'epoch': 1.8}\n",
      "{'loss': 0.0, 'grad_norm': 6.723442766087828e-06, 'learning_rate': 2.001111111111111e-05, 'epoch': 1.8}\n",
      "{'loss': 0.0, 'grad_norm': 7.221491614473052e-06, 'learning_rate': 1.9931746031746033e-05, 'epoch': 1.8}\n",
      "{'loss': 0.0, 'grad_norm': 7.505901976401219e-06, 'learning_rate': 1.9852380952380954e-05, 'epoch': 1.81}\n",
      "{'loss': 0.0, 'grad_norm': 2.4155724531738088e-06, 'learning_rate': 1.9773015873015872e-05, 'epoch': 1.81}\n",
      "{'loss': 0.0, 'grad_norm': 1.7401695004082285e-05, 'learning_rate': 1.9693650793650794e-05, 'epoch': 1.82}\n",
      "{'loss': 0.0, 'grad_norm': 8.405891094298568e-06, 'learning_rate': 1.9614285714285715e-05, 'epoch': 1.82}\n",
      "{'loss': 0.0, 'grad_norm': 7.5834236668015365e-06, 'learning_rate': 1.9534920634920637e-05, 'epoch': 1.83}\n",
      "{'loss': 0.0, 'grad_norm': 2.9764716600766405e-05, 'learning_rate': 1.9455555555555558e-05, 'epoch': 1.83}\n",
      "{'loss': 0.0, 'grad_norm': 1.4266472135204822e-05, 'learning_rate': 1.937619047619048e-05, 'epoch': 1.84}\n",
      "{'loss': 0.0, 'grad_norm': 1.4090481272432953e-05, 'learning_rate': 1.9296825396825398e-05, 'epoch': 1.84}\n",
      "{'loss': 0.0, 'grad_norm': 4.1865087041514926e-06, 'learning_rate': 1.9217460317460316e-05, 'epoch': 1.85}\n",
      "{'loss': 0.0, 'grad_norm': 1.7079803001252003e-05, 'learning_rate': 1.9138095238095237e-05, 'epoch': 1.85}\n",
      "{'loss': 0.0, 'grad_norm': 0.0009156209998764098, 'learning_rate': 1.905873015873016e-05, 'epoch': 1.86}\n",
      "{'loss': 0.0, 'grad_norm': 1.4125351299298927e-05, 'learning_rate': 1.897936507936508e-05, 'epoch': 1.86}\n",
      "{'loss': 0.0, 'grad_norm': 6.1205691963550635e-06, 'learning_rate': 1.8900000000000002e-05, 'epoch': 1.87}\n",
      "{'loss': 0.0, 'grad_norm': 7.227582500490826e-06, 'learning_rate': 1.8820634920634924e-05, 'epoch': 1.87}\n",
      "{'loss': 0.0, 'grad_norm': 7.72654493630398e-06, 'learning_rate': 1.874126984126984e-05, 'epoch': 1.88}\n",
      "{'loss': 0.0, 'grad_norm': 4.894023732049391e-06, 'learning_rate': 1.8661904761904763e-05, 'epoch': 1.88}\n",
      "{'loss': 0.0, 'grad_norm': 8.176788469427265e-06, 'learning_rate': 1.8582539682539685e-05, 'epoch': 1.89}\n",
      "{'loss': 0.0, 'grad_norm': 4.324770998209715e-06, 'learning_rate': 1.8503174603174603e-05, 'epoch': 1.89}\n",
      "{'loss': 0.0, 'grad_norm': 3.0246542337408755e-06, 'learning_rate': 1.8423809523809524e-05, 'epoch': 1.9}\n",
      "{'loss': 0.0, 'grad_norm': 1.2687322850979399e-05, 'learning_rate': 1.8344444444444446e-05, 'epoch': 1.9}\n",
      "{'loss': 0.0001, 'grad_norm': 1.0355703125242144e-05, 'learning_rate': 1.826666666666667e-05, 'epoch': 1.9}\n",
      "{'loss': 0.0, 'grad_norm': 6.313375251920661e-06, 'learning_rate': 1.818730158730159e-05, 'epoch': 1.91}\n",
      "{'loss': 0.0, 'grad_norm': 5.930255611019675e-06, 'learning_rate': 1.810793650793651e-05, 'epoch': 1.91}\n",
      "{'loss': 0.0, 'grad_norm': 7.278621069417568e-06, 'learning_rate': 1.802857142857143e-05, 'epoch': 1.92}\n",
      "{'loss': 0.0, 'grad_norm': 4.0681838981981855e-06, 'learning_rate': 1.794920634920635e-05, 'epoch': 1.92}\n",
      "{'loss': 0.0, 'grad_norm': 4.9268226575804874e-05, 'learning_rate': 1.786984126984127e-05, 'epoch': 1.93}\n",
      "{'loss': 0.0, 'grad_norm': 9.132587365456857e-06, 'learning_rate': 1.779047619047619e-05, 'epoch': 1.93}\n",
      "{'loss': 0.0, 'grad_norm': 2.5486384402029216e-05, 'learning_rate': 1.7711111111111113e-05, 'epoch': 1.94}\n",
      "{'loss': 0.0, 'grad_norm': 3.3688224903016817e-06, 'learning_rate': 1.763174603174603e-05, 'epoch': 1.94}\n",
      "{'loss': 0.0, 'grad_norm': 9.08245419850573e-06, 'learning_rate': 1.7552380952380952e-05, 'epoch': 1.95}\n",
      "{'loss': 0.0, 'grad_norm': 1.2433448318915907e-05, 'learning_rate': 1.7473015873015874e-05, 'epoch': 1.95}\n",
      "{'loss': 0.0, 'grad_norm': 6.697493972751545e-06, 'learning_rate': 1.7393650793650795e-05, 'epoch': 1.96}\n",
      "{'loss': 0.0, 'grad_norm': 3.538201781339012e-06, 'learning_rate': 1.7314285714285717e-05, 'epoch': 1.96}\n",
      "{'loss': 0.0, 'grad_norm': 1.543738108011894e-05, 'learning_rate': 1.723492063492064e-05, 'epoch': 1.97}\n",
      "{'loss': 0.0, 'grad_norm': 1.0974575161526445e-05, 'learning_rate': 1.7155555555555557e-05, 'epoch': 1.97}\n",
      "{'loss': 0.0, 'grad_norm': 1.3927356121712364e-05, 'learning_rate': 1.7076190476190475e-05, 'epoch': 1.98}\n",
      "{'loss': 0.0, 'grad_norm': 1.8629321857588366e-05, 'learning_rate': 1.6996825396825396e-05, 'epoch': 1.98}\n",
      "{'loss': 0.0, 'grad_norm': 3.8555253922822885e-06, 'learning_rate': 1.6917460317460318e-05, 'epoch': 1.99}\n",
      "{'loss': 0.0, 'grad_norm': 1.9919747501262464e-05, 'learning_rate': 1.683809523809524e-05, 'epoch': 1.99}\n",
      "{'loss': 0.0, 'grad_norm': 1.6879064787644893e-05, 'learning_rate': 1.675873015873016e-05, 'epoch': 2.0}\n",
      "{'loss': 0.0, 'grad_norm': 3.800883860094473e-05, 'learning_rate': 1.6679365079365082e-05, 'epoch': 2.0}\n",
      "{'loss': 0.0, 'grad_norm': 6.902479981363285e-06, 'learning_rate': 1.66e-05, 'epoch': 2.0}\n",
      "{'loss': 0.0, 'grad_norm': 8.766367500356864e-06, 'learning_rate': 1.6520634920634922e-05, 'epoch': 2.01}\n",
      "{'loss': 0.0, 'grad_norm': 8.456198884232435e-06, 'learning_rate': 1.644126984126984e-05, 'epoch': 2.01}\n",
      "{'loss': 0.0, 'grad_norm': 3.4090146527887555e-06, 'learning_rate': 1.636190476190476e-05, 'epoch': 2.02}\n",
      "{'loss': 0.0, 'grad_norm': 8.243601769208908e-06, 'learning_rate': 1.6282539682539683e-05, 'epoch': 2.02}\n",
      "{'loss': 0.0, 'grad_norm': 4.41716838395223e-06, 'learning_rate': 1.6203174603174604e-05, 'epoch': 2.03}\n",
      "{'loss': 0.0, 'grad_norm': 6.691302132821875e-06, 'learning_rate': 1.6123809523809526e-05, 'epoch': 2.03}\n",
      "{'loss': 0.0, 'grad_norm': 5.507837158802431e-06, 'learning_rate': 1.6044444444444444e-05, 'epoch': 2.04}\n",
      "{'loss': 0.0, 'grad_norm': 1.678418084338773e-05, 'learning_rate': 1.5965079365079366e-05, 'epoch': 2.04}\n",
      "{'loss': 0.0, 'grad_norm': 1.3761495210928842e-05, 'learning_rate': 1.5885714285714287e-05, 'epoch': 2.05}\n",
      "{'loss': 0.0, 'grad_norm': 0.0001147406583186239, 'learning_rate': 1.580634920634921e-05, 'epoch': 2.05}\n",
      "{'loss': 0.0, 'grad_norm': 2.9826992431480903e-06, 'learning_rate': 1.572698412698413e-05, 'epoch': 2.06}\n",
      "{'loss': 0.0, 'grad_norm': 4.9059080993174575e-06, 'learning_rate': 1.5647619047619048e-05, 'epoch': 2.06}\n",
      "{'loss': 0.0, 'grad_norm': 1.5321814998969785e-06, 'learning_rate': 1.556825396825397e-05, 'epoch': 2.07}\n",
      "{'loss': 0.0, 'grad_norm': 2.991339351865463e-05, 'learning_rate': 1.5488888888888888e-05, 'epoch': 2.07}\n",
      "{'loss': 0.0, 'grad_norm': 2.7020665584132075e-05, 'learning_rate': 1.540952380952381e-05, 'epoch': 2.08}\n",
      "{'loss': 0.0, 'grad_norm': 4.455581347428961e-06, 'learning_rate': 1.533015873015873e-05, 'epoch': 2.08}\n",
      "{'loss': 0.0, 'grad_norm': 2.744842049651197e-06, 'learning_rate': 1.5250793650793652e-05, 'epoch': 2.09}\n",
      "{'loss': 0.0, 'grad_norm': 3.406105179237784e-06, 'learning_rate': 1.5171428571428572e-05, 'epoch': 2.09}\n",
      "{'loss': 0.0, 'grad_norm': 3.351593841216527e-05, 'learning_rate': 1.5092063492063494e-05, 'epoch': 2.1}\n",
      "{'loss': 0.0, 'grad_norm': 4.045864898216678e-06, 'learning_rate': 1.5012698412698415e-05, 'epoch': 2.1}\n",
      "{'loss': 0.0, 'grad_norm': 1.870870983111672e-05, 'learning_rate': 1.4933333333333335e-05, 'epoch': 2.1}\n",
      "{'loss': 0.0, 'grad_norm': 3.95466668123845e-06, 'learning_rate': 1.4853968253968253e-05, 'epoch': 2.11}\n",
      "{'loss': 0.0, 'grad_norm': 3.801942875725217e-05, 'learning_rate': 1.4774603174603175e-05, 'epoch': 2.11}\n",
      "{'loss': 0.0, 'grad_norm': 6.9215193434502e-06, 'learning_rate': 1.4695238095238096e-05, 'epoch': 2.12}\n",
      "{'loss': 0.0, 'grad_norm': 5.735272225138033e-06, 'learning_rate': 1.4615873015873016e-05, 'epoch': 2.12}\n",
      "{'loss': 0.0, 'grad_norm': 3.7331781186367152e-06, 'learning_rate': 1.4536507936507937e-05, 'epoch': 2.13}\n",
      "{'loss': 0.0, 'grad_norm': 1.0435448530188296e-05, 'learning_rate': 1.4457142857142857e-05, 'epoch': 2.13}\n",
      "{'loss': 0.0, 'grad_norm': 1.3517194929590914e-05, 'learning_rate': 1.4377777777777779e-05, 'epoch': 2.14}\n",
      "{'loss': 0.0, 'grad_norm': 1.1722057024599053e-05, 'learning_rate': 1.42984126984127e-05, 'epoch': 2.14}\n",
      "{'loss': 0.0, 'grad_norm': 2.340652372367913e-06, 'learning_rate': 1.421904761904762e-05, 'epoch': 2.15}\n",
      "{'loss': 0.0, 'grad_norm': 4.761695436172886e-06, 'learning_rate': 1.4139682539682541e-05, 'epoch': 2.15}\n",
      "{'loss': 0.0, 'grad_norm': 1.6654505088808946e-05, 'learning_rate': 1.406031746031746e-05, 'epoch': 2.16}\n",
      "{'loss': 0.0, 'grad_norm': 7.094728789525107e-05, 'learning_rate': 1.3980952380952381e-05, 'epoch': 2.16}\n",
      "{'loss': 0.0, 'grad_norm': 7.210365765786264e-06, 'learning_rate': 1.3901587301587301e-05, 'epoch': 2.17}\n",
      "{'loss': 0.0, 'grad_norm': 3.146988774460624e-06, 'learning_rate': 1.3822222222222222e-05, 'epoch': 2.17}\n",
      "{'loss': 0.0, 'grad_norm': 1.1354767593729775e-05, 'learning_rate': 1.3742857142857144e-05, 'epoch': 2.18}\n",
      "{'loss': 0.0, 'grad_norm': 5.252566916169599e-05, 'learning_rate': 1.3663492063492064e-05, 'epoch': 2.18}\n",
      "{'loss': 0.0, 'grad_norm': 7.3900187089748215e-06, 'learning_rate': 1.3584126984126985e-05, 'epoch': 2.19}\n",
      "{'loss': 0.0, 'grad_norm': 2.4563303213653853e-06, 'learning_rate': 1.3504761904761907e-05, 'epoch': 2.19}\n",
      "{'loss': 0.0, 'grad_norm': 1.8823107893695123e-05, 'learning_rate': 1.3425396825396827e-05, 'epoch': 2.2}\n",
      "{'loss': 0.0, 'grad_norm': 1.6073985534603707e-05, 'learning_rate': 1.3346031746031748e-05, 'epoch': 2.2}\n",
      "{'loss': 0.0, 'grad_norm': 5.251294169283938e-06, 'learning_rate': 1.3266666666666666e-05, 'epoch': 2.2}\n",
      "{'loss': 0.0, 'grad_norm': 3.99338714487385e-05, 'learning_rate': 1.3187301587301588e-05, 'epoch': 2.21}\n",
      "{'loss': 0.0, 'grad_norm': 2.833481448760722e-05, 'learning_rate': 1.3107936507936507e-05, 'epoch': 2.21}\n",
      "{'loss': 0.0, 'grad_norm': 4.516581611824222e-06, 'learning_rate': 1.3028571428571429e-05, 'epoch': 2.22}\n",
      "{'loss': 0.0, 'grad_norm': 6.45120508124819e-06, 'learning_rate': 1.294920634920635e-05, 'epoch': 2.22}\n",
      "{'loss': 0.0, 'grad_norm': 3.0906930987839587e-06, 'learning_rate': 1.286984126984127e-05, 'epoch': 2.23}\n",
      "{'loss': 0.0, 'grad_norm': 8.973190233518835e-06, 'learning_rate': 1.2790476190476192e-05, 'epoch': 2.23}\n",
      "{'loss': 0.0, 'grad_norm': 3.375776259417762e-06, 'learning_rate': 1.2711111111111113e-05, 'epoch': 2.24}\n",
      "{'loss': 0.0, 'grad_norm': 1.7789130652090535e-05, 'learning_rate': 1.2631746031746033e-05, 'epoch': 2.24}\n",
      "{'loss': 0.0, 'grad_norm': 3.2286006899084896e-06, 'learning_rate': 1.2552380952380955e-05, 'epoch': 2.25}\n",
      "{'loss': 0.0, 'grad_norm': 7.588887910969788e-06, 'learning_rate': 1.2473015873015874e-05, 'epoch': 2.25}\n",
      "{'loss': 0.0, 'grad_norm': 8.944809451350011e-06, 'learning_rate': 1.2393650793650794e-05, 'epoch': 2.26}\n",
      "{'loss': 0.0, 'grad_norm': 1.4379404092323966e-05, 'learning_rate': 1.2314285714285714e-05, 'epoch': 2.26}\n",
      "{'loss': 0.0, 'grad_norm': 3.8674683310091496e-05, 'learning_rate': 1.2234920634920636e-05, 'epoch': 2.27}\n",
      "{'loss': 0.0, 'grad_norm': 3.740455213119276e-06, 'learning_rate': 1.2155555555555555e-05, 'epoch': 2.27}\n",
      "{'loss': 0.0, 'grad_norm': 2.979898636112921e-05, 'learning_rate': 1.2076190476190477e-05, 'epoch': 2.28}\n",
      "{'loss': 0.0, 'grad_norm': 8.789415005594492e-06, 'learning_rate': 1.1996825396825398e-05, 'epoch': 2.28}\n",
      "{'loss': 0.0, 'grad_norm': 7.317633844650118e-06, 'learning_rate': 1.1917460317460318e-05, 'epoch': 2.29}\n",
      "{'loss': 0.0, 'grad_norm': 1.0486596693226602e-05, 'learning_rate': 1.1838095238095238e-05, 'epoch': 2.29}\n",
      "{'loss': 0.0, 'grad_norm': 1.0415083124826197e-05, 'learning_rate': 1.175873015873016e-05, 'epoch': 2.3}\n",
      "{'loss': 0.0, 'grad_norm': 8.710130714462139e-06, 'learning_rate': 1.1679365079365081e-05, 'epoch': 2.3}\n",
      "{'loss': 0.0, 'grad_norm': 2.8018596367473947e-06, 'learning_rate': 1.16e-05, 'epoch': 2.3}\n",
      "{'loss': 0.0, 'grad_norm': 8.776378308539279e-06, 'learning_rate': 1.152063492063492e-05, 'epoch': 2.31}\n",
      "{'loss': 0.0, 'grad_norm': 2.976672931254143e-06, 'learning_rate': 1.1441269841269842e-05, 'epoch': 2.31}\n",
      "{'loss': 0.0, 'grad_norm': 3.3383599657099694e-05, 'learning_rate': 1.1361904761904762e-05, 'epoch': 2.32}\n",
      "{'loss': 0.0, 'grad_norm': 1.1830193216155749e-05, 'learning_rate': 1.1282539682539683e-05, 'epoch': 2.32}\n",
      "{'loss': 0.0, 'grad_norm': 5.390605565480655e-06, 'learning_rate': 1.1203174603174605e-05, 'epoch': 2.33}\n",
      "{'loss': 0.0, 'grad_norm': 7.647151505807415e-06, 'learning_rate': 1.1123809523809525e-05, 'epoch': 2.33}\n",
      "{'loss': 0.0, 'grad_norm': 2.5770590582396835e-05, 'learning_rate': 1.1044444444444444e-05, 'epoch': 2.34}\n",
      "{'loss': 0.0, 'grad_norm': 1.7812180885812268e-06, 'learning_rate': 1.0965079365079366e-05, 'epoch': 2.34}\n",
      "{'loss': 0.0, 'grad_norm': 4.698425982496701e-06, 'learning_rate': 1.0885714285714286e-05, 'epoch': 2.35}\n",
      "{'loss': 0.0, 'grad_norm': 2.7656999463943066e-06, 'learning_rate': 1.0806349206349207e-05, 'epoch': 2.35}\n",
      "{'loss': 0.0, 'grad_norm': 2.673389644769486e-05, 'learning_rate': 1.0726984126984127e-05, 'epoch': 2.36}\n",
      "{'loss': 0.0, 'grad_norm': 2.8511137770692585e-06, 'learning_rate': 1.0647619047619049e-05, 'epoch': 2.36}\n",
      "{'loss': 0.0, 'grad_norm': 3.937008841603529e-06, 'learning_rate': 1.056984126984127e-05, 'epoch': 2.37}\n",
      "{'loss': 0.0, 'grad_norm': 4.441084911377402e-06, 'learning_rate': 1.049047619047619e-05, 'epoch': 2.37}\n",
      "{'loss': 0.0, 'grad_norm': 2.4142057100107195e-06, 'learning_rate': 1.0411111111111112e-05, 'epoch': 2.38}\n",
      "{'loss': 0.0, 'grad_norm': 2.144450718333246e-06, 'learning_rate': 1.0331746031746033e-05, 'epoch': 2.38}\n",
      "{'loss': 0.0, 'grad_norm': 1.7322165604127804e-06, 'learning_rate': 1.0252380952380953e-05, 'epoch': 2.39}\n",
      "{'loss': 0.0, 'grad_norm': 0.00020784363732673228, 'learning_rate': 1.0173015873015873e-05, 'epoch': 2.39}\n",
      "{'loss': 0.0, 'grad_norm': 4.7944522520992905e-06, 'learning_rate': 1.0093650793650794e-05, 'epoch': 2.4}\n",
      "{'loss': 0.0, 'grad_norm': 1.97603803826496e-05, 'learning_rate': 1.0014285714285716e-05, 'epoch': 2.4}\n",
      "{'loss': 0.0, 'grad_norm': 4.933395757689141e-06, 'learning_rate': 9.934920634920636e-06, 'epoch': 2.4}\n",
      "{'loss': 0.0, 'grad_norm': 1.3716957255383022e-05, 'learning_rate': 9.855555555555555e-06, 'epoch': 2.41}\n",
      "{'loss': 0.0, 'grad_norm': 2.157362587240641e-06, 'learning_rate': 9.776190476190477e-06, 'epoch': 2.41}\n",
      "{'loss': 0.0, 'grad_norm': 2.5226498109987006e-05, 'learning_rate': 9.696825396825397e-06, 'epoch': 2.42}\n",
      "{'loss': 0.0, 'grad_norm': 3.49432993971277e-05, 'learning_rate': 9.617460317460318e-06, 'epoch': 2.42}\n",
      "{'loss': 0.0, 'grad_norm': 5.385846634453628e-06, 'learning_rate': 9.53809523809524e-06, 'epoch': 2.43}\n",
      "{'loss': 0.0, 'grad_norm': 1.3979480399939348e-06, 'learning_rate': 9.458730158730158e-06, 'epoch': 2.43}\n",
      "{'loss': 0.0, 'grad_norm': 1.1675331279548118e-06, 'learning_rate': 9.37936507936508e-06, 'epoch': 2.44}\n",
      "{'loss': 0.0, 'grad_norm': 1.8833148942576372e-06, 'learning_rate': 9.3e-06, 'epoch': 2.44}\n",
      "{'loss': 0.0, 'grad_norm': 2.850429154932499e-06, 'learning_rate': 9.22063492063492e-06, 'epoch': 2.45}\n",
      "{'loss': 0.0, 'grad_norm': 7.52365167500102e-06, 'learning_rate': 9.141269841269842e-06, 'epoch': 2.45}\n",
      "{'loss': 0.0, 'grad_norm': 0.00013759711873717606, 'learning_rate': 9.061904761904762e-06, 'epoch': 2.46}\n",
      "{'loss': 0.0, 'grad_norm': 6.060889518266777e-06, 'learning_rate': 8.982539682539683e-06, 'epoch': 2.46}\n",
      "{'loss': 0.0, 'grad_norm': 1.0535867659200449e-05, 'learning_rate': 8.903174603174603e-06, 'epoch': 2.47}\n",
      "{'loss': 0.0, 'grad_norm': 6.9802504185645375e-06, 'learning_rate': 8.823809523809525e-06, 'epoch': 2.47}\n",
      "{'loss': 0.0, 'grad_norm': 6.933325948921265e-06, 'learning_rate': 8.744444444444446e-06, 'epoch': 2.48}\n",
      "{'loss': 0.0, 'grad_norm': 4.540785084827803e-06, 'learning_rate': 8.665079365079364e-06, 'epoch': 2.48}\n",
      "{'loss': 0.0, 'grad_norm': 1.934072861331515e-05, 'learning_rate': 8.585714285714286e-06, 'epoch': 2.49}\n",
      "{'loss': 0.0, 'grad_norm': 1.5844285599087016e-06, 'learning_rate': 8.506349206349207e-06, 'epoch': 2.49}\n",
      "{'loss': 0.0, 'grad_norm': 4.955435997544555e-06, 'learning_rate': 8.426984126984127e-06, 'epoch': 2.5}\n",
      "{'loss': 0.0, 'grad_norm': 2.69479619419144e-06, 'learning_rate': 8.347619047619049e-06, 'epoch': 2.5}\n",
      "{'loss': 0.0, 'grad_norm': 3.648836809588829e-06, 'learning_rate': 8.268253968253968e-06, 'epoch': 2.5}\n",
      "{'loss': 0.0, 'grad_norm': 1.5701616575825028e-05, 'learning_rate': 8.188888888888888e-06, 'epoch': 2.51}\n",
      "{'loss': 0.0, 'grad_norm': 3.1152687824942404e-06, 'learning_rate': 8.10952380952381e-06, 'epoch': 2.51}\n",
      "{'loss': 0.0, 'grad_norm': 5.5673986025794875e-06, 'learning_rate': 8.030158730158731e-06, 'epoch': 2.52}\n",
      "{'loss': 0.0, 'grad_norm': 1.2322131624387112e-05, 'learning_rate': 7.950793650793651e-06, 'epoch': 2.52}\n",
      "{'loss': 0.0, 'grad_norm': 1.3296796169015579e-05, 'learning_rate': 7.87142857142857e-06, 'epoch': 2.53}\n",
      "{'loss': 0.0, 'grad_norm': 7.785753405187279e-05, 'learning_rate': 7.792063492063492e-06, 'epoch': 2.53}\n",
      "{'loss': 0.0, 'grad_norm': 2.94808251055656e-06, 'learning_rate': 7.712698412698414e-06, 'epoch': 2.54}\n",
      "{'loss': 0.0, 'grad_norm': 7.296564035641495e-06, 'learning_rate': 7.633333333333334e-06, 'epoch': 2.54}\n",
      "{'loss': 0.0, 'grad_norm': 2.5165570605167886e-06, 'learning_rate': 7.553968253968255e-06, 'epoch': 2.55}\n",
      "{'loss': 0.0, 'grad_norm': 3.0800104013906093e-06, 'learning_rate': 7.474603174603174e-06, 'epoch': 2.55}\n",
      "{'loss': 0.0, 'grad_norm': 1.0771932466013823e-05, 'learning_rate': 7.395238095238096e-06, 'epoch': 2.56}\n",
      "{'loss': 0.0, 'grad_norm': 3.861714048980502e-06, 'learning_rate': 7.315873015873016e-06, 'epoch': 2.56}\n",
      "{'loss': 0.0, 'grad_norm': 3.233768802601844e-05, 'learning_rate': 7.236507936507937e-06, 'epoch': 2.57}\n",
      "{'loss': 0.0, 'grad_norm': 7.144369192246813e-06, 'learning_rate': 7.1571428571428584e-06, 'epoch': 2.57}\n",
      "{'loss': 0.0, 'grad_norm': 1.5812319134056452e-06, 'learning_rate': 7.077777777777777e-06, 'epoch': 2.58}\n",
      "{'loss': 0.0, 'grad_norm': 4.495366283663316e-06, 'learning_rate': 6.998412698412699e-06, 'epoch': 2.58}\n",
      "{'loss': 0.0, 'grad_norm': 1.863706529547926e-06, 'learning_rate': 6.9190476190476196e-06, 'epoch': 2.59}\n",
      "{'loss': 0.0, 'grad_norm': 1.201829672936583e-05, 'learning_rate': 6.83968253968254e-06, 'epoch': 2.59}\n",
      "{'loss': 0.0, 'grad_norm': 1.7674435639492003e-06, 'learning_rate': 6.760317460317461e-06, 'epoch': 2.6}\n",
      "{'loss': 0.0, 'grad_norm': 8.71396878210362e-06, 'learning_rate': 6.680952380952381e-06, 'epoch': 2.6}\n",
      "{'loss': 0.0, 'grad_norm': 3.5474506603350164e-06, 'learning_rate': 6.601587301587301e-06, 'epoch': 2.6}\n",
      "{'loss': 0.0, 'grad_norm': 3.888987976097269e-06, 'learning_rate': 6.522222222222223e-06, 'epoch': 2.61}\n",
      "{'loss': 0.0, 'grad_norm': 1.1546026144060306e-05, 'learning_rate': 6.4428571428571435e-06, 'epoch': 2.61}\n",
      "{'loss': 0.0, 'grad_norm': 4.094310952496016e-06, 'learning_rate': 6.363492063492064e-06, 'epoch': 2.62}\n",
      "{'loss': 0.0, 'grad_norm': 3.427992851356976e-05, 'learning_rate': 6.284126984126984e-06, 'epoch': 2.62}\n",
      "{'loss': 0.0, 'grad_norm': 2.208734986197669e-05, 'learning_rate': 6.204761904761905e-06, 'epoch': 2.63}\n",
      "{'loss': 0.0, 'grad_norm': 3.4742115531116724e-06, 'learning_rate': 6.125396825396826e-06, 'epoch': 2.63}\n",
      "{'loss': 0.0, 'grad_norm': 8.87479109223932e-06, 'learning_rate': 6.046031746031746e-06, 'epoch': 2.64}\n",
      "{'loss': 0.0, 'grad_norm': 1.4072415979171637e-06, 'learning_rate': 5.9666666666666666e-06, 'epoch': 2.64}\n",
      "{'loss': 0.0, 'grad_norm': 1.686373025222565e-06, 'learning_rate': 5.887301587301588e-06, 'epoch': 2.65}\n",
      "{'loss': 0.0, 'grad_norm': 3.241938657083665e-06, 'learning_rate': 5.807936507936508e-06, 'epoch': 2.65}\n",
      "{'loss': 0.0, 'grad_norm': 5.102286650071619e-06, 'learning_rate': 5.728571428571429e-06, 'epoch': 2.66}\n",
      "{'loss': 0.0, 'grad_norm': 1.638777007428871e-06, 'learning_rate': 5.649206349206349e-06, 'epoch': 2.66}\n",
      "{'loss': 0.0, 'grad_norm': 3.087590130235185e-06, 'learning_rate': 5.56984126984127e-06, 'epoch': 2.67}\n",
      "{'loss': 0.0, 'grad_norm': 4.341352905612439e-06, 'learning_rate': 5.490476190476191e-06, 'epoch': 2.67}\n",
      "{'loss': 0.0, 'grad_norm': 2.7972996576863807e-06, 'learning_rate': 5.411111111111111e-06, 'epoch': 2.68}\n",
      "{'loss': 0.0, 'grad_norm': 2.7896627216250636e-05, 'learning_rate': 5.331746031746032e-06, 'epoch': 2.68}\n",
      "{'loss': 0.0, 'grad_norm': 3.9643306081416085e-05, 'learning_rate': 5.2523809523809525e-06, 'epoch': 2.69}\n",
      "{'loss': 0.0, 'grad_norm': 5.747516297560651e-06, 'learning_rate': 5.173015873015873e-06, 'epoch': 2.69}\n",
      "{'loss': 0.0, 'grad_norm': 2.5042743800440803e-05, 'learning_rate': 5.093650793650794e-06, 'epoch': 2.7}\n",
      "{'loss': 0.0, 'grad_norm': 1.6120792452056776e-06, 'learning_rate': 5.0142857142857144e-06, 'epoch': 2.7}\n",
      "{'loss': 0.0, 'grad_norm': 1.571922257426195e-05, 'learning_rate': 4.934920634920635e-06, 'epoch': 2.7}\n",
      "{'loss': 0.0, 'grad_norm': 5.0602889132278506e-06, 'learning_rate': 4.855555555555556e-06, 'epoch': 2.71}\n",
      "{'loss': 0.0, 'grad_norm': 4.779992195835803e-06, 'learning_rate': 4.776190476190476e-06, 'epoch': 2.71}\n",
      "{'loss': 0.0, 'grad_norm': 2.80061840385315e-06, 'learning_rate': 4.696825396825397e-06, 'epoch': 2.72}\n",
      "{'loss': 0.0, 'grad_norm': 3.817973629338667e-05, 'learning_rate': 4.617460317460318e-06, 'epoch': 2.72}\n",
      "{'loss': 0.0, 'grad_norm': 1.92802986020979e-06, 'learning_rate': 4.538095238095238e-06, 'epoch': 2.73}\n",
      "{'loss': 0.0, 'grad_norm': 5.188057548366487e-05, 'learning_rate': 4.458730158730159e-06, 'epoch': 2.73}\n",
      "{'loss': 0.0, 'grad_norm': 0.00012008027988485992, 'learning_rate': 4.37936507936508e-06, 'epoch': 2.74}\n",
      "{'loss': 0.0, 'grad_norm': 5.455304290080676e-06, 'learning_rate': 4.2999999999999995e-06, 'epoch': 2.74}\n",
      "{'loss': 0.0, 'grad_norm': 1.3737984545514337e-06, 'learning_rate': 4.220634920634921e-06, 'epoch': 2.75}\n",
      "{'loss': 0.0, 'grad_norm': 2.687131654965924e-06, 'learning_rate': 4.141269841269842e-06, 'epoch': 2.75}\n",
      "{'loss': 0.0, 'grad_norm': 1.765417323440488e-06, 'learning_rate': 4.061904761904762e-06, 'epoch': 2.76}\n",
      "{'loss': 0.0, 'grad_norm': 2.627128742460627e-05, 'learning_rate': 3.982539682539683e-06, 'epoch': 2.76}\n",
      "{'loss': 0.0, 'grad_norm': 4.082710347574903e-06, 'learning_rate': 3.903174603174603e-06, 'epoch': 2.77}\n",
      "{'loss': 0.0, 'grad_norm': 6.90289789417875e-06, 'learning_rate': 3.823809523809524e-06, 'epoch': 2.77}\n",
      "{'loss': 0.0, 'grad_norm': 7.402080882457085e-06, 'learning_rate': 3.744444444444445e-06, 'epoch': 2.78}\n",
      "{'loss': 0.0, 'grad_norm': 1.461151896364754e-06, 'learning_rate': 3.665079365079365e-06, 'epoch': 2.78}\n",
      "{'loss': 0.0, 'grad_norm': 3.120611836493481e-06, 'learning_rate': 3.585714285714286e-06, 'epoch': 2.79}\n",
      "{'loss': 0.0, 'grad_norm': 6.321790806396166e-06, 'learning_rate': 3.506349206349206e-06, 'epoch': 2.79}\n",
      "{'loss': 0.0, 'grad_norm': 2.31600688493927e-06, 'learning_rate': 3.426984126984127e-06, 'epoch': 2.8}\n",
      "{'loss': 0.0, 'grad_norm': 4.00878207074129e-06, 'learning_rate': 3.347619047619048e-06, 'epoch': 2.8}\n",
      "{'loss': 0.0, 'grad_norm': 2.0086174117750488e-05, 'learning_rate': 3.26984126984127e-06, 'epoch': 2.8}\n",
      "{'loss': 0.0, 'grad_norm': 1.1669801551761338e-06, 'learning_rate': 3.190476190476191e-06, 'epoch': 2.81}\n",
      "{'loss': 0.0, 'grad_norm': 6.878037766000489e-06, 'learning_rate': 3.111111111111111e-06, 'epoch': 2.81}\n",
      "{'loss': 0.0, 'grad_norm': 2.6364534733147593e-06, 'learning_rate': 3.031746031746032e-06, 'epoch': 2.82}\n",
      "{'loss': 0.0, 'grad_norm': 5.729500571760582e-06, 'learning_rate': 2.9523809523809525e-06, 'epoch': 2.82}\n",
      "{'loss': 0.0, 'grad_norm': 1.4577533875126392e-05, 'learning_rate': 2.873015873015873e-06, 'epoch': 2.83}\n",
      "{'loss': 0.0, 'grad_norm': 2.804445330184535e-06, 'learning_rate': 2.7936507936507938e-06, 'epoch': 2.83}\n",
      "{'loss': 0.0, 'grad_norm': 4.540477220871253e-06, 'learning_rate': 2.7142857142857144e-06, 'epoch': 2.84}\n",
      "{'loss': 0.0, 'grad_norm': 2.4674100131960586e-06, 'learning_rate': 2.634920634920635e-06, 'epoch': 2.84}\n",
      "{'loss': 0.0, 'grad_norm': 9.087392754736356e-06, 'learning_rate': 2.5555555555555557e-06, 'epoch': 2.85}\n",
      "{'loss': 0.0, 'grad_norm': 1.9301337488286663e-06, 'learning_rate': 2.4761904761904764e-06, 'epoch': 2.85}\n",
      "{'loss': 0.0, 'grad_norm': 2.8981271498196293e-06, 'learning_rate': 2.396825396825397e-06, 'epoch': 2.86}\n",
      "{'loss': 0.0, 'grad_norm': 2.838957698259037e-06, 'learning_rate': 2.3174603174603177e-06, 'epoch': 2.86}\n",
      "{'loss': 0.0, 'grad_norm': 2.1134924281795975e-06, 'learning_rate': 2.238095238095238e-06, 'epoch': 2.87}\n",
      "{'loss': 0.0, 'grad_norm': 1.5031481780169997e-05, 'learning_rate': 2.158730158730159e-06, 'epoch': 2.87}\n",
      "{'loss': 0.0, 'grad_norm': 5.417797183326911e-06, 'learning_rate': 2.0793650793650797e-06, 'epoch': 2.88}\n",
      "{'loss': 0.0, 'grad_norm': 2.11673318517569e-06, 'learning_rate': 2.0000000000000003e-06, 'epoch': 2.88}\n",
      "{'loss': 0.0, 'grad_norm': 5.0996413847315125e-06, 'learning_rate': 1.9206349206349206e-06, 'epoch': 2.89}\n",
      "{'loss': 0.0, 'grad_norm': 2.379881379965809e-06, 'learning_rate': 1.8412698412698412e-06, 'epoch': 2.89}\n",
      "{'loss': 0.0, 'grad_norm': 4.040132807858754e-06, 'learning_rate': 1.7619047619047619e-06, 'epoch': 2.9}\n",
      "{'loss': 0.0, 'grad_norm': 3.50927330146078e-05, 'learning_rate': 1.6825396825396827e-06, 'epoch': 2.9}\n",
      "{'loss': 0.0, 'grad_norm': 3.1035679057822563e-06, 'learning_rate': 1.6031746031746034e-06, 'epoch': 2.9}\n",
      "{'loss': 0.0, 'grad_norm': 8.889996934158262e-07, 'learning_rate': 1.5238095238095238e-06, 'epoch': 2.91}\n",
      "{'loss': 0.0, 'grad_norm': 2.112184120051097e-06, 'learning_rate': 1.4444444444444445e-06, 'epoch': 2.91}\n",
      "{'loss': 0.0, 'grad_norm': 2.10741222872457e-06, 'learning_rate': 1.3650793650793652e-06, 'epoch': 2.92}\n",
      "{'loss': 0.0, 'grad_norm': 1.362556417916494e-06, 'learning_rate': 1.2857142857142858e-06, 'epoch': 2.92}\n",
      "{'loss': 0.0, 'grad_norm': 4.1538578443578444e-06, 'learning_rate': 1.2063492063492065e-06, 'epoch': 2.93}\n",
      "{'loss': 0.0, 'grad_norm': 2.9402674499579007e-06, 'learning_rate': 1.1269841269841271e-06, 'epoch': 2.93}\n",
      "{'loss': 0.0, 'grad_norm': 5.03978444612585e-05, 'learning_rate': 1.0476190476190476e-06, 'epoch': 2.94}\n",
      "{'loss': 0.0, 'grad_norm': 1.401647864440747e-06, 'learning_rate': 9.682539682539684e-07, 'epoch': 2.94}\n",
      "{'loss': 0.0, 'grad_norm': 5.690063062502304e-06, 'learning_rate': 8.88888888888889e-07, 'epoch': 2.95}\n",
      "{'loss': 0.0, 'grad_norm': 0.00014078128151595592, 'learning_rate': 8.095238095238095e-07, 'epoch': 2.95}\n",
      "{'loss': 0.0, 'grad_norm': 3.128530806861818e-06, 'learning_rate': 7.301587301587303e-07, 'epoch': 2.96}\n",
      "{'loss': 0.0, 'grad_norm': 1.1600067182371276e-06, 'learning_rate': 6.507936507936508e-07, 'epoch': 2.96}\n",
      "{'loss': 0.0, 'grad_norm': 7.624235422554193e-06, 'learning_rate': 5.714285714285715e-07, 'epoch': 2.97}\n",
      "{'loss': 0.0, 'grad_norm': 7.426925094478065e-06, 'learning_rate': 4.920634920634921e-07, 'epoch': 2.97}\n",
      "{'loss': 0.0, 'grad_norm': 7.074477252899669e-06, 'learning_rate': 4.1269841269841275e-07, 'epoch': 2.98}\n",
      "{'loss': 0.0, 'grad_norm': 2.2682841063215164e-06, 'learning_rate': 3.3333333333333335e-07, 'epoch': 2.98}\n",
      "{'loss': 0.0, 'grad_norm': 1.4606204103984055e-06, 'learning_rate': 2.5396825396825396e-07, 'epoch': 2.99}\n",
      "{'loss': 0.0, 'grad_norm': 7.263920997502282e-05, 'learning_rate': 1.746031746031746e-07, 'epoch': 2.99}\n",
      "{'loss': 0.0, 'grad_norm': 1.0488377483852673e-05, 'learning_rate': 9.523809523809524e-08, 'epoch': 3.0}\n",
      "{'loss': 0.0, 'grad_norm': 1.8672636770133977e-06, 'learning_rate': 1.5873015873015872e-08, 'epoch': 3.0}\n",
      "{'train_runtime': 1569.3307, 'train_samples_per_second': 80.289, 'train_steps_per_second': 20.072, 'train_loss': 0.006899901330589302, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=31500, training_loss=0.006899901330589302, metrics={'train_runtime': 1569.3307, 'train_samples_per_second': 80.289, 'train_steps_per_second': 20.072, 'total_flos': 2131633373184000.0, 'train_loss': 0.006899901330589302, 'epoch': 3.0})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\").to(\"cuda\")\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./t5-emotion-gen\",\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=50,\n",
    "    save_total_limit=1,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    save_strategy=\"no\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## G√©n√©ration & √âvaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt     : emotion: admiration message: Je suis impressionn√© par la qualit√© de votre service.\n",
      "R√©f√©rence  : Merci pour votre message concernant admiration. Nous allons vous r√©pondre au mieux.\n",
      "G√©n√©r√©     : Merci pour votre message concernant admiration. Nous allons vous r√©pondre au mieux.\n",
      "BLEU       : 1.0\n",
      "ROUGE-1    : 1.0\n",
      "ROUGE-2    : 1.0\n",
      "ROUGE-L    : 1.0\n",
      "Perplexity : 1.0\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "import math\n",
    "\n",
    "# G√©n√©ration de test\n",
    "sample = df.iloc[0]\n",
    "prompt = \"emotion: \" + sample[\"emotion\"] + \" message: \" + sample[\"text_input\"]\n",
    "reference = sample[\"target_text\"]\n",
    "\n",
    "# Encode + generate\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "output_ids = model.generate(**inputs, max_new_tokens=50)\n",
    "generated = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Evaluation BLEU, ROUGE, Perplexity\n",
    "smoothie = SmoothingFunction().method4\n",
    "scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
    "\n",
    "bleu = sentence_bleu([reference.split()], generated.split(), smoothing_function=smoothie)\n",
    "rouge = scorer.score(reference, generated)\n",
    "\n",
    "# Perplexity\n",
    "enc = tokenizer(generated, return_tensors=\"pt\").to(\"cuda\")\n",
    "with torch.no_grad():\n",
    "    out = model(**enc, labels=enc[\"input_ids\"])\n",
    "perplexity = math.exp(out.loss.item()) if out.loss.item() < 100 else float(\"inf\")\n",
    "\n",
    "# Affichage\n",
    "print(f\"Prompt     : {prompt}\")\n",
    "print(f\"R√©f√©rence  : {reference}\")\n",
    "print(f\"G√©n√©r√©     : {generated}\")\n",
    "print(f\"BLEU       : {round(bleu, 4)}\")\n",
    "print(f\"ROUGE-1    : {round(rouge['rouge1'].fmeasure, 4)}\")\n",
    "print(f\"ROUGE-2    : {round(rouge['rouge2'].fmeasure, 4)}\")\n",
    "print(f\"ROUGE-L    : {round(rouge['rougeL'].fmeasure, 4)}\")\n",
    "print(f\"Perplexity : {round(perplexity, 4)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Model Bart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pr√©paration des donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration, Trainer, TrainingArguments, DataCollatorForSeq2Seq\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# 1. Chargement du CSV\n",
    "df = pd.read_csv(\"../data/emotion_datasets/emo_reviews.csv\")\n",
    "\n",
    "# 2. Cr√©ation du prompt BART : \"<emotion> : ... | message: ...\"\n",
    "df[\"input_text\"] = \"emotion: \" + df[\"emotion\"] + \" | message: \" + df[\"text_input\"]\n",
    "df[\"target_text\"] = df[\"text_output\"]\n",
    "\n",
    "# 3. HuggingFace Dataset\n",
    "dataset = Dataset.from_pandas(df[[\"input_text\", \"target_text\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenisation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Yann\\anaconda3\\envs\\hf_gpu_env\\lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b83d4181698a4ae8844b13b1623a2a97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Yann\\anaconda3\\envs\\hf_gpu_env\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Yann\\.cache\\huggingface\\hub\\models--facebook--bart-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2fa190c70ed4fbea430cbae3486a2cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "887ccb2511a644e0addd9fe8b5f8ac98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4591454067c044d0a6468b3d60a58d3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e965d80b4ac4cfe8a646d06ab8ebd5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/42000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "\n",
    "def tokenize(batch):\n",
    "    inputs = tokenizer(batch[\"input_text\"], padding=\"max_length\", truncation=True, max_length=64)\n",
    "    targets = tokenizer(batch[\"target_text\"], padding=\"max_length\", truncation=True, max_length=64)\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize, batched=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Entra√Ænement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3627cb7bf6f645ee978247001e70aa87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/558M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Yann\\anaconda3\\envs\\hf_gpu_env\\lib\\site-packages\\accelerate\\accelerator.py:469: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef753acc4df34ec0b56e2c97c6669d13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.788, 'grad_norm': 28.637229919433594, 'learning_rate': 4.992857142857143e-05, 'epoch': 0.0}\n",
      "{'loss': 0.3402, 'grad_norm': 0.42420294880867004, 'learning_rate': 4.984920634920635e-05, 'epoch': 0.01}\n",
      "{'loss': 0.0088, 'grad_norm': 0.5754862427711487, 'learning_rate': 4.976984126984127e-05, 'epoch': 0.01}\n",
      "{'loss': 0.0091, 'grad_norm': 0.03672761842608452, 'learning_rate': 4.969047619047619e-05, 'epoch': 0.02}\n",
      "{'loss': 0.0112, 'grad_norm': 0.21209941804409027, 'learning_rate': 4.961111111111111e-05, 'epoch': 0.02}\n",
      "{'loss': 0.0161, 'grad_norm': 0.07746761292219162, 'learning_rate': 4.9531746031746034e-05, 'epoch': 0.03}\n",
      "{'loss': 0.003, 'grad_norm': 0.053823091089725494, 'learning_rate': 4.9452380952380955e-05, 'epoch': 0.03}\n",
      "{'loss': 0.0006, 'grad_norm': 0.010721960105001926, 'learning_rate': 4.937301587301588e-05, 'epoch': 0.04}\n",
      "{'loss': 0.0088, 'grad_norm': 0.01150433998554945, 'learning_rate': 4.92936507936508e-05, 'epoch': 0.04}\n",
      "{'loss': 0.0021, 'grad_norm': 0.3227491080760956, 'learning_rate': 4.921428571428572e-05, 'epoch': 0.05}\n",
      "{'loss': 0.0007, 'grad_norm': 0.010110882110893726, 'learning_rate': 4.913492063492064e-05, 'epoch': 0.05}\n",
      "{'loss': 0.0002, 'grad_norm': 0.006218066904693842, 'learning_rate': 4.905555555555556e-05, 'epoch': 0.06}\n",
      "{'loss': 0.0031, 'grad_norm': 0.059714168310165405, 'learning_rate': 4.8976190476190484e-05, 'epoch': 0.06}\n",
      "{'loss': 0.0001, 'grad_norm': 0.00482964375987649, 'learning_rate': 4.88968253968254e-05, 'epoch': 0.07}\n",
      "{'loss': 0.0138, 'grad_norm': 0.11720097064971924, 'learning_rate': 4.881746031746032e-05, 'epoch': 0.07}\n",
      "{'loss': 0.0033, 'grad_norm': 0.047638293355703354, 'learning_rate': 4.8738095238095235e-05, 'epoch': 0.08}\n",
      "{'loss': 0.0063, 'grad_norm': 0.013766298070549965, 'learning_rate': 4.865873015873016e-05, 'epoch': 0.08}\n",
      "{'loss': 0.0038, 'grad_norm': 0.010873780585825443, 'learning_rate': 4.857936507936508e-05, 'epoch': 0.09}\n",
      "{'loss': 0.0042, 'grad_norm': 0.00455077737569809, 'learning_rate': 4.85e-05, 'epoch': 0.09}\n",
      "{'loss': 0.0011, 'grad_norm': 0.004441040568053722, 'learning_rate': 4.842063492063492e-05, 'epoch': 0.1}\n",
      "{'loss': 0.0018, 'grad_norm': 0.027302589267492294, 'learning_rate': 4.834126984126984e-05, 'epoch': 0.1}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0027696331962943077, 'learning_rate': 4.8261904761904764e-05, 'epoch': 0.1}\n",
      "{'loss': 0.0009, 'grad_norm': 0.002725697588175535, 'learning_rate': 4.8182539682539686e-05, 'epoch': 0.11}\n",
      "{'loss': 0.0029, 'grad_norm': 0.008606520481407642, 'learning_rate': 4.810317460317461e-05, 'epoch': 0.11}\n",
      "{'loss': 0.0008, 'grad_norm': 0.002817245200276375, 'learning_rate': 4.802380952380953e-05, 'epoch': 0.12}\n",
      "{'loss': 0.0001, 'grad_norm': 0.004185309633612633, 'learning_rate': 4.794444444444445e-05, 'epoch': 0.12}\n",
      "{'loss': 0.0016, 'grad_norm': 0.0020105328876525164, 'learning_rate': 4.7865079365079365e-05, 'epoch': 0.13}\n",
      "{'loss': 0.0, 'grad_norm': 0.0019772739615291357, 'learning_rate': 4.7785714285714287e-05, 'epoch': 0.13}\n",
      "{'loss': 0.0011, 'grad_norm': 0.0020324625074863434, 'learning_rate': 4.770634920634921e-05, 'epoch': 0.14}\n",
      "{'loss': 0.0095, 'grad_norm': 0.001986078452318907, 'learning_rate': 4.762698412698413e-05, 'epoch': 0.14}\n",
      "{'loss': 0.001, 'grad_norm': 0.008178369142115116, 'learning_rate': 4.754761904761905e-05, 'epoch': 0.15}\n",
      "{'loss': 0.0032, 'grad_norm': 0.00502773467451334, 'learning_rate': 4.746825396825397e-05, 'epoch': 0.15}\n",
      "{'loss': 0.0045, 'grad_norm': 0.008324086666107178, 'learning_rate': 4.7388888888888894e-05, 'epoch': 0.16}\n",
      "{'loss': 0.0004, 'grad_norm': 0.0021009985357522964, 'learning_rate': 4.730952380952381e-05, 'epoch': 0.16}\n",
      "{'loss': 0.0, 'grad_norm': 0.0017319676699116826, 'learning_rate': 4.723015873015873e-05, 'epoch': 0.17}\n",
      "{'loss': 0.0, 'grad_norm': 0.0016875927103683352, 'learning_rate': 4.715079365079365e-05, 'epoch': 0.17}\n",
      "{'loss': 0.0, 'grad_norm': 0.0016086220275610685, 'learning_rate': 4.707142857142857e-05, 'epoch': 0.18}\n",
      "{'loss': 0.0, 'grad_norm': 0.0015353516209870577, 'learning_rate': 4.6992063492063495e-05, 'epoch': 0.18}\n",
      "{'loss': 0.0004, 'grad_norm': 0.0016634495696052909, 'learning_rate': 4.6912698412698416e-05, 'epoch': 0.19}\n",
      "{'loss': 0.0008, 'grad_norm': 0.0014411136507987976, 'learning_rate': 4.683333333333334e-05, 'epoch': 0.19}\n",
      "{'loss': 0.0011, 'grad_norm': 0.005688547622412443, 'learning_rate': 4.675396825396825e-05, 'epoch': 0.2}\n",
      "{'loss': 0.0045, 'grad_norm': 0.00705018499866128, 'learning_rate': 4.6674603174603174e-05, 'epoch': 0.2}\n",
      "{'loss': 0.0009, 'grad_norm': 0.0014080086257308722, 'learning_rate': 4.6595238095238096e-05, 'epoch': 0.2}\n",
      "{'loss': 0.001, 'grad_norm': 0.0012716080527752638, 'learning_rate': 4.651587301587302e-05, 'epoch': 0.21}\n",
      "{'loss': 0.0002, 'grad_norm': 0.0014197796117514372, 'learning_rate': 4.643650793650794e-05, 'epoch': 0.21}\n",
      "{'loss': 0.0008, 'grad_norm': 0.0011731158010661602, 'learning_rate': 4.635714285714286e-05, 'epoch': 0.22}\n",
      "{'loss': 0.0, 'grad_norm': 0.0010037338361144066, 'learning_rate': 4.627777777777778e-05, 'epoch': 0.22}\n",
      "{'loss': 0.0005, 'grad_norm': 0.0010039733024314046, 'learning_rate': 4.61984126984127e-05, 'epoch': 0.23}\n",
      "{'loss': 0.001, 'grad_norm': 0.0015289376024156809, 'learning_rate': 4.6119047619047625e-05, 'epoch': 0.23}\n",
      "{'loss': 0.0021, 'grad_norm': 0.006585362832993269, 'learning_rate': 4.6039682539682546e-05, 'epoch': 0.24}\n",
      "{'loss': 0.0029, 'grad_norm': 0.0038107289001345634, 'learning_rate': 4.596031746031747e-05, 'epoch': 0.24}\n",
      "{'loss': 0.0, 'grad_norm': 0.0009487225324846804, 'learning_rate': 4.588095238095238e-05, 'epoch': 0.25}\n",
      "{'loss': 0.0, 'grad_norm': 0.0012882320443168283, 'learning_rate': 4.5801587301587304e-05, 'epoch': 0.25}\n",
      "{'loss': 0.0, 'grad_norm': 0.0007759883883409202, 'learning_rate': 4.572222222222222e-05, 'epoch': 0.26}\n",
      "{'loss': 0.0, 'grad_norm': 0.0009076901478692889, 'learning_rate': 4.564285714285714e-05, 'epoch': 0.26}\n",
      "{'loss': 0.0, 'grad_norm': 0.0006341161788441241, 'learning_rate': 4.556349206349206e-05, 'epoch': 0.27}\n",
      "{'loss': 0.0, 'grad_norm': 0.0006588537362404168, 'learning_rate': 4.548412698412698e-05, 'epoch': 0.27}\n",
      "{'loss': 0.0, 'grad_norm': 0.0006375042721629143, 'learning_rate': 4.5404761904761905e-05, 'epoch': 0.28}\n",
      "{'loss': 0.001, 'grad_norm': 0.0006238152855075896, 'learning_rate': 4.5325396825396826e-05, 'epoch': 0.28}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0006580975605174899, 'learning_rate': 4.524603174603175e-05, 'epoch': 0.29}\n",
      "{'loss': 0.0, 'grad_norm': 0.0006217873306013644, 'learning_rate': 4.516666666666667e-05, 'epoch': 0.29}\n",
      "{'loss': 0.0, 'grad_norm': 0.000636427488643676, 'learning_rate': 4.508730158730159e-05, 'epoch': 0.3}\n",
      "{'loss': 0.0, 'grad_norm': 0.0008693552808836102, 'learning_rate': 4.500793650793651e-05, 'epoch': 0.3}\n",
      "{'loss': 0.0, 'grad_norm': 0.000623103347606957, 'learning_rate': 4.4928571428571434e-05, 'epoch': 0.3}\n",
      "{'loss': 0.0, 'grad_norm': 0.0006081500323489308, 'learning_rate': 4.4849206349206355e-05, 'epoch': 0.31}\n",
      "{'loss': 0.001, 'grad_norm': 0.0008686490473337471, 'learning_rate': 4.476984126984127e-05, 'epoch': 0.31}\n",
      "{'loss': 0.0011, 'grad_norm': 0.0011298799654468894, 'learning_rate': 4.469047619047619e-05, 'epoch': 0.32}\n",
      "{'loss': 0.0118, 'grad_norm': 0.0013134365435689688, 'learning_rate': 4.461111111111111e-05, 'epoch': 0.32}\n",
      "{'loss': 0.0004, 'grad_norm': 0.0006706219865009189, 'learning_rate': 4.4531746031746034e-05, 'epoch': 0.33}\n",
      "{'loss': 0.0101, 'grad_norm': 0.001119111548177898, 'learning_rate': 4.4452380952380956e-05, 'epoch': 0.33}\n",
      "{'loss': 0.0139, 'grad_norm': 0.0007600472308695316, 'learning_rate': 4.437460317460317e-05, 'epoch': 0.34}\n",
      "{'loss': 0.002, 'grad_norm': 0.009491881355643272, 'learning_rate': 4.4295238095238094e-05, 'epoch': 0.34}\n",
      "{'loss': 0.0019, 'grad_norm': 2.4437389373779297, 'learning_rate': 4.4215873015873015e-05, 'epoch': 0.35}\n",
      "{'loss': 0.0013, 'grad_norm': 0.00045619337470270693, 'learning_rate': 4.413650793650794e-05, 'epoch': 0.35}\n",
      "{'loss': 0.0008, 'grad_norm': 0.0007383785559795797, 'learning_rate': 4.405714285714286e-05, 'epoch': 0.36}\n",
      "{'loss': 0.0263, 'grad_norm': 0.0022846225183457136, 'learning_rate': 4.397777777777778e-05, 'epoch': 0.36}\n",
      "{'loss': 0.0012, 'grad_norm': 0.0010283580049872398, 'learning_rate': 4.38984126984127e-05, 'epoch': 0.37}\n",
      "{'loss': 0.0, 'grad_norm': 0.0005996509571559727, 'learning_rate': 4.381904761904762e-05, 'epoch': 0.37}\n",
      "{'loss': 0.0, 'grad_norm': 0.0008275876170955598, 'learning_rate': 4.3739682539682544e-05, 'epoch': 0.38}\n",
      "{'loss': 0.0008, 'grad_norm': 0.1375238001346588, 'learning_rate': 4.3660317460317466e-05, 'epoch': 0.38}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0005481703556142747, 'learning_rate': 4.358095238095238e-05, 'epoch': 0.39}\n",
      "{'loss': 0.0, 'grad_norm': 0.0005753362784162164, 'learning_rate': 4.35015873015873e-05, 'epoch': 0.39}\n",
      "{'loss': 0.001, 'grad_norm': 0.04508370906114578, 'learning_rate': 4.3422222222222224e-05, 'epoch': 0.4}\n",
      "{'loss': 0.0, 'grad_norm': 0.000620541803073138, 'learning_rate': 4.3342857142857145e-05, 'epoch': 0.4}\n",
      "{'loss': 0.0, 'grad_norm': 0.0004663907166104764, 'learning_rate': 4.3263492063492067e-05, 'epoch': 0.4}\n",
      "{'loss': 0.0, 'grad_norm': 0.0004522811505012214, 'learning_rate': 4.318412698412699e-05, 'epoch': 0.41}\n",
      "{'loss': 0.0004, 'grad_norm': 0.022920677438378334, 'learning_rate': 4.310476190476191e-05, 'epoch': 0.41}\n",
      "{'loss': 0.0, 'grad_norm': 0.00046371930511668324, 'learning_rate': 4.302539682539683e-05, 'epoch': 0.42}\n",
      "{'loss': 0.0, 'grad_norm': 0.0004454027221072465, 'learning_rate': 4.294603174603175e-05, 'epoch': 0.42}\n",
      "{'loss': 0.0063, 'grad_norm': 0.0020749696996062994, 'learning_rate': 4.286666666666667e-05, 'epoch': 0.43}\n",
      "{'loss': 0.001, 'grad_norm': 0.0004633207863662392, 'learning_rate': 4.278730158730159e-05, 'epoch': 0.43}\n",
      "{'loss': 0.0, 'grad_norm': 0.0004881851782556623, 'learning_rate': 4.270793650793651e-05, 'epoch': 0.44}\n",
      "{'loss': 0.0, 'grad_norm': 0.00038209452759474516, 'learning_rate': 4.262857142857143e-05, 'epoch': 0.44}\n",
      "{'loss': 0.0081, 'grad_norm': 0.0015001923311501741, 'learning_rate': 4.2549206349206347e-05, 'epoch': 0.45}\n",
      "{'loss': 0.0031, 'grad_norm': 0.00041853764560073614, 'learning_rate': 4.246984126984127e-05, 'epoch': 0.45}\n",
      "{'loss': 0.0, 'grad_norm': 0.00039411033503711224, 'learning_rate': 4.239047619047619e-05, 'epoch': 0.46}\n",
      "{'loss': 0.0, 'grad_norm': 0.0004692344518844038, 'learning_rate': 4.231111111111111e-05, 'epoch': 0.46}\n",
      "{'loss': 0.0, 'grad_norm': 0.00036983692552894354, 'learning_rate': 4.223174603174603e-05, 'epoch': 0.47}\n",
      "{'loss': 0.002, 'grad_norm': 0.0024522170424461365, 'learning_rate': 4.2152380952380954e-05, 'epoch': 0.47}\n",
      "{'loss': 0.0001, 'grad_norm': 0.00038519047666341066, 'learning_rate': 4.2073015873015876e-05, 'epoch': 0.48}\n",
      "{'loss': 0.0, 'grad_norm': 0.00036146293859928846, 'learning_rate': 4.19936507936508e-05, 'epoch': 0.48}\n",
      "{'loss': 0.0, 'grad_norm': 0.0004005187947768718, 'learning_rate': 4.191428571428572e-05, 'epoch': 0.49}\n",
      "{'loss': 0.0, 'grad_norm': 0.0003667259879875928, 'learning_rate': 4.183492063492064e-05, 'epoch': 0.49}\n",
      "{'loss': 0.0025, 'grad_norm': 0.8345885872840881, 'learning_rate': 4.175555555555556e-05, 'epoch': 0.5}\n",
      "{'loss': 0.0023, 'grad_norm': 0.00040841507143341005, 'learning_rate': 4.167619047619048e-05, 'epoch': 0.5}\n",
      "{'loss': 0.002, 'grad_norm': 0.0007064865785650909, 'learning_rate': 4.15968253968254e-05, 'epoch': 0.5}\n",
      "{'loss': 0.0012, 'grad_norm': 0.0004516413901001215, 'learning_rate': 4.151746031746032e-05, 'epoch': 0.51}\n",
      "{'loss': 0.0029, 'grad_norm': 0.0005831258604303002, 'learning_rate': 4.143809523809524e-05, 'epoch': 0.51}\n",
      "{'loss': 0.0013, 'grad_norm': 0.0038966515567153692, 'learning_rate': 4.135873015873016e-05, 'epoch': 0.52}\n",
      "{'loss': 0.0005, 'grad_norm': 0.0009400899289175868, 'learning_rate': 4.127936507936508e-05, 'epoch': 0.52}\n",
      "{'loss': 0.0013, 'grad_norm': 0.0006073860567994416, 'learning_rate': 4.12e-05, 'epoch': 0.53}\n",
      "{'loss': 0.0014, 'grad_norm': 1.1911135911941528, 'learning_rate': 4.112063492063492e-05, 'epoch': 0.53}\n",
      "{'loss': 0.0018, 'grad_norm': 0.0004520502407103777, 'learning_rate': 4.104126984126984e-05, 'epoch': 0.54}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0002728794643189758, 'learning_rate': 4.096190476190476e-05, 'epoch': 0.54}\n",
      "{'loss': 0.0, 'grad_norm': 0.00045262303319759667, 'learning_rate': 4.0882539682539685e-05, 'epoch': 0.55}\n",
      "{'loss': 0.0, 'grad_norm': 0.00022299955890048295, 'learning_rate': 4.0803174603174606e-05, 'epoch': 0.55}\n",
      "{'loss': 0.0, 'grad_norm': 0.00044751158566214144, 'learning_rate': 4.072380952380953e-05, 'epoch': 0.56}\n",
      "{'loss': 0.0, 'grad_norm': 0.00026040751254186034, 'learning_rate': 4.064444444444445e-05, 'epoch': 0.56}\n",
      "{'loss': 0.0, 'grad_norm': 0.00024145790666807443, 'learning_rate': 4.0565079365079364e-05, 'epoch': 0.57}\n",
      "{'loss': 0.0, 'grad_norm': 0.00022390679805539548, 'learning_rate': 4.0485714285714285e-05, 'epoch': 0.57}\n",
      "{'loss': 0.0, 'grad_norm': 0.0002511441125534475, 'learning_rate': 4.040634920634921e-05, 'epoch': 0.58}\n",
      "{'loss': 0.0002, 'grad_norm': 0.1268877387046814, 'learning_rate': 4.032698412698413e-05, 'epoch': 0.58}\n",
      "{'loss': 0.0, 'grad_norm': 0.0002587444905657321, 'learning_rate': 4.024761904761905e-05, 'epoch': 0.59}\n",
      "{'loss': 0.0, 'grad_norm': 0.00024814644712023437, 'learning_rate': 4.016825396825397e-05, 'epoch': 0.59}\n",
      "{'loss': 0.0005, 'grad_norm': 0.00028698507230728865, 'learning_rate': 4.008888888888889e-05, 'epoch': 0.6}\n",
      "{'loss': 0.0, 'grad_norm': 0.0002597166458144784, 'learning_rate': 4.0009523809523814e-05, 'epoch': 0.6}\n",
      "{'loss': 0.0, 'grad_norm': 0.0002459499810356647, 'learning_rate': 3.9930158730158736e-05, 'epoch': 0.6}\n",
      "{'loss': 0.0004, 'grad_norm': 0.00041204289300367236, 'learning_rate': 3.985079365079366e-05, 'epoch': 0.61}\n",
      "{'loss': 0.0, 'grad_norm': 0.0002237414155388251, 'learning_rate': 3.977142857142857e-05, 'epoch': 0.61}\n",
      "{'loss': 0.0012, 'grad_norm': 0.0001444915251340717, 'learning_rate': 3.9692063492063494e-05, 'epoch': 0.62}\n",
      "{'loss': 0.0011, 'grad_norm': 0.0006743270205333829, 'learning_rate': 3.9612698412698415e-05, 'epoch': 0.62}\n",
      "{'loss': 0.0023, 'grad_norm': 0.0003004514437634498, 'learning_rate': 3.9533333333333337e-05, 'epoch': 0.63}\n",
      "{'loss': 0.0, 'grad_norm': 0.00020356338063720614, 'learning_rate': 3.945396825396825e-05, 'epoch': 0.63}\n",
      "{'loss': 0.0, 'grad_norm': 0.0002175453118979931, 'learning_rate': 3.937460317460317e-05, 'epoch': 0.64}\n",
      "{'loss': 0.0, 'grad_norm': 0.0001734448451315984, 'learning_rate': 3.9295238095238094e-05, 'epoch': 0.64}\n",
      "{'loss': 0.0054, 'grad_norm': 0.00013874677824787796, 'learning_rate': 3.9215873015873016e-05, 'epoch': 0.65}\n",
      "{'loss': 0.0014, 'grad_norm': 0.00046494888374581933, 'learning_rate': 3.913650793650794e-05, 'epoch': 0.65}\n",
      "{'loss': 0.0045, 'grad_norm': 0.00016189819143619388, 'learning_rate': 3.905714285714286e-05, 'epoch': 0.66}\n",
      "{'loss': 0.0001, 'grad_norm': 0.03742307797074318, 'learning_rate': 3.897777777777778e-05, 'epoch': 0.66}\n",
      "{'loss': 0.0, 'grad_norm': 8.852563769323751e-05, 'learning_rate': 3.88984126984127e-05, 'epoch': 0.67}\n",
      "{'loss': 0.0, 'grad_norm': 6.716660573147237e-05, 'learning_rate': 3.881904761904762e-05, 'epoch': 0.67}\n",
      "{'loss': 0.0, 'grad_norm': 0.0002183375327149406, 'learning_rate': 3.8739682539682545e-05, 'epoch': 0.68}\n",
      "{'loss': 0.0, 'grad_norm': 0.00019527500262483954, 'learning_rate': 3.8660317460317466e-05, 'epoch': 0.68}\n",
      "{'loss': 0.0, 'grad_norm': 0.00023216729459818453, 'learning_rate': 3.858095238095238e-05, 'epoch': 0.69}\n",
      "{'loss': 0.0, 'grad_norm': 6.214177119545639e-05, 'learning_rate': 3.85015873015873e-05, 'epoch': 0.69}\n",
      "{'loss': 0.0, 'grad_norm': 7.089844439178705e-05, 'learning_rate': 3.8422222222222224e-05, 'epoch': 0.7}\n",
      "{'loss': 0.0017, 'grad_norm': 0.00825421791523695, 'learning_rate': 3.8342857142857146e-05, 'epoch': 0.7}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0003373867366462946, 'learning_rate': 3.826349206349207e-05, 'epoch': 0.7}\n",
      "{'loss': 0.0, 'grad_norm': 0.00026014092145487666, 'learning_rate': 3.818412698412698e-05, 'epoch': 0.71}\n",
      "{'loss': 0.0, 'grad_norm': 0.0005573331145569682, 'learning_rate': 3.81047619047619e-05, 'epoch': 0.71}\n",
      "{'loss': 0.0029, 'grad_norm': 0.0007572432514280081, 'learning_rate': 3.8025396825396825e-05, 'epoch': 0.72}\n",
      "{'loss': 0.0003, 'grad_norm': 0.00027124100597575307, 'learning_rate': 3.7946031746031746e-05, 'epoch': 0.72}\n",
      "{'loss': 0.0015, 'grad_norm': 0.00017351566930301487, 'learning_rate': 3.786666666666667e-05, 'epoch': 0.73}\n",
      "{'loss': 0.0, 'grad_norm': 0.0002894280478358269, 'learning_rate': 3.778730158730159e-05, 'epoch': 0.73}\n",
      "{'loss': 0.0, 'grad_norm': 8.788363629719242e-05, 'learning_rate': 3.770793650793651e-05, 'epoch': 0.74}\n",
      "{'loss': 0.0, 'grad_norm': 9.476043487666175e-05, 'learning_rate': 3.762857142857143e-05, 'epoch': 0.74}\n",
      "{'loss': 0.0, 'grad_norm': 8.993536175694317e-05, 'learning_rate': 3.7549206349206354e-05, 'epoch': 0.75}\n",
      "{'loss': 0.0001, 'grad_norm': 9.082932228920981e-05, 'learning_rate': 3.747142857142858e-05, 'epoch': 0.75}\n",
      "{'loss': 0.0, 'grad_norm': 9.905364277074113e-05, 'learning_rate': 3.739206349206349e-05, 'epoch': 0.76}\n",
      "{'loss': 0.0, 'grad_norm': 4.3572297727223486e-05, 'learning_rate': 3.731269841269841e-05, 'epoch': 0.76}\n",
      "{'loss': 0.0013, 'grad_norm': 0.032278597354888916, 'learning_rate': 3.7233333333333335e-05, 'epoch': 0.77}\n",
      "{'loss': 0.0, 'grad_norm': 0.00013033169670961797, 'learning_rate': 3.7153968253968256e-05, 'epoch': 0.77}\n",
      "{'loss': 0.0, 'grad_norm': 0.0001379491586703807, 'learning_rate': 3.707460317460318e-05, 'epoch': 0.78}\n",
      "{'loss': 0.0006, 'grad_norm': 6.639942148467526e-05, 'learning_rate': 3.69952380952381e-05, 'epoch': 0.78}\n",
      "{'loss': 0.0004, 'grad_norm': 0.0009391785133630037, 'learning_rate': 3.691587301587302e-05, 'epoch': 0.79}\n",
      "{'loss': 0.0, 'grad_norm': 8.205506310332566e-05, 'learning_rate': 3.6836507936507936e-05, 'epoch': 0.79}\n",
      "{'loss': 0.0, 'grad_norm': 8.848655852489173e-05, 'learning_rate': 3.675714285714286e-05, 'epoch': 0.8}\n",
      "{'loss': 0.0, 'grad_norm': 6.642745574936271e-05, 'learning_rate': 3.667777777777778e-05, 'epoch': 0.8}\n",
      "{'loss': 0.0, 'grad_norm': 5.892453555134125e-05, 'learning_rate': 3.65984126984127e-05, 'epoch': 0.8}\n",
      "{'loss': 0.0008, 'grad_norm': 8.959207480074838e-05, 'learning_rate': 3.651904761904762e-05, 'epoch': 0.81}\n",
      "{'loss': 0.0, 'grad_norm': 7.45978468330577e-05, 'learning_rate': 3.643968253968254e-05, 'epoch': 0.81}\n",
      "{'loss': 0.0, 'grad_norm': 6.323914567474276e-05, 'learning_rate': 3.6360317460317465e-05, 'epoch': 0.82}\n",
      "{'loss': 0.0006, 'grad_norm': 0.0002128781343344599, 'learning_rate': 3.628095238095238e-05, 'epoch': 0.82}\n",
      "{'loss': 0.0, 'grad_norm': 0.00011864098632941023, 'learning_rate': 3.62015873015873e-05, 'epoch': 0.83}\n",
      "{'loss': 0.0026, 'grad_norm': 7.935738540254533e-05, 'learning_rate': 3.612222222222222e-05, 'epoch': 0.83}\n",
      "{'loss': 0.0, 'grad_norm': 9.936226706486195e-05, 'learning_rate': 3.6042857142857144e-05, 'epoch': 0.84}\n",
      "{'loss': 0.0, 'grad_norm': 8.400982187595218e-05, 'learning_rate': 3.5963492063492065e-05, 'epoch': 0.84}\n",
      "{'loss': 0.004, 'grad_norm': 0.00012399665138218552, 'learning_rate': 3.588412698412699e-05, 'epoch': 0.85}\n",
      "{'loss': 0.0021, 'grad_norm': 0.00016387025243602693, 'learning_rate': 3.580476190476191e-05, 'epoch': 0.85}\n",
      "{'loss': 0.0043, 'grad_norm': 0.015885574743151665, 'learning_rate': 3.572539682539683e-05, 'epoch': 0.86}\n",
      "{'loss': 0.0, 'grad_norm': 0.0004120222292840481, 'learning_rate': 3.564603174603175e-05, 'epoch': 0.86}\n",
      "{'loss': 0.0, 'grad_norm': 0.00030731287552043796, 'learning_rate': 3.556666666666667e-05, 'epoch': 0.87}\n",
      "{'loss': 0.0, 'grad_norm': 0.00015655616880394518, 'learning_rate': 3.5487301587301594e-05, 'epoch': 0.87}\n",
      "{'loss': 0.0, 'grad_norm': 0.00024284794926643372, 'learning_rate': 3.540793650793651e-05, 'epoch': 0.88}\n",
      "{'loss': 0.0, 'grad_norm': 0.00014130474301055074, 'learning_rate': 3.532857142857143e-05, 'epoch': 0.88}\n",
      "{'loss': 0.0, 'grad_norm': 0.00011913720663869753, 'learning_rate': 3.5249206349206345e-05, 'epoch': 0.89}\n",
      "{'loss': 0.0, 'grad_norm': 0.00012038971908623353, 'learning_rate': 3.516984126984127e-05, 'epoch': 0.89}\n",
      "{'loss': 0.0, 'grad_norm': 0.00028002995532006025, 'learning_rate': 3.509047619047619e-05, 'epoch': 0.9}\n",
      "{'loss': 0.0, 'grad_norm': 0.00011266864748904482, 'learning_rate': 3.501111111111111e-05, 'epoch': 0.9}\n",
      "{'loss': 0.0, 'grad_norm': 0.00010599770757835358, 'learning_rate': 3.493174603174603e-05, 'epoch': 0.9}\n",
      "{'loss': 0.0, 'grad_norm': 8.463350241072476e-05, 'learning_rate': 3.485238095238095e-05, 'epoch': 0.91}\n",
      "{'loss': 0.0, 'grad_norm': 5.8258188801119104e-05, 'learning_rate': 3.4773015873015874e-05, 'epoch': 0.91}\n",
      "{'loss': 0.0, 'grad_norm': 0.00011811698641395196, 'learning_rate': 3.4693650793650796e-05, 'epoch': 0.92}\n",
      "{'loss': 0.0, 'grad_norm': 7.855692092562094e-05, 'learning_rate': 3.461428571428572e-05, 'epoch': 0.92}\n",
      "{'loss': 0.0, 'grad_norm': 4.403849379741587e-05, 'learning_rate': 3.453492063492064e-05, 'epoch': 0.93}\n",
      "{'loss': 0.0, 'grad_norm': 0.0001234374358318746, 'learning_rate': 3.445555555555556e-05, 'epoch': 0.93}\n",
      "{'loss': 0.0, 'grad_norm': 4.056882607983425e-05, 'learning_rate': 3.437619047619048e-05, 'epoch': 0.94}\n",
      "{'loss': 0.0, 'grad_norm': 5.0845937948906794e-05, 'learning_rate': 3.4296825396825397e-05, 'epoch': 0.94}\n",
      "{'loss': 0.0, 'grad_norm': 7.424213254125789e-05, 'learning_rate': 3.421746031746032e-05, 'epoch': 0.95}\n",
      "{'loss': 0.0, 'grad_norm': 0.00023050182790029794, 'learning_rate': 3.413809523809524e-05, 'epoch': 0.95}\n",
      "{'loss': 0.0, 'grad_norm': 8.872713806340471e-05, 'learning_rate': 3.405873015873016e-05, 'epoch': 0.96}\n",
      "{'loss': 0.0, 'grad_norm': 9.745117131387815e-05, 'learning_rate': 3.397936507936508e-05, 'epoch': 0.96}\n",
      "{'loss': 0.0, 'grad_norm': 6.272608879953623e-05, 'learning_rate': 3.3900000000000004e-05, 'epoch': 0.97}\n",
      "{'loss': 0.0, 'grad_norm': 4.6260323870228603e-05, 'learning_rate': 3.3820634920634926e-05, 'epoch': 0.97}\n",
      "{'loss': 0.0, 'grad_norm': 4.165842256043106e-05, 'learning_rate': 3.374126984126984e-05, 'epoch': 0.98}\n",
      "{'loss': 0.0, 'grad_norm': 3.997980093117803e-05, 'learning_rate': 3.366190476190476e-05, 'epoch': 0.98}\n",
      "{'loss': 0.0, 'grad_norm': 4.4607404561247677e-05, 'learning_rate': 3.358253968253968e-05, 'epoch': 0.99}\n",
      "{'loss': 0.0, 'grad_norm': 4.3904674384975806e-05, 'learning_rate': 3.3503174603174605e-05, 'epoch': 0.99}\n",
      "{'loss': 0.0, 'grad_norm': 6.186599057400599e-05, 'learning_rate': 3.3423809523809526e-05, 'epoch': 1.0}\n",
      "{'loss': 0.0, 'grad_norm': 2.802263043122366e-05, 'learning_rate': 3.334444444444445e-05, 'epoch': 1.0}\n",
      "{'loss': 0.0, 'grad_norm': 4.758349678013474e-05, 'learning_rate': 3.326507936507936e-05, 'epoch': 1.0}\n",
      "{'loss': 0.0, 'grad_norm': 3.687842036015354e-05, 'learning_rate': 3.3185714285714284e-05, 'epoch': 1.01}\n",
      "{'loss': 0.0, 'grad_norm': 6.486203346867114e-05, 'learning_rate': 3.3106349206349206e-05, 'epoch': 1.01}\n",
      "{'loss': 0.0, 'grad_norm': 5.061615593149327e-05, 'learning_rate': 3.302698412698413e-05, 'epoch': 1.02}\n",
      "{'loss': 0.0, 'grad_norm': 3.745013600564562e-05, 'learning_rate': 3.294761904761905e-05, 'epoch': 1.02}\n",
      "{'loss': 0.0, 'grad_norm': 3.1798594136489555e-05, 'learning_rate': 3.286825396825397e-05, 'epoch': 1.03}\n",
      "{'loss': 0.0, 'grad_norm': 2.9020633519394323e-05, 'learning_rate': 3.278888888888889e-05, 'epoch': 1.03}\n",
      "{'loss': 0.0, 'grad_norm': 3.15054421662353e-05, 'learning_rate': 3.270952380952381e-05, 'epoch': 1.04}\n",
      "{'loss': 0.0, 'grad_norm': 2.9134924261597916e-05, 'learning_rate': 3.2630158730158735e-05, 'epoch': 1.04}\n",
      "{'loss': 0.0, 'grad_norm': 5.085239536128938e-05, 'learning_rate': 3.2550793650793656e-05, 'epoch': 1.05}\n",
      "{'loss': 0.0, 'grad_norm': 4.048168921144679e-05, 'learning_rate': 3.247142857142858e-05, 'epoch': 1.05}\n",
      "{'loss': 0.0, 'grad_norm': 2.6224895918858238e-05, 'learning_rate': 3.23920634920635e-05, 'epoch': 1.06}\n",
      "{'loss': 0.0, 'grad_norm': 2.5774908863240853e-05, 'learning_rate': 3.2312698412698414e-05, 'epoch': 1.06}\n",
      "{'loss': 0.0, 'grad_norm': 3.942153125535697e-05, 'learning_rate': 3.2233333333333335e-05, 'epoch': 1.07}\n",
      "{'loss': 0.0, 'grad_norm': 2.2758493287255988e-05, 'learning_rate': 3.215396825396825e-05, 'epoch': 1.07}\n",
      "{'loss': 0.0002, 'grad_norm': 0.0014125594170764089, 'learning_rate': 3.207460317460317e-05, 'epoch': 1.08}\n",
      "{'loss': 0.0003, 'grad_norm': 2.0534063878585584e-05, 'learning_rate': 3.199523809523809e-05, 'epoch': 1.08}\n",
      "{'loss': 0.0, 'grad_norm': 1.8668986740522087e-05, 'learning_rate': 3.1915873015873015e-05, 'epoch': 1.09}\n",
      "{'loss': 0.0, 'grad_norm': 1.6418089217040688e-05, 'learning_rate': 3.1836507936507936e-05, 'epoch': 1.09}\n",
      "{'loss': 0.0, 'grad_norm': 2.0948094970663078e-05, 'learning_rate': 3.175714285714286e-05, 'epoch': 1.1}\n",
      "{'loss': 0.0, 'grad_norm': 1.6805573977762833e-05, 'learning_rate': 3.167777777777778e-05, 'epoch': 1.1}\n",
      "{'loss': 0.0, 'grad_norm': 4.6914079575799406e-05, 'learning_rate': 3.15984126984127e-05, 'epoch': 1.1}\n",
      "{'loss': 0.0, 'grad_norm': 1.7003449102048762e-05, 'learning_rate': 3.151904761904762e-05, 'epoch': 1.11}\n",
      "{'loss': 0.0, 'grad_norm': 1.8740702216746286e-05, 'learning_rate': 3.1439682539682544e-05, 'epoch': 1.11}\n",
      "{'loss': 0.0, 'grad_norm': 1.5571689800708555e-05, 'learning_rate': 3.1360317460317465e-05, 'epoch': 1.12}\n",
      "{'loss': 0.0, 'grad_norm': 1.6333489838871174e-05, 'learning_rate': 3.128095238095238e-05, 'epoch': 1.12}\n",
      "{'loss': 0.0, 'grad_norm': 1.7708607629174367e-05, 'learning_rate': 3.12015873015873e-05, 'epoch': 1.13}\n",
      "{'loss': 0.0, 'grad_norm': 1.478009744459996e-05, 'learning_rate': 3.112222222222222e-05, 'epoch': 1.13}\n",
      "{'loss': 0.0015, 'grad_norm': 0.00047506197006441653, 'learning_rate': 3.1042857142857144e-05, 'epoch': 1.14}\n",
      "{'loss': 0.0, 'grad_norm': 6.90691958880052e-05, 'learning_rate': 3.0963492063492066e-05, 'epoch': 1.14}\n",
      "{'loss': 0.0, 'grad_norm': 3.722827023011632e-05, 'learning_rate': 3.088412698412699e-05, 'epoch': 1.15}\n",
      "{'loss': 0.0, 'grad_norm': 3.52369861502666e-05, 'learning_rate': 3.080476190476191e-05, 'epoch': 1.15}\n",
      "{'loss': 0.0, 'grad_norm': 3.5907054552808404e-05, 'learning_rate': 3.072539682539683e-05, 'epoch': 1.16}\n",
      "{'loss': 0.0, 'grad_norm': 6.261931412154809e-05, 'learning_rate': 3.0646031746031745e-05, 'epoch': 1.16}\n",
      "{'loss': 0.0, 'grad_norm': 2.9173088478273712e-05, 'learning_rate': 3.0566666666666667e-05, 'epoch': 1.17}\n",
      "{'loss': 0.0, 'grad_norm': 2.7319154469296336e-05, 'learning_rate': 3.0487301587301588e-05, 'epoch': 1.17}\n",
      "{'loss': 0.0, 'grad_norm': 2.975234565383289e-05, 'learning_rate': 3.0407936507936506e-05, 'epoch': 1.18}\n",
      "{'loss': 0.0004, 'grad_norm': 0.004004251677542925, 'learning_rate': 3.0328571428571428e-05, 'epoch': 1.18}\n",
      "{'loss': 0.0004, 'grad_norm': 0.0001348936348222196, 'learning_rate': 3.024920634920635e-05, 'epoch': 1.19}\n",
      "{'loss': 0.0, 'grad_norm': 0.00017020496306940913, 'learning_rate': 3.016984126984127e-05, 'epoch': 1.19}\n",
      "{'loss': 0.0006, 'grad_norm': 3.4475386200938374e-05, 'learning_rate': 3.0090476190476192e-05, 'epoch': 1.2}\n",
      "{'loss': 0.0, 'grad_norm': 6.172927533043548e-05, 'learning_rate': 3.0011111111111114e-05, 'epoch': 1.2}\n",
      "{'loss': 0.0012, 'grad_norm': 0.00017461925745010376, 'learning_rate': 2.9931746031746032e-05, 'epoch': 1.2}\n",
      "{'loss': 0.0, 'grad_norm': 8.69295108714141e-05, 'learning_rate': 2.9852380952380953e-05, 'epoch': 1.21}\n",
      "{'loss': 0.0001, 'grad_norm': 3.6536335945129395, 'learning_rate': 2.9773015873015875e-05, 'epoch': 1.21}\n",
      "{'loss': 0.0011, 'grad_norm': 0.009630736894905567, 'learning_rate': 2.9693650793650796e-05, 'epoch': 1.22}\n",
      "{'loss': 0.0, 'grad_norm': 0.02195196971297264, 'learning_rate': 2.9614285714285718e-05, 'epoch': 1.22}\n",
      "{'loss': 0.0, 'grad_norm': 0.00011737056047422811, 'learning_rate': 2.9534920634920636e-05, 'epoch': 1.23}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0003141925553791225, 'learning_rate': 2.9455555555555557e-05, 'epoch': 1.23}\n",
      "{'loss': 0.0, 'grad_norm': 6.976025906624272e-05, 'learning_rate': 2.937619047619048e-05, 'epoch': 1.24}\n",
      "{'loss': 0.0, 'grad_norm': 3.877355993608944e-05, 'learning_rate': 2.92968253968254e-05, 'epoch': 1.24}\n",
      "{'loss': 0.0, 'grad_norm': 6.138841126812622e-05, 'learning_rate': 2.9217460317460322e-05, 'epoch': 1.25}\n",
      "{'loss': 0.0, 'grad_norm': 8.649571827845648e-05, 'learning_rate': 2.9138095238095243e-05, 'epoch': 1.25}\n",
      "{'loss': 0.0, 'grad_norm': 7.17414150130935e-05, 'learning_rate': 2.9058730158730158e-05, 'epoch': 1.26}\n",
      "{'loss': 0.0, 'grad_norm': 7.193392957560718e-05, 'learning_rate': 2.897936507936508e-05, 'epoch': 1.26}\n",
      "{'loss': 0.0, 'grad_norm': 3.52817696693819e-05, 'learning_rate': 2.8899999999999998e-05, 'epoch': 1.27}\n",
      "{'loss': 0.0, 'grad_norm': 2.7695565222529694e-05, 'learning_rate': 2.882063492063492e-05, 'epoch': 1.27}\n",
      "{'loss': 0.0, 'grad_norm': 4.779509981744923e-05, 'learning_rate': 2.874126984126984e-05, 'epoch': 1.28}\n",
      "{'loss': 0.001, 'grad_norm': 0.001608522841706872, 'learning_rate': 2.8661904761904762e-05, 'epoch': 1.28}\n",
      "{'loss': 0.0, 'grad_norm': 2.2041958800400607e-05, 'learning_rate': 2.8582539682539684e-05, 'epoch': 1.29}\n",
      "{'loss': 0.0, 'grad_norm': 0.00011791187716880813, 'learning_rate': 2.8503174603174605e-05, 'epoch': 1.29}\n",
      "{'loss': 0.0, 'grad_norm': 2.2684935174765997e-05, 'learning_rate': 2.8423809523809523e-05, 'epoch': 1.3}\n",
      "{'loss': 0.0, 'grad_norm': 0.04410604014992714, 'learning_rate': 2.8344444444444445e-05, 'epoch': 1.3}\n",
      "{'loss': 0.0, 'grad_norm': 1.6391673852922395e-05, 'learning_rate': 2.8265079365079366e-05, 'epoch': 1.3}\n",
      "{'loss': 0.0, 'grad_norm': 2.4855338779161684e-05, 'learning_rate': 2.8185714285714288e-05, 'epoch': 1.31}\n",
      "{'loss': 0.0, 'grad_norm': 2.0124462025705725e-05, 'learning_rate': 2.810634920634921e-05, 'epoch': 1.31}\n",
      "{'loss': 0.0, 'grad_norm': 2.0577463146764785e-05, 'learning_rate': 2.802698412698413e-05, 'epoch': 1.32}\n",
      "{'loss': 0.0, 'grad_norm': 2.4757198843872175e-05, 'learning_rate': 2.794761904761905e-05, 'epoch': 1.32}\n",
      "{'loss': 0.0, 'grad_norm': 1.781618266250007e-05, 'learning_rate': 2.786825396825397e-05, 'epoch': 1.33}\n",
      "{'loss': 0.0, 'grad_norm': 2.558638334448915e-05, 'learning_rate': 2.7788888888888892e-05, 'epoch': 1.33}\n",
      "{'loss': 0.0, 'grad_norm': 2.0959716493962333e-05, 'learning_rate': 2.7709523809523813e-05, 'epoch': 1.34}\n",
      "{'loss': 0.0, 'grad_norm': 1.7914735508384183e-05, 'learning_rate': 2.7630158730158735e-05, 'epoch': 1.34}\n",
      "{'loss': 0.0, 'grad_norm': 1.7866052075987682e-05, 'learning_rate': 2.7550793650793657e-05, 'epoch': 1.35}\n",
      "{'loss': 0.0, 'grad_norm': 1.7774449588614516e-05, 'learning_rate': 2.747142857142857e-05, 'epoch': 1.35}\n",
      "{'loss': 0.0, 'grad_norm': 1.5213767255772837e-05, 'learning_rate': 2.7392063492063493e-05, 'epoch': 1.36}\n",
      "{'loss': 0.0, 'grad_norm': 3.066920180572197e-05, 'learning_rate': 2.731269841269841e-05, 'epoch': 1.36}\n",
      "{'loss': 0.0, 'grad_norm': 1.0767303137981798e-05, 'learning_rate': 2.7233333333333332e-05, 'epoch': 1.37}\n",
      "{'loss': 0.0, 'grad_norm': 0.00021214595471974462, 'learning_rate': 2.7153968253968254e-05, 'epoch': 1.37}\n",
      "{'loss': 0.0, 'grad_norm': 1.0095456673298031e-05, 'learning_rate': 2.7074603174603175e-05, 'epoch': 1.38}\n",
      "{'loss': 0.0, 'grad_norm': 3.95984789065551e-05, 'learning_rate': 2.6995238095238097e-05, 'epoch': 1.38}\n",
      "{'loss': 0.0, 'grad_norm': 1.5224062735796906e-05, 'learning_rate': 2.6915873015873015e-05, 'epoch': 1.39}\n",
      "{'loss': 0.0, 'grad_norm': 9.318277989223134e-06, 'learning_rate': 2.6836507936507936e-05, 'epoch': 1.39}\n",
      "{'loss': 0.0, 'grad_norm': 1.3024945474171545e-05, 'learning_rate': 2.6757142857142858e-05, 'epoch': 1.4}\n",
      "{'loss': 0.0, 'grad_norm': 9.010263056552503e-06, 'learning_rate': 2.667777777777778e-05, 'epoch': 1.4}\n",
      "{'loss': 0.0, 'grad_norm': 1.1423479008954018e-05, 'learning_rate': 2.65984126984127e-05, 'epoch': 1.4}\n",
      "{'loss': 0.0, 'grad_norm': 2.0801466234843247e-05, 'learning_rate': 2.6519047619047622e-05, 'epoch': 1.41}\n",
      "{'loss': 0.0, 'grad_norm': 4.05718274123501e-05, 'learning_rate': 2.643968253968254e-05, 'epoch': 1.41}\n",
      "{'loss': 0.0, 'grad_norm': 1.572333167132456e-05, 'learning_rate': 2.6360317460317462e-05, 'epoch': 1.42}\n",
      "{'loss': 0.0, 'grad_norm': 7.416868356813211e-06, 'learning_rate': 2.6280952380952384e-05, 'epoch': 1.42}\n",
      "{'loss': 0.0, 'grad_norm': 1.3331730770005379e-05, 'learning_rate': 2.6201587301587305e-05, 'epoch': 1.43}\n",
      "{'loss': 0.0, 'grad_norm': 1.2507460269262083e-05, 'learning_rate': 2.6122222222222227e-05, 'epoch': 1.43}\n",
      "{'loss': 0.0, 'grad_norm': 9.410180609847885e-06, 'learning_rate': 2.6042857142857148e-05, 'epoch': 1.44}\n",
      "{'loss': 0.0, 'grad_norm': 1.9015002180822194e-05, 'learning_rate': 2.5963492063492063e-05, 'epoch': 1.44}\n",
      "{'loss': 0.0, 'grad_norm': 1.7794824088923633e-05, 'learning_rate': 2.5884126984126984e-05, 'epoch': 1.45}\n",
      "{'loss': 0.0, 'grad_norm': 1.4579924027202651e-05, 'learning_rate': 2.5804761904761902e-05, 'epoch': 1.45}\n",
      "{'loss': 0.0, 'grad_norm': 9.575723197485786e-06, 'learning_rate': 2.5725396825396824e-05, 'epoch': 1.46}\n",
      "{'loss': 0.0, 'grad_norm': 1.706654620647896e-05, 'learning_rate': 2.5646031746031745e-05, 'epoch': 1.46}\n",
      "{'loss': 0.0, 'grad_norm': 8.825854820315726e-06, 'learning_rate': 2.5566666666666667e-05, 'epoch': 1.47}\n",
      "{'loss': 0.0, 'grad_norm': 2.5535331587889232e-05, 'learning_rate': 2.548730158730159e-05, 'epoch': 1.47}\n",
      "{'loss': 0.0, 'grad_norm': 1.0580451089481357e-05, 'learning_rate': 2.540793650793651e-05, 'epoch': 1.48}\n",
      "{'loss': 0.0, 'grad_norm': 9.475653314439114e-06, 'learning_rate': 2.5328571428571428e-05, 'epoch': 1.48}\n",
      "{'loss': 0.0, 'grad_norm': 6.314900474535534e-06, 'learning_rate': 2.524920634920635e-05, 'epoch': 1.49}\n",
      "{'loss': 0.0, 'grad_norm': 6.39880818198435e-06, 'learning_rate': 2.516984126984127e-05, 'epoch': 1.49}\n",
      "{'loss': 0.0, 'grad_norm': 4.59153889096342e-05, 'learning_rate': 2.5090476190476193e-05, 'epoch': 1.5}\n",
      "{'loss': 0.0, 'grad_norm': 6.239491995074786e-06, 'learning_rate': 2.5011111111111114e-05, 'epoch': 1.5}\n",
      "{'loss': 0.0, 'grad_norm': 8.45660633785883e-06, 'learning_rate': 2.4931746031746032e-05, 'epoch': 1.5}\n",
      "{'loss': 0.0, 'grad_norm': 1.0857896086235996e-05, 'learning_rate': 2.4852380952380954e-05, 'epoch': 1.51}\n",
      "{'loss': 0.0, 'grad_norm': 5.872584551980253e-06, 'learning_rate': 2.4773015873015872e-05, 'epoch': 1.51}\n",
      "{'loss': 0.0, 'grad_norm': 5.295415576256346e-06, 'learning_rate': 2.4693650793650793e-05, 'epoch': 1.52}\n",
      "{'loss': 0.0, 'grad_norm': 9.962635886040516e-06, 'learning_rate': 2.4614285714285715e-05, 'epoch': 1.52}\n",
      "{'loss': 0.0, 'grad_norm': 1.369302026432706e-05, 'learning_rate': 2.4534920634920636e-05, 'epoch': 1.53}\n",
      "{'loss': 0.0, 'grad_norm': 6.646208476013271e-06, 'learning_rate': 2.4455555555555558e-05, 'epoch': 1.53}\n",
      "{'loss': 0.0, 'grad_norm': 4.748562787426636e-06, 'learning_rate': 2.4376190476190476e-05, 'epoch': 1.54}\n",
      "{'loss': 0.0, 'grad_norm': 5.759834493801463e-06, 'learning_rate': 2.4296825396825397e-05, 'epoch': 1.54}\n",
      "{'loss': 0.0, 'grad_norm': 4.89656258650939e-06, 'learning_rate': 2.421746031746032e-05, 'epoch': 1.55}\n",
      "{'loss': 0.0, 'grad_norm': 5.152552603249205e-06, 'learning_rate': 2.413809523809524e-05, 'epoch': 1.55}\n",
      "{'loss': 0.0, 'grad_norm': 5.751356638938887e-06, 'learning_rate': 2.4058730158730162e-05, 'epoch': 1.56}\n",
      "{'loss': 0.0, 'grad_norm': 5.45917964700493e-06, 'learning_rate': 2.397936507936508e-05, 'epoch': 1.56}\n",
      "{'loss': 0.0, 'grad_norm': 1.0189829481532797e-05, 'learning_rate': 2.39e-05, 'epoch': 1.57}\n",
      "{'loss': 0.0, 'grad_norm': 6.632005352003034e-06, 'learning_rate': 2.382063492063492e-05, 'epoch': 1.57}\n",
      "{'loss': 0.0, 'grad_norm': 6.627262337133288e-06, 'learning_rate': 2.374126984126984e-05, 'epoch': 1.58}\n",
      "{'loss': 0.0, 'grad_norm': 6.092629064369248e-06, 'learning_rate': 2.3661904761904763e-05, 'epoch': 1.58}\n",
      "{'loss': 0.0, 'grad_norm': 4.740224994748132e-06, 'learning_rate': 2.3582539682539684e-05, 'epoch': 1.59}\n",
      "{'loss': 0.0, 'grad_norm': 6.865851446491433e-06, 'learning_rate': 2.3503174603174606e-05, 'epoch': 1.59}\n",
      "{'loss': 0.0, 'grad_norm': 3.3282321965089068e-06, 'learning_rate': 2.3423809523809527e-05, 'epoch': 1.6}\n",
      "{'loss': 0.0, 'grad_norm': 4.016745606350014e-06, 'learning_rate': 2.3344444444444445e-05, 'epoch': 1.6}\n",
      "{'loss': 0.0, 'grad_norm': 5.46897263120627e-06, 'learning_rate': 2.3265079365079367e-05, 'epoch': 1.6}\n",
      "{'loss': 0.0, 'grad_norm': 4.07607603847282e-06, 'learning_rate': 2.3185714285714285e-05, 'epoch': 1.61}\n",
      "{'loss': 0.0, 'grad_norm': 4.108042503503384e-06, 'learning_rate': 2.3106349206349206e-05, 'epoch': 1.61}\n",
      "{'loss': 0.0, 'grad_norm': 6.834502528363373e-06, 'learning_rate': 2.3026984126984128e-05, 'epoch': 1.62}\n",
      "{'loss': 0.0, 'grad_norm': 4.4635930862568785e-06, 'learning_rate': 2.294761904761905e-05, 'epoch': 1.62}\n",
      "{'loss': 0.0, 'grad_norm': 3.7229472127364716e-06, 'learning_rate': 2.286825396825397e-05, 'epoch': 1.63}\n",
      "{'loss': 0.0, 'grad_norm': 5.3374933486338705e-06, 'learning_rate': 2.278888888888889e-05, 'epoch': 1.63}\n",
      "{'loss': 0.0, 'grad_norm': 6.251650120248087e-06, 'learning_rate': 2.270952380952381e-05, 'epoch': 1.64}\n",
      "{'loss': 0.0, 'grad_norm': 9.784176654648036e-06, 'learning_rate': 2.2630158730158732e-05, 'epoch': 1.64}\n",
      "{'loss': 0.0, 'grad_norm': 5.057546331954654e-06, 'learning_rate': 2.2550793650793654e-05, 'epoch': 1.65}\n",
      "{'loss': 0.0, 'grad_norm': 5.325532129063504e-06, 'learning_rate': 2.2471428571428575e-05, 'epoch': 1.65}\n",
      "{'loss': 0.0, 'grad_norm': 5.190375304664485e-06, 'learning_rate': 2.2392063492063493e-05, 'epoch': 1.66}\n",
      "{'loss': 0.0, 'grad_norm': 2.5263377665396547e-06, 'learning_rate': 2.231269841269841e-05, 'epoch': 1.66}\n",
      "{'loss': 0.0, 'grad_norm': 2.127848802047083e-06, 'learning_rate': 2.2233333333333333e-05, 'epoch': 1.67}\n",
      "{'loss': 0.0, 'grad_norm': 5.2664108807221055e-06, 'learning_rate': 2.2153968253968254e-05, 'epoch': 1.67}\n",
      "{'loss': 0.0, 'grad_norm': 2.5234501208615256e-06, 'learning_rate': 2.2074603174603176e-05, 'epoch': 1.68}\n",
      "{'loss': 0.0, 'grad_norm': 2.675296855159104e-06, 'learning_rate': 2.1995238095238097e-05, 'epoch': 1.68}\n",
      "{'loss': 0.0, 'grad_norm': 7.422828275593929e-06, 'learning_rate': 2.191587301587302e-05, 'epoch': 1.69}\n",
      "{'loss': 0.0, 'grad_norm': 2.6790974061441375e-06, 'learning_rate': 2.1836507936507937e-05, 'epoch': 1.69}\n",
      "{'loss': 0.0, 'grad_norm': 2.5731424102559686e-06, 'learning_rate': 2.175714285714286e-05, 'epoch': 1.7}\n",
      "{'loss': 0.0, 'grad_norm': 3.2238508538284805e-06, 'learning_rate': 2.167777777777778e-05, 'epoch': 1.7}\n",
      "{'loss': 0.0, 'grad_norm': 2.5196914066327736e-05, 'learning_rate': 2.1598412698412698e-05, 'epoch': 1.7}\n",
      "{'loss': 0.0, 'grad_norm': 3.989941887994064e-06, 'learning_rate': 2.151904761904762e-05, 'epoch': 1.71}\n",
      "{'loss': 0.0, 'grad_norm': 2.5266060674766777e-06, 'learning_rate': 2.143968253968254e-05, 'epoch': 1.71}\n",
      "{'loss': 0.0, 'grad_norm': 1.959336032086867e-06, 'learning_rate': 2.1360317460317463e-05, 'epoch': 1.72}\n",
      "{'loss': 0.0, 'grad_norm': 2.0776737983396742e-06, 'learning_rate': 2.128095238095238e-05, 'epoch': 1.72}\n",
      "{'loss': 0.0, 'grad_norm': 2.4510595721949358e-06, 'learning_rate': 2.1201587301587302e-05, 'epoch': 1.73}\n",
      "{'loss': 0.0, 'grad_norm': 3.883993031195132e-06, 'learning_rate': 2.1122222222222224e-05, 'epoch': 1.73}\n",
      "{'loss': 0.0, 'grad_norm': 1.6805561244837008e-06, 'learning_rate': 2.1042857142857145e-05, 'epoch': 1.74}\n",
      "{'loss': 0.0, 'grad_norm': 1.0365673006162979e-05, 'learning_rate': 2.0963492063492067e-05, 'epoch': 1.74}\n",
      "{'loss': 0.0, 'grad_norm': 1.5205058616629685e-06, 'learning_rate': 2.0884126984126985e-05, 'epoch': 1.75}\n",
      "{'loss': 0.0, 'grad_norm': 1.7462689356761985e-06, 'learning_rate': 2.0804761904761906e-05, 'epoch': 1.75}\n",
      "{'loss': 0.0, 'grad_norm': 1.3604009154732921e-06, 'learning_rate': 2.0725396825396824e-05, 'epoch': 1.76}\n",
      "{'loss': 0.0, 'grad_norm': 3.716483206517296e-06, 'learning_rate': 2.0646031746031746e-05, 'epoch': 1.76}\n",
      "{'loss': 0.0, 'grad_norm': 1.4980062132963212e-06, 'learning_rate': 2.0566666666666667e-05, 'epoch': 1.77}\n",
      "{'loss': 0.0, 'grad_norm': 3.6944982184650144e-06, 'learning_rate': 2.048730158730159e-05, 'epoch': 1.77}\n",
      "{'loss': 0.0, 'grad_norm': 1.2218981737532886e-06, 'learning_rate': 2.040793650793651e-05, 'epoch': 1.78}\n",
      "{'loss': 0.0, 'grad_norm': 3.0345786399266217e-06, 'learning_rate': 2.032857142857143e-05, 'epoch': 1.78}\n",
      "{'loss': 0.0, 'grad_norm': 1.7937484244612278e-06, 'learning_rate': 2.024920634920635e-05, 'epoch': 1.79}\n",
      "{'loss': 0.0, 'grad_norm': 4.07079232900287e-06, 'learning_rate': 2.016984126984127e-05, 'epoch': 1.79}\n",
      "{'loss': 0.0, 'grad_norm': 3.7652366700058337e-06, 'learning_rate': 2.009047619047619e-05, 'epoch': 1.8}\n",
      "{'loss': 0.0, 'grad_norm': 5.636109108309029e-06, 'learning_rate': 2.001111111111111e-05, 'epoch': 1.8}\n",
      "{'loss': 0.0, 'grad_norm': 2.2421322682930622e-06, 'learning_rate': 1.9931746031746033e-05, 'epoch': 1.8}\n",
      "{'loss': 0.0, 'grad_norm': 1.3658354873768985e-06, 'learning_rate': 1.9852380952380954e-05, 'epoch': 1.81}\n",
      "{'loss': 0.0, 'grad_norm': 1.8340195993005182e-06, 'learning_rate': 1.9773015873015872e-05, 'epoch': 1.81}\n",
      "{'loss': 0.0, 'grad_norm': 5.171760221855948e-06, 'learning_rate': 1.9693650793650794e-05, 'epoch': 1.82}\n",
      "{'loss': 0.0, 'grad_norm': 1.3256008060125168e-06, 'learning_rate': 1.9614285714285715e-05, 'epoch': 1.82}\n",
      "{'loss': 0.0, 'grad_norm': 2.1510149963432923e-06, 'learning_rate': 1.9534920634920637e-05, 'epoch': 1.83}\n",
      "{'loss': 0.0, 'grad_norm': 9.057876582119206e-07, 'learning_rate': 1.9455555555555558e-05, 'epoch': 1.83}\n",
      "{'loss': 0.0, 'grad_norm': 1.1018184977729106e-06, 'learning_rate': 1.937619047619048e-05, 'epoch': 1.84}\n",
      "{'loss': 0.0, 'grad_norm': 8.511827900292701e-07, 'learning_rate': 1.9296825396825398e-05, 'epoch': 1.84}\n",
      "{'loss': 0.0, 'grad_norm': 4.7977764552342705e-06, 'learning_rate': 1.9217460317460316e-05, 'epoch': 1.85}\n",
      "{'loss': 0.0, 'grad_norm': 1.983038373509771e-06, 'learning_rate': 1.9138095238095237e-05, 'epoch': 1.85}\n",
      "{'loss': 0.0, 'grad_norm': 9.476380000705831e-07, 'learning_rate': 1.905873015873016e-05, 'epoch': 1.86}\n",
      "{'loss': 0.0, 'grad_norm': 1.0956135838569026e-06, 'learning_rate': 1.897936507936508e-05, 'epoch': 1.86}\n",
      "{'loss': 0.0, 'grad_norm': 1.2568451666084002e-06, 'learning_rate': 1.8900000000000002e-05, 'epoch': 1.87}\n",
      "{'loss': 0.0, 'grad_norm': 1.822551098484837e-06, 'learning_rate': 1.8820634920634924e-05, 'epoch': 1.87}\n",
      "{'loss': 0.0, 'grad_norm': 1.900753659356269e-06, 'learning_rate': 1.874126984126984e-05, 'epoch': 1.88}\n",
      "{'loss': 0.0, 'grad_norm': 1.0155946483791922e-06, 'learning_rate': 1.8661904761904763e-05, 'epoch': 1.88}\n",
      "{'loss': 0.0, 'grad_norm': 9.769428288564086e-07, 'learning_rate': 1.8582539682539685e-05, 'epoch': 1.89}\n",
      "{'loss': 0.0, 'grad_norm': 1.0695147238948266e-06, 'learning_rate': 1.8503174603174603e-05, 'epoch': 1.89}\n",
      "{'loss': 0.0, 'grad_norm': 1.0003303714256617e-06, 'learning_rate': 1.8423809523809524e-05, 'epoch': 1.9}\n",
      "{'loss': 0.0, 'grad_norm': 1.5879916190897347e-06, 'learning_rate': 1.8344444444444446e-05, 'epoch': 1.9}\n",
      "{'loss': 0.0, 'grad_norm': 6.621997954425751e-07, 'learning_rate': 1.8265079365079367e-05, 'epoch': 1.9}\n",
      "{'loss': 0.0, 'grad_norm': 1.4577826732420363e-06, 'learning_rate': 1.8185714285714285e-05, 'epoch': 1.91}\n",
      "{'loss': 0.0, 'grad_norm': 1.353046855001594e-06, 'learning_rate': 1.8106349206349207e-05, 'epoch': 1.91}\n",
      "{'loss': 0.0, 'grad_norm': 1.5256933920682059e-06, 'learning_rate': 1.802698412698413e-05, 'epoch': 1.92}\n",
      "{'loss': 0.0, 'grad_norm': 1.0196774837822886e-06, 'learning_rate': 1.794761904761905e-05, 'epoch': 1.92}\n",
      "{'loss': 0.0, 'grad_norm': 2.952820523205446e-06, 'learning_rate': 1.786825396825397e-05, 'epoch': 1.93}\n",
      "{'loss': 0.0, 'grad_norm': 7.626118190273701e-07, 'learning_rate': 1.778888888888889e-05, 'epoch': 1.93}\n",
      "{'loss': 0.0, 'grad_norm': 1.4873673990223324e-06, 'learning_rate': 1.7709523809523808e-05, 'epoch': 1.94}\n",
      "{'loss': 0.0, 'grad_norm': 2.7279597816232126e-06, 'learning_rate': 1.763015873015873e-05, 'epoch': 1.94}\n",
      "{'loss': 0.0, 'grad_norm': 9.45847261846211e-07, 'learning_rate': 1.755079365079365e-05, 'epoch': 1.95}\n",
      "{'loss': 0.0, 'grad_norm': 7.621380291311652e-07, 'learning_rate': 1.7471428571428572e-05, 'epoch': 1.95}\n",
      "{'loss': 0.0, 'grad_norm': 2.2262477159529226e-06, 'learning_rate': 1.7392063492063494e-05, 'epoch': 1.96}\n",
      "{'loss': 0.0, 'grad_norm': 5.871351731912e-06, 'learning_rate': 1.7312698412698415e-05, 'epoch': 1.96}\n",
      "{'loss': 0.0, 'grad_norm': 4.078264282725286e-06, 'learning_rate': 1.7233333333333333e-05, 'epoch': 1.97}\n",
      "{'loss': 0.0, 'grad_norm': 5.394147706283547e-07, 'learning_rate': 1.7153968253968255e-05, 'epoch': 1.97}\n",
      "{'loss': 0.0, 'grad_norm': 2.979521696033771e-06, 'learning_rate': 1.7074603174603176e-05, 'epoch': 1.98}\n",
      "{'loss': 0.0, 'grad_norm': 8.468346663903503e-07, 'learning_rate': 1.6995238095238098e-05, 'epoch': 1.98}\n",
      "{'loss': 0.0, 'grad_norm': 6.997214541115682e-07, 'learning_rate': 1.6915873015873016e-05, 'epoch': 1.99}\n",
      "{'loss': 0.0, 'grad_norm': 1.2788856338374899e-06, 'learning_rate': 1.6836507936507937e-05, 'epoch': 1.99}\n",
      "{'loss': 0.0, 'grad_norm': 3.821008704107953e-06, 'learning_rate': 1.675714285714286e-05, 'epoch': 2.0}\n",
      "{'loss': 0.0, 'grad_norm': 1.259845248569036e-06, 'learning_rate': 1.6677777777777777e-05, 'epoch': 2.0}\n",
      "{'loss': 0.0, 'grad_norm': 6.819618079134671e-07, 'learning_rate': 1.65984126984127e-05, 'epoch': 2.0}\n",
      "{'loss': 0.0, 'grad_norm': 8.013852834665158e-07, 'learning_rate': 1.651904761904762e-05, 'epoch': 2.01}\n",
      "{'loss': 0.0, 'grad_norm': 8.911832196645264e-07, 'learning_rate': 1.643968253968254e-05, 'epoch': 2.01}\n",
      "{'loss': 0.0, 'grad_norm': 1.196248831547564e-06, 'learning_rate': 1.6360317460317463e-05, 'epoch': 2.02}\n",
      "{'loss': 0.0, 'grad_norm': 7.919135782685771e-07, 'learning_rate': 1.6280952380952384e-05, 'epoch': 2.02}\n",
      "{'loss': 0.0, 'grad_norm': 9.179073572340712e-07, 'learning_rate': 1.6201587301587303e-05, 'epoch': 2.03}\n",
      "{'loss': 0.0, 'grad_norm': 8.680883070155687e-07, 'learning_rate': 1.612222222222222e-05, 'epoch': 2.03}\n",
      "{'loss': 0.0, 'grad_norm': 7.584539503113774e-07, 'learning_rate': 1.6042857142857142e-05, 'epoch': 2.04}\n",
      "{'loss': 0.0, 'grad_norm': 2.1975020558784308e-07, 'learning_rate': 1.5963492063492064e-05, 'epoch': 2.04}\n",
      "{'loss': 0.0, 'grad_norm': 3.397006764771504e-07, 'learning_rate': 1.5884126984126985e-05, 'epoch': 2.05}\n",
      "{'loss': 0.0, 'grad_norm': 6.747198426637624e-07, 'learning_rate': 1.5804761904761907e-05, 'epoch': 2.05}\n",
      "{'loss': 0.0, 'grad_norm': 8.3262722228028e-07, 'learning_rate': 1.5725396825396825e-05, 'epoch': 2.06}\n",
      "{'loss': 0.0, 'grad_norm': 7.316758683373337e-07, 'learning_rate': 1.5646031746031746e-05, 'epoch': 2.06}\n",
      "{'loss': 0.0, 'grad_norm': 2.3107345441530924e-06, 'learning_rate': 1.5566666666666668e-05, 'epoch': 2.07}\n",
      "{'loss': 0.0, 'grad_norm': 4.275892706573359e-07, 'learning_rate': 1.548730158730159e-05, 'epoch': 2.07}\n",
      "{'loss': 0.0, 'grad_norm': 2.775473149085883e-07, 'learning_rate': 1.5407936507936507e-05, 'epoch': 2.08}\n",
      "{'loss': 0.0, 'grad_norm': 6.102575866862026e-07, 'learning_rate': 1.532857142857143e-05, 'epoch': 2.08}\n",
      "{'loss': 0.0, 'grad_norm': 2.988845835716347e-06, 'learning_rate': 1.5249206349206349e-05, 'epoch': 2.09}\n",
      "{'loss': 0.0, 'grad_norm': 1.309086201217724e-06, 'learning_rate': 1.516984126984127e-05, 'epoch': 2.09}\n",
      "{'loss': 0.0, 'grad_norm': 3.423689634018956e-07, 'learning_rate': 1.5090476190476192e-05, 'epoch': 2.1}\n",
      "{'loss': 0.0, 'grad_norm': 4.610616315403604e-07, 'learning_rate': 1.5011111111111112e-05, 'epoch': 2.1}\n",
      "{'loss': 0.0, 'grad_norm': 4.314049704134959e-07, 'learning_rate': 1.4931746031746033e-05, 'epoch': 2.1}\n",
      "{'loss': 0.0, 'grad_norm': 1.172138354377239e-06, 'learning_rate': 1.4852380952380953e-05, 'epoch': 2.11}\n",
      "{'loss': 0.0, 'grad_norm': 8.585764703639143e-07, 'learning_rate': 1.4773015873015874e-05, 'epoch': 2.11}\n",
      "{'loss': 0.0, 'grad_norm': 8.098801913547504e-07, 'learning_rate': 1.4693650793650796e-05, 'epoch': 2.12}\n",
      "{'loss': 0.0, 'grad_norm': 9.97182041828637e-07, 'learning_rate': 1.4614285714285714e-05, 'epoch': 2.12}\n",
      "{'loss': 0.0, 'grad_norm': 8.003090670172242e-07, 'learning_rate': 1.4534920634920634e-05, 'epoch': 2.13}\n",
      "{'loss': 0.0, 'grad_norm': 5.636482569570944e-07, 'learning_rate': 1.4455555555555555e-05, 'epoch': 2.13}\n",
      "{'loss': 0.0, 'grad_norm': 5.086826035949343e-07, 'learning_rate': 1.4376190476190477e-05, 'epoch': 2.14}\n",
      "{'loss': 0.0, 'grad_norm': 4.3880649513994285e-07, 'learning_rate': 1.4296825396825397e-05, 'epoch': 2.14}\n",
      "{'loss': 0.0, 'grad_norm': 5.013687314203708e-07, 'learning_rate': 1.4217460317460318e-05, 'epoch': 2.15}\n",
      "{'loss': 0.0, 'grad_norm': 7.118695748431492e-07, 'learning_rate': 1.413809523809524e-05, 'epoch': 2.15}\n",
      "{'loss': 0.0, 'grad_norm': 4.337839243362396e-07, 'learning_rate': 1.405873015873016e-05, 'epoch': 2.16}\n",
      "{'loss': 0.0, 'grad_norm': 8.283607826342632e-07, 'learning_rate': 1.3979365079365081e-05, 'epoch': 2.16}\n",
      "{'loss': 0.0, 'grad_norm': 9.492887329543009e-07, 'learning_rate': 1.3900000000000002e-05, 'epoch': 2.17}\n",
      "{'loss': 0.0, 'grad_norm': 4.050582447234774e-07, 'learning_rate': 1.382063492063492e-05, 'epoch': 2.17}\n",
      "{'loss': 0.0, 'grad_norm': 8.247979508269054e-07, 'learning_rate': 1.374126984126984e-05, 'epoch': 2.18}\n",
      "{'loss': 0.0, 'grad_norm': 3.293795032277558e-07, 'learning_rate': 1.3661904761904762e-05, 'epoch': 2.18}\n",
      "{'loss': 0.0, 'grad_norm': 5.758544148193323e-07, 'learning_rate': 1.3582539682539683e-05, 'epoch': 2.19}\n",
      "{'loss': 0.0, 'grad_norm': 2.2781973996188754e-07, 'learning_rate': 1.3503174603174603e-05, 'epoch': 2.19}\n",
      "{'loss': 0.0, 'grad_norm': 3.412830437810044e-06, 'learning_rate': 1.3423809523809525e-05, 'epoch': 2.2}\n",
      "{'loss': 0.0, 'grad_norm': 4.148604602960404e-06, 'learning_rate': 1.3344444444444446e-05, 'epoch': 2.2}\n",
      "{'loss': 0.0, 'grad_norm': 5.697765459444781e-07, 'learning_rate': 1.3265079365079366e-05, 'epoch': 2.2}\n",
      "{'loss': 0.0, 'grad_norm': 5.337690822670993e-07, 'learning_rate': 1.3185714285714287e-05, 'epoch': 2.21}\n",
      "{'loss': 0.0, 'grad_norm': 6.676787620563118e-07, 'learning_rate': 1.3106349206349209e-05, 'epoch': 2.21}\n",
      "{'loss': 0.0, 'grad_norm': 5.014353519072756e-07, 'learning_rate': 1.3026984126984127e-05, 'epoch': 2.22}\n",
      "{'loss': 0.0, 'grad_norm': 2.941563479907927e-06, 'learning_rate': 1.2947619047619047e-05, 'epoch': 2.22}\n",
      "{'loss': 0.0, 'grad_norm': 1.6069802200036065e-07, 'learning_rate': 1.2868253968253968e-05, 'epoch': 2.23}\n",
      "{'loss': 0.0, 'grad_norm': 3.7154978826947627e-07, 'learning_rate': 1.278888888888889e-05, 'epoch': 2.23}\n",
      "{'loss': 0.0, 'grad_norm': 5.867762524758291e-07, 'learning_rate': 1.270952380952381e-05, 'epoch': 2.24}\n",
      "{'loss': 0.0, 'grad_norm': 4.6983527113297896e-07, 'learning_rate': 1.2630158730158731e-05, 'epoch': 2.24}\n",
      "{'loss': 0.0, 'grad_norm': 9.84538928605616e-07, 'learning_rate': 1.2550793650793651e-05, 'epoch': 2.25}\n",
      "{'loss': 0.0, 'grad_norm': 1.2006584029222722e-06, 'learning_rate': 1.2471428571428571e-05, 'epoch': 2.25}\n",
      "{'loss': 0.0, 'grad_norm': 3.256371314819262e-07, 'learning_rate': 1.2392063492063492e-05, 'epoch': 2.26}\n",
      "{'loss': 0.0, 'grad_norm': 2.9172633730922826e-07, 'learning_rate': 1.2312698412698414e-05, 'epoch': 2.26}\n",
      "{'loss': 0.0, 'grad_norm': 1.002374574454734e-06, 'learning_rate': 1.2233333333333334e-05, 'epoch': 2.27}\n",
      "{'loss': 0.0, 'grad_norm': 8.487571108162228e-07, 'learning_rate': 1.2153968253968255e-05, 'epoch': 2.27}\n",
      "{'loss': 0.0, 'grad_norm': 2.451253919844021e-07, 'learning_rate': 1.2074603174603175e-05, 'epoch': 2.28}\n",
      "{'loss': 0.0, 'grad_norm': 3.305653990537394e-07, 'learning_rate': 1.1995238095238095e-05, 'epoch': 2.28}\n",
      "{'loss': 0.0001, 'grad_norm': 2.7549224341782974e-06, 'learning_rate': 1.1917460317460318e-05, 'epoch': 2.29}\n",
      "{'loss': 0.0, 'grad_norm': 3.887297737037443e-07, 'learning_rate': 1.1838095238095238e-05, 'epoch': 2.29}\n",
      "{'loss': 0.0, 'grad_norm': 1.3009420740672795e-07, 'learning_rate': 1.175873015873016e-05, 'epoch': 2.3}\n",
      "{'loss': 0.0, 'grad_norm': 3.3378501029801555e-07, 'learning_rate': 1.1679365079365081e-05, 'epoch': 2.3}\n",
      "{'loss': 0.0, 'grad_norm': 2.5300533934569103e-07, 'learning_rate': 1.16e-05, 'epoch': 2.3}\n",
      "{'loss': 0.0, 'grad_norm': 3.276526854278927e-07, 'learning_rate': 1.152063492063492e-05, 'epoch': 2.31}\n",
      "{'loss': 0.0013, 'grad_norm': 8.090877372524119e-07, 'learning_rate': 1.1442857142857144e-05, 'epoch': 2.31}\n",
      "{'loss': 0.0015, 'grad_norm': 0.00011331342102494091, 'learning_rate': 1.1363492063492064e-05, 'epoch': 2.32}\n",
      "{'loss': 0.0, 'grad_norm': 1.3834310266247485e-05, 'learning_rate': 1.1284126984126985e-05, 'epoch': 2.32}\n",
      "{'loss': 0.0, 'grad_norm': 1.8063317838823423e-05, 'learning_rate': 1.1204761904761905e-05, 'epoch': 2.33}\n",
      "{'loss': 0.0, 'grad_norm': 1.679585147940088e-05, 'learning_rate': 1.1125396825396827e-05, 'epoch': 2.33}\n",
      "{'loss': 0.0, 'grad_norm': 1.2147415873187128e-05, 'learning_rate': 1.1046031746031748e-05, 'epoch': 2.34}\n",
      "{'loss': 0.0, 'grad_norm': 1.4244459634937812e-05, 'learning_rate': 1.0966666666666666e-05, 'epoch': 2.34}\n",
      "{'loss': 0.0, 'grad_norm': 1.1894526323885657e-05, 'learning_rate': 1.0887301587301588e-05, 'epoch': 2.35}\n",
      "{'loss': 0.0, 'grad_norm': 1.1200465451111086e-05, 'learning_rate': 1.0807936507936509e-05, 'epoch': 2.35}\n",
      "{'loss': 0.0, 'grad_norm': 5.987004897178849e-06, 'learning_rate': 1.0728571428571429e-05, 'epoch': 2.36}\n",
      "{'loss': 0.0, 'grad_norm': 3.3921039630513405e-06, 'learning_rate': 1.064920634920635e-05, 'epoch': 2.36}\n",
      "{'loss': 0.0, 'grad_norm': 3.008709654750419e-06, 'learning_rate': 1.056984126984127e-05, 'epoch': 2.37}\n",
      "{'loss': 0.0, 'grad_norm': 4.21474760514684e-06, 'learning_rate': 1.049047619047619e-05, 'epoch': 2.37}\n",
      "{'loss': 0.0, 'grad_norm': 2.9228401672298787e-06, 'learning_rate': 1.0411111111111112e-05, 'epoch': 2.38}\n",
      "{'loss': 0.0, 'grad_norm': 6.09079870628193e-06, 'learning_rate': 1.0331746031746033e-05, 'epoch': 2.38}\n",
      "{'loss': 0.0, 'grad_norm': 3.862468929582974e-06, 'learning_rate': 1.0252380952380953e-05, 'epoch': 2.39}\n",
      "{'loss': 0.0, 'grad_norm': 2.075232941933791e-06, 'learning_rate': 1.0173015873015873e-05, 'epoch': 2.39}\n",
      "{'loss': 0.0, 'grad_norm': 3.3069320579670602e-06, 'learning_rate': 1.0093650793650794e-05, 'epoch': 2.4}\n",
      "{'loss': 0.0, 'grad_norm': 1.6933729511947604e-06, 'learning_rate': 1.0014285714285716e-05, 'epoch': 2.4}\n",
      "{'loss': 0.0, 'grad_norm': 1.2365518387014163e-06, 'learning_rate': 9.934920634920636e-06, 'epoch': 2.4}\n",
      "{'loss': 0.0, 'grad_norm': 4.403609636938199e-06, 'learning_rate': 9.855555555555555e-06, 'epoch': 2.41}\n",
      "{'loss': 0.0, 'grad_norm': 1.1735364751075394e-06, 'learning_rate': 9.776190476190477e-06, 'epoch': 2.41}\n",
      "{'loss': 0.0, 'grad_norm': 1.7434938399674138e-06, 'learning_rate': 9.696825396825397e-06, 'epoch': 2.42}\n",
      "{'loss': 0.0, 'grad_norm': 1.143638655776158e-06, 'learning_rate': 9.617460317460318e-06, 'epoch': 2.42}\n",
      "{'loss': 0.0, 'grad_norm': 2.899067794714938e-06, 'learning_rate': 9.53809523809524e-06, 'epoch': 2.43}\n",
      "{'loss': 0.0, 'grad_norm': 3.675135758385295e-06, 'learning_rate': 9.458730158730158e-06, 'epoch': 2.43}\n",
      "{'loss': 0.0, 'grad_norm': 1.5005599607320619e-06, 'learning_rate': 9.37936507936508e-06, 'epoch': 2.44}\n",
      "{'loss': 0.0, 'grad_norm': 1.2136162013121066e-06, 'learning_rate': 9.3e-06, 'epoch': 2.44}\n",
      "{'loss': 0.0, 'grad_norm': 2.308351440660772e-06, 'learning_rate': 9.22063492063492e-06, 'epoch': 2.45}\n",
      "{'loss': 0.0, 'grad_norm': 9.373987950311857e-07, 'learning_rate': 9.141269841269842e-06, 'epoch': 2.45}\n",
      "{'loss': 0.0, 'grad_norm': 2.9970647119625937e-06, 'learning_rate': 9.061904761904762e-06, 'epoch': 2.46}\n",
      "{'loss': 0.0, 'grad_norm': 1.4152154790281202e-06, 'learning_rate': 8.982539682539683e-06, 'epoch': 2.46}\n",
      "{'loss': 0.0, 'grad_norm': 2.2053088741813553e-06, 'learning_rate': 8.903174603174603e-06, 'epoch': 2.47}\n",
      "{'loss': 0.0, 'grad_norm': 1.397817641191068e-06, 'learning_rate': 8.823809523809525e-06, 'epoch': 2.47}\n",
      "{'loss': 0.0, 'grad_norm': 1.54455563006195e-06, 'learning_rate': 8.744444444444446e-06, 'epoch': 2.48}\n",
      "{'loss': 0.0, 'grad_norm': 1.1644513051578542e-06, 'learning_rate': 8.665079365079364e-06, 'epoch': 2.48}\n",
      "{'loss': 0.0, 'grad_norm': 4.156255215548299e-07, 'learning_rate': 8.585714285714286e-06, 'epoch': 2.49}\n",
      "{'loss': 0.0, 'grad_norm': 1.7664960978436284e-06, 'learning_rate': 8.506349206349207e-06, 'epoch': 2.49}\n",
      "{'loss': 0.0, 'grad_norm': 4.4783266162085056e-07, 'learning_rate': 8.426984126984127e-06, 'epoch': 2.5}\n",
      "{'loss': 0.0, 'grad_norm': 6.088152417760284e-07, 'learning_rate': 8.347619047619049e-06, 'epoch': 2.5}\n",
      "{'loss': 0.0, 'grad_norm': 1.5774978692206787e-06, 'learning_rate': 8.268253968253968e-06, 'epoch': 2.5}\n",
      "{'loss': 0.0, 'grad_norm': 2.6045538561447756e-06, 'learning_rate': 8.188888888888888e-06, 'epoch': 2.51}\n",
      "{'loss': 0.0, 'grad_norm': 1.2181528745713877e-06, 'learning_rate': 8.10952380952381e-06, 'epoch': 2.51}\n",
      "{'loss': 0.0, 'grad_norm': 6.127775691311399e-07, 'learning_rate': 8.030158730158731e-06, 'epoch': 2.52}\n",
      "{'loss': 0.0, 'grad_norm': 1.067428115675284e-06, 'learning_rate': 7.950793650793651e-06, 'epoch': 2.52}\n",
      "{'loss': 0.0, 'grad_norm': 1.0898439768425305e-06, 'learning_rate': 7.87142857142857e-06, 'epoch': 2.53}\n",
      "{'loss': 0.0021, 'grad_norm': 1.9022668311663438e-06, 'learning_rate': 7.793650793650794e-06, 'epoch': 2.53}\n",
      "{'loss': 0.0, 'grad_norm': 1.0264700449624797e-06, 'learning_rate': 7.714285714285714e-06, 'epoch': 2.54}\n",
      "{'loss': 0.0, 'grad_norm': 6.544530037899676e-07, 'learning_rate': 7.634920634920635e-06, 'epoch': 2.54}\n",
      "{'loss': 0.0, 'grad_norm': 7.818251219759986e-07, 'learning_rate': 7.555555555555556e-06, 'epoch': 2.55}\n",
      "{'loss': 0.0, 'grad_norm': 4.491387244343059e-07, 'learning_rate': 7.476190476190477e-06, 'epoch': 2.55}\n",
      "{'loss': 0.0, 'grad_norm': 1.285031316911045e-06, 'learning_rate': 7.3968253968253975e-06, 'epoch': 2.56}\n",
      "{'loss': 0.0, 'grad_norm': 2.8445236921470496e-07, 'learning_rate': 7.317460317460317e-06, 'epoch': 2.56}\n",
      "{'loss': 0.0, 'grad_norm': 8.688148227520287e-07, 'learning_rate': 7.238095238095238e-06, 'epoch': 2.57}\n",
      "{'loss': 0.0, 'grad_norm': 1.4522811397910118e-06, 'learning_rate': 7.1587301587301594e-06, 'epoch': 2.57}\n",
      "{'loss': 0.0, 'grad_norm': 2.7350820346327964e-06, 'learning_rate': 7.07936507936508e-06, 'epoch': 2.58}\n",
      "{'loss': 0.0, 'grad_norm': 6.866212629574875e-07, 'learning_rate': 7.000000000000001e-06, 'epoch': 2.58}\n",
      "{'loss': 0.0, 'grad_norm': 1.085597432393115e-06, 'learning_rate': 6.9206349206349206e-06, 'epoch': 2.59}\n",
      "{'loss': 0.0, 'grad_norm': 1.1038824823117466e-06, 'learning_rate': 6.841269841269841e-06, 'epoch': 2.59}\n",
      "{'loss': 0.0, 'grad_norm': 2.7151992298968253e-07, 'learning_rate': 6.761904761904763e-06, 'epoch': 2.6}\n",
      "{'loss': 0.0, 'grad_norm': 4.73015518309694e-07, 'learning_rate': 6.682539682539683e-06, 'epoch': 2.6}\n",
      "{'loss': 0.0, 'grad_norm': 1.1871469496327336e-06, 'learning_rate': 6.603174603174604e-06, 'epoch': 2.6}\n",
      "{'loss': 0.0, 'grad_norm': 3.290646759523952e-07, 'learning_rate': 6.523809523809524e-06, 'epoch': 2.61}\n",
      "{'loss': 0.0, 'grad_norm': 3.245538096052769e-07, 'learning_rate': 6.4444444444444445e-06, 'epoch': 2.61}\n",
      "{'loss': 0.0, 'grad_norm': 1.5815065808055806e-06, 'learning_rate': 6.365079365079365e-06, 'epoch': 2.62}\n",
      "{'loss': 0.0, 'grad_norm': 5.359063379728468e-07, 'learning_rate': 6.285714285714287e-06, 'epoch': 2.62}\n",
      "{'loss': 0.0, 'grad_norm': 1.4863564956613118e-06, 'learning_rate': 6.2063492063492064e-06, 'epoch': 2.63}\n",
      "{'loss': 0.0, 'grad_norm': 1.5255595826602075e-06, 'learning_rate': 6.126984126984128e-06, 'epoch': 2.63}\n",
      "{'loss': 0.0, 'grad_norm': 9.977661648008507e-07, 'learning_rate': 6.047619047619048e-06, 'epoch': 2.64}\n",
      "{'loss': 0.0, 'grad_norm': 8.303056233671668e-07, 'learning_rate': 5.968253968253968e-06, 'epoch': 2.64}\n",
      "{'loss': 0.0, 'grad_norm': 1.4523018307954771e-06, 'learning_rate': 5.888888888888889e-06, 'epoch': 2.65}\n",
      "{'loss': 0.0, 'grad_norm': 9.039739552463288e-07, 'learning_rate': 5.80952380952381e-06, 'epoch': 2.65}\n",
      "{'loss': 0.0, 'grad_norm': 2.5178205760312267e-06, 'learning_rate': 5.73015873015873e-06, 'epoch': 2.66}\n",
      "{'loss': 0.0, 'grad_norm': 7.219567237370939e-07, 'learning_rate': 5.650793650793651e-06, 'epoch': 2.66}\n",
      "{'loss': 0.0, 'grad_norm': 3.349832979893108e-07, 'learning_rate': 5.571428571428572e-06, 'epoch': 2.67}\n",
      "{'loss': 0.0, 'grad_norm': 3.6372773593029706e-06, 'learning_rate': 5.492063492063492e-06, 'epoch': 2.67}\n",
      "{'loss': 0.0, 'grad_norm': 1.0620096873026341e-06, 'learning_rate': 5.412698412698413e-06, 'epoch': 2.68}\n",
      "{'loss': 0.0, 'grad_norm': 1.863117233824596e-07, 'learning_rate': 5.333333333333334e-06, 'epoch': 2.68}\n",
      "{'loss': 0.0, 'grad_norm': 6.171687232381373e-07, 'learning_rate': 5.253968253968254e-06, 'epoch': 2.69}\n",
      "{'loss': 0.0, 'grad_norm': 2.953820228412951e-07, 'learning_rate': 5.174603174603175e-06, 'epoch': 2.69}\n",
      "{'loss': 0.0, 'grad_norm': 2.2151921257318463e-06, 'learning_rate': 5.095238095238096e-06, 'epoch': 2.7}\n",
      "{'loss': 0.0, 'grad_norm': 6.019564011694456e-07, 'learning_rate': 5.015873015873016e-06, 'epoch': 2.7}\n",
      "{'loss': 0.0, 'grad_norm': 3.5852679047820857e-07, 'learning_rate': 4.936507936507936e-06, 'epoch': 2.7}\n",
      "{'loss': 0.0, 'grad_norm': 7.530215384576877e-07, 'learning_rate': 4.857142857142858e-06, 'epoch': 2.71}\n",
      "{'loss': 0.0005, 'grad_norm': 2.9989323024892656e-07, 'learning_rate': 4.777777777777778e-06, 'epoch': 2.71}\n",
      "{'loss': 0.0, 'grad_norm': 1.0682506399462e-06, 'learning_rate': 4.698412698412698e-06, 'epoch': 2.72}\n",
      "{'loss': 0.0, 'grad_norm': 2.503422592781135e-06, 'learning_rate': 4.6190476190476196e-06, 'epoch': 2.72}\n",
      "{'loss': 0.0, 'grad_norm': 8.822771633276716e-07, 'learning_rate': 4.539682539682539e-06, 'epoch': 2.73}\n",
      "{'loss': 0.0, 'grad_norm': 2.987539176046994e-07, 'learning_rate': 4.460317460317461e-06, 'epoch': 2.73}\n",
      "{'loss': 0.0, 'grad_norm': 0.00017897074576467276, 'learning_rate': 4.3809523809523815e-06, 'epoch': 2.74}\n",
      "{'loss': 0.0, 'grad_norm': 1.4649345985162654e-06, 'learning_rate': 4.301587301587301e-06, 'epoch': 2.74}\n",
      "{'loss': 0.0, 'grad_norm': 9.067540531759732e-07, 'learning_rate': 4.222222222222223e-06, 'epoch': 2.75}\n",
      "{'loss': 0.0, 'grad_norm': 8.372885673679775e-08, 'learning_rate': 4.142857142857143e-06, 'epoch': 2.75}\n",
      "{'loss': 0.0029, 'grad_norm': 2.8305100840952946e-07, 'learning_rate': 4.065079365079365e-06, 'epoch': 2.76}\n",
      "{'loss': 0.0, 'grad_norm': 6.401700716196501e-07, 'learning_rate': 3.985714285714286e-06, 'epoch': 2.76}\n",
      "{'loss': 0.0, 'grad_norm': 4.7186210849758936e-07, 'learning_rate': 3.9063492063492064e-06, 'epoch': 2.77}\n",
      "{'loss': 0.0, 'grad_norm': 9.254432598027051e-07, 'learning_rate': 3.826984126984127e-06, 'epoch': 2.77}\n",
      "{'loss': 0.0, 'grad_norm': 9.377928336107288e-07, 'learning_rate': 3.7476190476190478e-06, 'epoch': 2.78}\n",
      "{'loss': 0.0, 'grad_norm': 7.666534429517924e-07, 'learning_rate': 3.668253968253969e-06, 'epoch': 2.78}\n",
      "{'loss': 0.0, 'grad_norm': 1.8935975276690442e-07, 'learning_rate': 3.588888888888889e-06, 'epoch': 2.79}\n",
      "{'loss': 0.0, 'grad_norm': 2.0744415451190434e-05, 'learning_rate': 3.5095238095238097e-06, 'epoch': 2.79}\n",
      "{'loss': 0.0018, 'grad_norm': 1.1845695553347468e-06, 'learning_rate': 3.431746031746032e-06, 'epoch': 2.8}\n",
      "{'loss': 0.0, 'grad_norm': 5.742533630836988e-07, 'learning_rate': 3.352380952380953e-06, 'epoch': 2.8}\n",
      "{'loss': 0.0, 'grad_norm': 2.9151919989089947e-07, 'learning_rate': 3.273015873015873e-06, 'epoch': 2.8}\n",
      "{'loss': 0.0, 'grad_norm': 3.9260774542526633e-07, 'learning_rate': 3.1936507936507938e-06, 'epoch': 2.81}\n",
      "{'loss': 0.0, 'grad_norm': 1.4901162614933128e-07, 'learning_rate': 3.1142857142857144e-06, 'epoch': 2.81}\n",
      "{'loss': 0.0028, 'grad_norm': 4.899469786323607e-06, 'learning_rate': 3.036507936507937e-06, 'epoch': 2.82}\n",
      "{'loss': 0.0, 'grad_norm': 3.534810275596101e-07, 'learning_rate': 2.957142857142857e-06, 'epoch': 2.82}\n",
      "{'loss': 0.0, 'grad_norm': 3.518609332786582e-07, 'learning_rate': 2.877777777777778e-06, 'epoch': 2.83}\n",
      "{'loss': 0.0, 'grad_norm': 1.1652207376755541e-06, 'learning_rate': 2.7984126984126985e-06, 'epoch': 2.83}\n",
      "{'loss': 0.0, 'grad_norm': 1.1132642612210475e-06, 'learning_rate': 2.7190476190476195e-06, 'epoch': 2.84}\n",
      "{'loss': 0.0, 'grad_norm': 2.3707836760422651e-07, 'learning_rate': 2.6396825396825398e-06, 'epoch': 2.84}\n",
      "{'loss': 0.0, 'grad_norm': 1.1087703342127497e-06, 'learning_rate': 2.5603174603174604e-06, 'epoch': 2.85}\n",
      "{'loss': 0.0, 'grad_norm': 9.781369953998365e-07, 'learning_rate': 2.480952380952381e-06, 'epoch': 2.85}\n",
      "{'loss': 0.0, 'grad_norm': 3.4984432772944274e-07, 'learning_rate': 2.4015873015873017e-06, 'epoch': 2.86}\n",
      "{'loss': 0.0, 'grad_norm': 4.978152787771251e-07, 'learning_rate': 2.322222222222222e-06, 'epoch': 2.86}\n",
      "{'loss': 0.0, 'grad_norm': 6.765646389794711e-07, 'learning_rate': 2.242857142857143e-06, 'epoch': 2.87}\n",
      "{'loss': 0.0, 'grad_norm': 4.7325821128652024e-07, 'learning_rate': 2.1634920634920637e-06, 'epoch': 2.87}\n",
      "{'loss': 0.0, 'grad_norm': 1.3982312339066993e-06, 'learning_rate': 2.0841269841269844e-06, 'epoch': 2.88}\n",
      "{'loss': 0.0, 'grad_norm': 2.4152348032657756e-07, 'learning_rate': 2.0047619047619046e-06, 'epoch': 2.88}\n",
      "{'loss': 0.0, 'grad_norm': 7.29170324120787e-07, 'learning_rate': 1.9253968253968252e-06, 'epoch': 2.89}\n",
      "{'loss': 0.0, 'grad_norm': 1.1955389709328301e-06, 'learning_rate': 1.8460317460317463e-06, 'epoch': 2.89}\n",
      "{'loss': 0.0, 'grad_norm': 2.125861726653966e-07, 'learning_rate': 1.7666666666666668e-06, 'epoch': 2.9}\n",
      "{'loss': 0.0, 'grad_norm': 8.974704996944638e-07, 'learning_rate': 1.6873015873015874e-06, 'epoch': 2.9}\n",
      "{'loss': 0.0, 'grad_norm': 1.883529762380931e-06, 'learning_rate': 1.6079365079365079e-06, 'epoch': 2.9}\n",
      "{'loss': 0.0, 'grad_norm': 8.828184832054831e-07, 'learning_rate': 1.5285714285714287e-06, 'epoch': 2.91}\n",
      "{'loss': 0.0, 'grad_norm': 2.8539699314933387e-07, 'learning_rate': 1.4492063492063492e-06, 'epoch': 2.91}\n",
      "{'loss': 0.0, 'grad_norm': 1.8422170455778542e-07, 'learning_rate': 1.3698412698412698e-06, 'epoch': 2.92}\n",
      "{'loss': 0.0, 'grad_norm': 7.962028121255571e-07, 'learning_rate': 1.2904761904761905e-06, 'epoch': 2.92}\n",
      "{'loss': 0.0, 'grad_norm': 5.375288196773909e-07, 'learning_rate': 1.2111111111111111e-06, 'epoch': 2.93}\n",
      "{'loss': 0.0, 'grad_norm': 5.825987159369106e-07, 'learning_rate': 1.1317460317460318e-06, 'epoch': 2.93}\n",
      "{'loss': 0.0, 'grad_norm': 2.944443906471861e-07, 'learning_rate': 1.0523809523809524e-06, 'epoch': 2.94}\n",
      "{'loss': 0.0, 'grad_norm': 6.988869358792726e-07, 'learning_rate': 9.73015873015873e-07, 'epoch': 2.94}\n",
      "{'loss': 0.0, 'grad_norm': 1.7836536869708652e-07, 'learning_rate': 8.936507936507938e-07, 'epoch': 2.95}\n",
      "{'loss': 0.0, 'grad_norm': 1.1320461226205225e-06, 'learning_rate': 8.142857142857143e-07, 'epoch': 2.95}\n",
      "{'loss': 0.0, 'grad_norm': 2.138198851753259e-06, 'learning_rate': 7.34920634920635e-07, 'epoch': 2.96}\n",
      "{'loss': 0.0, 'grad_norm': 7.624463478350663e-07, 'learning_rate': 6.555555555555556e-07, 'epoch': 2.96}\n",
      "{'loss': 0.0, 'grad_norm': 1.3375489515965455e-06, 'learning_rate': 5.761904761904762e-07, 'epoch': 2.97}\n",
      "{'loss': 0.0, 'grad_norm': 6.011750883772038e-07, 'learning_rate': 4.968253968253968e-07, 'epoch': 2.97}\n",
      "{'loss': 0.0, 'grad_norm': 3.294027806077793e-07, 'learning_rate': 4.174603174603175e-07, 'epoch': 2.98}\n",
      "{'loss': 0.0, 'grad_norm': 1.7524260442769446e-07, 'learning_rate': 3.380952380952381e-07, 'epoch': 2.98}\n",
      "{'loss': 0.0, 'grad_norm': 9.803517286854913e-07, 'learning_rate': 2.5873015873015874e-07, 'epoch': 2.99}\n",
      "{'loss': 0.0, 'grad_norm': 3.9814290175854694e-07, 'learning_rate': 1.7936507936507937e-07, 'epoch': 2.99}\n",
      "{'loss': 0.0, 'grad_norm': 7.665664725209353e-07, 'learning_rate': 1.0000000000000001e-07, 'epoch': 3.0}\n",
      "{'loss': 0.0, 'grad_norm': 6.371475365085644e-07, 'learning_rate': 2.0634920634920634e-08, 'epoch': 3.0}\n",
      "{'train_runtime': 2247.0538, 'train_samples_per_second': 56.073, 'train_steps_per_second': 14.018, 'train_loss': 0.008639363925872338, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=31500, training_loss=0.008639363925872338, metrics={'train_runtime': 2247.0538, 'train_samples_per_second': 56.073, 'train_steps_per_second': 14.018, 'total_flos': 4801674608640000.0, 'train_loss': 0.008639363925872338, 'epoch': 3.0})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\").to(\"cuda\")\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bart-emotion-gen\",\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=50,\n",
    "    save_total_limit=1,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    save_strategy=\"no\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## G√©n√©ration & √âvaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt     : emotion: admiration | message: Je suis impressionn√© par la qualit√© de votre service.\n",
      "R√©f√©rence  : Merci pour votre message concernant admiration. Nous allons vous r√©pondre au mieux.\n",
      "G√©n√©r√©     : Merci pour votre message concernant admiration. Nous allons vous r√©pondre au mieux.\n",
      "BLEU       : 1.0\n",
      "ROUGE-1    : 1.0\n",
      "ROUGE-2    : 1.0\n",
      "ROUGE-L    : 1.0\n",
      "Perplexity : 1.5847\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "import math\n",
    "\n",
    "# Exemple de test\n",
    "sample = df.iloc[0]\n",
    "prompt = \"emotion: \" + sample[\"emotion\"] + \" | message: \" + sample[\"text_input\"]\n",
    "reference = sample[\"target_text\"]\n",
    "\n",
    "# Encode + generate\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "output_ids = model.generate(**inputs, max_new_tokens=50)\n",
    "generated = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# √âvaluation\n",
    "smoothie = SmoothingFunction().method4\n",
    "scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
    "\n",
    "bleu = sentence_bleu([reference.split()], generated.split(), smoothing_function=smoothie)\n",
    "rouge = scorer.score(reference, generated)\n",
    "\n",
    "# Perplexity\n",
    "enc = tokenizer(generated, return_tensors=\"pt\").to(\"cuda\")\n",
    "with torch.no_grad():\n",
    "    out = model(**enc, labels=enc[\"input_ids\"])\n",
    "perplexity = math.exp(out.loss.item()) if out.loss.item() < 100 else float(\"inf\")\n",
    "\n",
    "# Affichage\n",
    "print(f\"Prompt     : {prompt}\")\n",
    "print(f\"R√©f√©rence  : {reference}\")\n",
    "print(f\"G√©n√©r√©     : {generated}\")\n",
    "print(f\"BLEU       : {round(bleu, 4)}\")\n",
    "print(f\"ROUGE-1    : {round(rouge['rouge1'].fmeasure, 4)}\")\n",
    "print(f\"ROUGE-2    : {round(rouge['rouge2'].fmeasure, 4)}\")\n",
    "print(f\"ROUGE-L    : {round(rouge['rougeL'].fmeasure, 4)}\")\n",
    "print(f\"Perplexity : {round(perplexity, 4)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf_gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
