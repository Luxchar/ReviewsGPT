{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fb82a26",
   "metadata": {},
   "source": [
    "# ğŸ•µï¸ Advanced Marketplace Scraper - Anti-Detection System\n",
    "\n",
    "Ce notebook implÃ©mente un scraper Selenium avancÃ© avec systÃ¨me anti-dÃ©tection pour scraper des marketplaces anglaises :\n",
    "\n",
    "## ğŸ›¡ï¸ Protections Anti-Ban :\n",
    "- **Fake User Agents** : Rotation automatique d'user agents rÃ©alistes\n",
    "- **Proxy Rotation** : Rotation d'IP pour Ã©viter la dÃ©tection\n",
    "- **Comportement Humain** : Mouvements de souris, scrolling, pauses naturelles\n",
    "- **DÃ©lais AlÃ©atoires** : Timing humain entre les actions\n",
    "- **Headers RÃ©alistes** : Simulation complÃ¨te de navigateur rÃ©el\n",
    "\n",
    "## ğŸ¯ Sites CiblÃ©s :\n",
    "- **E-commerce** : Amazon-like sites, eBay, etc.\n",
    "- **Reviews** : Trustpilot, Google Reviews, Yelp\n",
    "- **Product Data** : Prix, descriptions, reviews avec dates\n",
    "- **APIs publiques** quand disponibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b4054a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (4.11.2)\n",
      "Requirement already satisfied: webdriver-manager in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (4.0.2)\n",
      "Requirement already satisfied: fake-useragent in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (2.2.0)\n",
      "Requirement already satisfied: requests in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (2.28.2)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (4.13.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (2.3.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.26.15)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from selenium) (0.30.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from selenium) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from selenium) (2025.6.15)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium) (25.3.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium) (2.8)\n",
      "Requirement already satisfied: outcome in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium) (1.17.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from webdriver-manager) (1.1.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from webdriver-manager) (25.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from requests) (3.4.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from beautifulsoup4) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from beautifulsoup4) (4.13.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from pandas) (2.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: pycparser in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.22)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: undetected-chromedriver in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (3.5.3)\n",
      "Requirement already satisfied: selenium-stealth in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (1.0.6)\n",
      "Requirement already satisfied: pyautogui in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (0.9.54)\n",
      "Requirement already satisfied: selenium>=4.9.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from undetected-chromedriver) (4.11.2)\n",
      "Requirement already satisfied: requests in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from undetected-chromedriver) (2.28.2)\n",
      "Requirement already satisfied: websockets in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from undetected-chromedriver) (15.0.1)\n",
      "Requirement already satisfied: pymsgbox in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from pyautogui) (1.0.9)\n",
      "Requirement already satisfied: pytweening>=1.0.4 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from pyautogui) (1.2.0)\n",
      "Requirement already satisfied: pyscreeze>=0.1.21 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from pyautogui) (1.0.1)\n",
      "Requirement already satisfied: pygetwindow>=0.0.5 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from pyautogui) (0.0.9)\n",
      "Requirement already satisfied: mouseinfo in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from pyautogui) (0.1.3)\n",
      "Requirement already satisfied: pyrect in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from pygetwindow>=0.0.5->pyautogui) (0.2.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium>=4.9.0->undetected-chromedriver) (1.26.15)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver) (0.30.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver) (2025.6.15)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (25.3.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (2.8)\n",
      "Requirement already satisfied: outcome in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (1.17.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio-websocket~=0.9->selenium>=4.9.0->undetected-chromedriver) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium>=4.9.0->undetected-chromedriver) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (2.22)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium>=4.9.0->undetected-chromedriver) (0.16.0)\n",
      "Requirement already satisfied: pyperclip in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from mouseinfo->pyautogui) (1.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from requests->undetected-chromedriver) (3.4.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: undetected-chromedriver in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (3.5.3)\n",
      "Requirement already satisfied: selenium-stealth in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (1.0.6)\n",
      "Requirement already satisfied: pyautogui in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (0.9.54)\n",
      "Requirement already satisfied: selenium>=4.9.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from undetected-chromedriver) (4.11.2)\n",
      "Requirement already satisfied: requests in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from undetected-chromedriver) (2.28.2)\n",
      "Requirement already satisfied: websockets in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from undetected-chromedriver) (15.0.1)\n",
      "Requirement already satisfied: pymsgbox in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from pyautogui) (1.0.9)\n",
      "Requirement already satisfied: pytweening>=1.0.4 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from pyautogui) (1.2.0)\n",
      "Requirement already satisfied: pyscreeze>=0.1.21 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from pyautogui) (1.0.1)\n",
      "Requirement already satisfied: pygetwindow>=0.0.5 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from pyautogui) (0.0.9)\n",
      "Requirement already satisfied: mouseinfo in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from pyautogui) (0.1.3)\n",
      "Requirement already satisfied: pyrect in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from pygetwindow>=0.0.5->pyautogui) (0.2.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium>=4.9.0->undetected-chromedriver) (1.26.15)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver) (0.30.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver) (2025.6.15)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (25.3.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (2.8)\n",
      "Requirement already satisfied: outcome in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (1.17.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio-websocket~=0.9->selenium>=4.9.0->undetected-chromedriver) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium>=4.9.0->undetected-chromedriver) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (2.22)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium>=4.9.0->undetected-chromedriver) (0.16.0)\n",
      "Requirement already satisfied: pyperclip in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from mouseinfo->pyautogui) (1.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from requests->undetected-chromedriver) (3.4.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: requests-proxy-adapter in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (0.1.1)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (2.9.0.post0)\n",
      "Requirement already satisfied: lxml in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (6.0.0)\n",
      "Requirement already satisfied: pluggy==0.11.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from requests-proxy-adapter) (0.11.0)\n",
      "Collecting requests==2.22.0 (from requests-proxy-adapter)\n",
      "  Downloading requests-2.22.0-py2.py3-none-any.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: urllib3>=1.15 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from requests-proxy-adapter) (1.26.15)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from requests==2.22.0->requests-proxy-adapter) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from requests==2.22.0->requests-proxy-adapter) (2.8)\n",
      "Collecting urllib3>=1.15 (from requests-proxy-adapter)\n",
      "  Downloading urllib3-1.25.11-py2.py3-none-any.whl.metadata (41 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from requests==2.22.0->requests-proxy-adapter) (2025.6.15)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from python-dateutil) (1.17.0)\n",
      "Downloading requests-2.22.0-py2.py3-none-any.whl (57 kB)\n",
      "Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
      "Installing collected packages: urllib3, requests\n",
      "\n",
      "  Attempting uninstall: urllib3\n",
      "\n",
      "    Found existing installation: urllib3 1.26.15\n",
      "\n",
      "    Uninstalling urllib3-1.26.15:\n",
      "\n",
      "      Successfully uninstalled urllib3-1.26.15\n",
      "\n",
      "   ---------------------------------------- 0/2 [urllib3]\n",
      "  Attempting uninstall: requests\n",
      "   ---------------------------------------- 0/2 [urllib3]\n",
      "    Found existing installation: requests 2.28.2\n",
      "   ---------------------------------------- 0/2 [urllib3]\n",
      "    Uninstalling requests-2.28.2:\n",
      "   ---------------------------------------- 0/2 [urllib3]\n",
      "      Successfully uninstalled requests-2.28.2\n",
      "   ---------------------------------------- 0/2 [urllib3]\n",
      "   -------------------- ------------------- 1/2 [requests]\n",
      "   ---------------------------------------- 2/2 [requests]\n",
      "\n",
      "Successfully installed requests-2.22.0 urllib3-1.25.11\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "ğŸ“¦ Installation terminÃ©e - Scraper anti-dÃ©tection prÃªt !\n",
      "Requirement already satisfied: requests-proxy-adapter in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (0.1.1)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (2.9.0.post0)\n",
      "Requirement already satisfied: lxml in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (6.0.0)\n",
      "Requirement already satisfied: pluggy==0.11.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from requests-proxy-adapter) (0.11.0)\n",
      "Collecting requests==2.22.0 (from requests-proxy-adapter)\n",
      "  Downloading requests-2.22.0-py2.py3-none-any.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: urllib3>=1.15 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from requests-proxy-adapter) (1.26.15)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from requests==2.22.0->requests-proxy-adapter) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from requests==2.22.0->requests-proxy-adapter) (2.8)\n",
      "Collecting urllib3>=1.15 (from requests-proxy-adapter)\n",
      "  Downloading urllib3-1.25.11-py2.py3-none-any.whl.metadata (41 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from requests==2.22.0->requests-proxy-adapter) (2025.6.15)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from python-dateutil) (1.17.0)\n",
      "Downloading requests-2.22.0-py2.py3-none-any.whl (57 kB)\n",
      "Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
      "Installing collected packages: urllib3, requests\n",
      "\n",
      "  Attempting uninstall: urllib3\n",
      "\n",
      "    Found existing installation: urllib3 1.26.15\n",
      "\n",
      "    Uninstalling urllib3-1.26.15:\n",
      "\n",
      "      Successfully uninstalled urllib3-1.26.15\n",
      "\n",
      "   ---------------------------------------- 0/2 [urllib3]\n",
      "  Attempting uninstall: requests\n",
      "   ---------------------------------------- 0/2 [urllib3]\n",
      "    Found existing installation: requests 2.28.2\n",
      "   ---------------------------------------- 0/2 [urllib3]\n",
      "    Uninstalling requests-2.28.2:\n",
      "   ---------------------------------------- 0/2 [urllib3]\n",
      "      Successfully uninstalled requests-2.28.2\n",
      "   ---------------------------------------- 0/2 [urllib3]\n",
      "   -------------------- ------------------- 1/2 [requests]\n",
      "   ---------------------------------------- 2/2 [requests]\n",
      "\n",
      "Successfully installed requests-2.22.0 urllib3-1.25.11\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "ğŸ“¦ Installation terminÃ©e - Scraper anti-dÃ©tection prÃªt !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "jupyterlab-server 2.27.3 requires requests>=2.31, but you have requests 2.22.0 which is incompatible.\n",
      "selenium 4.11.2 requires urllib3[socks]<3,>=1.26, but you have urllib3 1.25.11 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "# Installation des packages pour scraping anti-dÃ©tection\n",
    "%pip install selenium webdriver-manager fake-useragent requests beautifulsoup4 pandas\n",
    "%pip install undetected-chromedriver selenium-stealth pyautogui\n",
    "%pip install requests-proxy-adapter python-dateutil lxml\n",
    "\n",
    "print(\"ğŸ“¦ Installation terminÃ©e - Scraper anti-dÃ©tection prÃªt !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88736027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting urllib3==1.26.18\n",
      "  Downloading urllib3-1.26.18-py2.py3-none-any.whl.metadata (48 kB)\n",
      "Downloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
      "Installing collected packages: urllib3\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.25.11\n",
      "    Uninstalling urllib3-1.25.11:\n",
      "      Successfully uninstalled urllib3-1.25.11\n",
      "Successfully installed urllib3-1.26.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "requests 2.22.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you have urllib3 1.26.18 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium==4.15.0\n",
      "  Downloading selenium-4.15.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium==4.15.0) (1.26.18)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from selenium==4.15.0) (0.30.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from selenium==4.15.0) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from selenium==4.15.0) (2025.6.15)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium==4.15.0) (25.3.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium==4.15.0) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium==4.15.0) (2.8)\n",
      "Requirement already satisfied: outcome in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium==4.15.0) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium==4.15.0) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium==4.15.0) (1.17.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio-websocket~=0.9->selenium==4.15.0) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium==4.15.0) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium==4.15.0) (2.22)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium==4.15.0) (0.16.0)\n",
      "Downloading selenium-4.15.0-py3-none-any.whl (10.2 MB)\n",
      "   ---------------------------------------- 0.0/10.2 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 1.0/10.2 MB 6.6 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 2.6/10.2 MB 6.7 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 4.2/10.2 MB 6.8 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 5.5/10.2 MB 6.7 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 7.1/10.2 MB 6.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 8.4/10.2 MB 6.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.0/10.2 MB 6.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.0/10.2 MB 6.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.2/10.2 MB 5.4 MB/s eta 0:00:00\n",
      "Installing collected packages: selenium\n",
      "  Attempting uninstall: selenium\n",
      "    Found existing installation: selenium 4.11.2\n",
      "    Uninstalling selenium-4.11.2:\n",
      "      Successfully uninstalled selenium-4.11.2\n",
      "Successfully installed selenium-4.15.0\n",
      "Collecting undetected-chromedriver==3.5.4\n",
      "  Downloading undetected-chromedriver-3.5.4.tar.gz (65 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: selenium>=4.9.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from undetected-chromedriver==3.5.4) (4.15.0)\n",
      "Requirement already satisfied: requests in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from undetected-chromedriver==3.5.4) (2.22.0)\n",
      "Requirement already satisfied: websockets in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from undetected-chromedriver==3.5.4) (15.0.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium>=4.9.0->undetected-chromedriver==3.5.4) (1.26.18)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver==3.5.4) (0.30.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver==3.5.4) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver==3.5.4) (2025.6.15)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver==3.5.4) (25.3.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver==3.5.4) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver==3.5.4) (2.8)\n",
      "Requirement already satisfied: outcome in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver==3.5.4) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver==3.5.4) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver==3.5.4) (1.17.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio-websocket~=0.9->selenium>=4.9.0->undetected-chromedriver==3.5.4) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium>=4.9.0->undetected-chromedriver==3.5.4) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium>=4.9.0->undetected-chromedriver==3.5.4) (2.22)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium>=4.9.0->undetected-chromedriver==3.5.4) (0.16.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from requests->undetected-chromedriver==3.5.4) (3.0.4)\n",
      "INFO: pip is looking at multiple versions of requests to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting requests (from undetected-chromedriver==3.5.4)\n",
      "  Downloading requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from requests->undetected-chromedriver==3.5.4) (3.4.2)\n",
      "Downloading requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "Building wheels for collected packages: undetected-chromedriver\n",
      "  Building wheel for undetected-chromedriver (pyproject.toml): started\n",
      "  Building wheel for undetected-chromedriver (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for undetected-chromedriver: filename=undetected_chromedriver-3.5.4-py3-none-any.whl size=47221 sha256=2906bdf9288dd8e24264f2f1eb6fd072f3be1fa9fd107e7123947cc5c3c2d2fa\n",
      "  Stored in directory: c:\\users\\yann\\appdata\\local\\pip\\cache\\wheels\\ca\\7f\\a2\\7feaa6b6bfdf742311206d2b103d185711dda87177add2ee71\n",
      "Successfully built undetected-chromedriver\n",
      "Installing collected packages: requests, undetected-chromedriver\n",
      "\n",
      "  Attempting uninstall: requests\n",
      "\n",
      "    Found existing installation: requests 2.22.0\n",
      "\n",
      "    Uninstalling requests-2.22.0:\n",
      "\n",
      "      Successfully uninstalled requests-2.22.0\n",
      "\n",
      "  Attempting uninstall: undetected-chromedriver\n",
      "\n",
      "    Found existing installation: undetected-chromedriver 3.5.3\n",
      "\n",
      "    Uninstalling undetected-chromedriver-3.5.3:\n",
      "\n",
      "      Successfully uninstalled undetected-chromedriver-3.5.3\n",
      "\n",
      "   -------------------- ------------------- 1/2 [undetected-chromedriver]\n",
      "   -------------------- ------------------- 1/2 [undetected-chromedriver]\n",
      "   ---------------------------------------- 2/2 [undetected-chromedriver]\n",
      "\n",
      "Successfully installed requests-2.32.4 undetected-chromedriver-3.5.4\n",
      "Collecting undetected-chromedriver==3.5.4\n",
      "  Downloading undetected-chromedriver-3.5.4.tar.gz (65 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: selenium>=4.9.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from undetected-chromedriver==3.5.4) (4.15.0)\n",
      "Requirement already satisfied: requests in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from undetected-chromedriver==3.5.4) (2.22.0)\n",
      "Requirement already satisfied: websockets in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from undetected-chromedriver==3.5.4) (15.0.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium>=4.9.0->undetected-chromedriver==3.5.4) (1.26.18)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver==3.5.4) (0.30.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver==3.5.4) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver==3.5.4) (2025.6.15)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver==3.5.4) (25.3.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver==3.5.4) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver==3.5.4) (2.8)\n",
      "Requirement already satisfied: outcome in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver==3.5.4) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver==3.5.4) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver==3.5.4) (1.17.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio-websocket~=0.9->selenium>=4.9.0->undetected-chromedriver==3.5.4) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium>=4.9.0->undetected-chromedriver==3.5.4) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium>=4.9.0->undetected-chromedriver==3.5.4) (2.22)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium>=4.9.0->undetected-chromedriver==3.5.4) (0.16.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from requests->undetected-chromedriver==3.5.4) (3.0.4)\n",
      "INFO: pip is looking at multiple versions of requests to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting requests (from undetected-chromedriver==3.5.4)\n",
      "  Downloading requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from requests->undetected-chromedriver==3.5.4) (3.4.2)\n",
      "Downloading requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "Building wheels for collected packages: undetected-chromedriver\n",
      "  Building wheel for undetected-chromedriver (pyproject.toml): started\n",
      "  Building wheel for undetected-chromedriver (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for undetected-chromedriver: filename=undetected_chromedriver-3.5.4-py3-none-any.whl size=47221 sha256=2906bdf9288dd8e24264f2f1eb6fd072f3be1fa9fd107e7123947cc5c3c2d2fa\n",
      "  Stored in directory: c:\\users\\yann\\appdata\\local\\pip\\cache\\wheels\\ca\\7f\\a2\\7feaa6b6bfdf742311206d2b103d185711dda87177add2ee71\n",
      "Successfully built undetected-chromedriver\n",
      "Installing collected packages: requests, undetected-chromedriver\n",
      "\n",
      "  Attempting uninstall: requests\n",
      "\n",
      "    Found existing installation: requests 2.22.0\n",
      "\n",
      "    Uninstalling requests-2.22.0:\n",
      "\n",
      "      Successfully uninstalled requests-2.22.0\n",
      "\n",
      "  Attempting uninstall: undetected-chromedriver\n",
      "\n",
      "    Found existing installation: undetected-chromedriver 3.5.3\n",
      "\n",
      "    Uninstalling undetected-chromedriver-3.5.3:\n",
      "\n",
      "      Successfully uninstalled undetected-chromedriver-3.5.3\n",
      "\n",
      "   -------------------- ------------------- 1/2 [undetected-chromedriver]\n",
      "   -------------------- ------------------- 1/2 [undetected-chromedriver]\n",
      "   ---------------------------------------- 2/2 [undetected-chromedriver]\n",
      "\n",
      "Successfully installed requests-2.32.4 undetected-chromedriver-3.5.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "requests-proxy-adapter 0.1.1 requires requests==2.22.0, but you have requests 2.32.4 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "# Dans votre terminal ou une cellule :\n",
    "!pip install urllib3==1.26.18\n",
    "!pip install selenium==4.15.0\n",
    "!pip install undetected-chromedriver==3.5.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "839c3eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¥ RÃ‰PARATION FORCE - urllib3.packages.six.moves\n",
      "ğŸ—‘ï¸ Nettoyage: uninstall urllib3 -y\n",
      "ğŸ—‘ï¸ Nettoyage: uninstall urllib3 -y\n",
      "ğŸ—‘ï¸ Nettoyage: uninstall selenium -y\n",
      "ğŸ—‘ï¸ Nettoyage: uninstall selenium -y\n",
      "ğŸ—‘ï¸ Nettoyage: uninstall undetected-chromedriver -y\n",
      "ğŸ—‘ï¸ Nettoyage: uninstall undetected-chromedriver -y\n",
      "ğŸ—‘ï¸ Nettoyage: uninstall requests -y\n",
      "ğŸ—‘ï¸ Nettoyage: uninstall requests -y\n",
      "ğŸ—‘ï¸ Nettoyage: cache purge\n",
      "ğŸ—‘ï¸ Nettoyage: cache purge\n",
      "âœ… urllib3==1.26.15\n",
      "âœ… urllib3==1.26.15\n",
      "âœ… requests==2.28.2\n",
      "âœ… requests==2.28.2\n",
      "âœ… selenium==4.11.2\n",
      "âœ… selenium==4.11.2\n",
      "âœ… undetected-chromedriver==3.5.3\n",
      "\n",
      "ğŸ§ª TEST FINAL...\n",
      "âœ… urllib3 version: 1.26.15\n",
      "âœ… selenium version: 4.11.2\n",
      "âœ… undetected_chromedriver: OK\n",
      "\n",
      "ğŸ‰ RÃ‰PARATION RÃ‰USSIE !\n",
      "âœ… undetected-chromedriver==3.5.3\n",
      "\n",
      "ğŸ§ª TEST FINAL...\n",
      "âœ… urllib3 version: 1.26.15\n",
      "âœ… selenium version: 4.11.2\n",
      "âœ… undetected_chromedriver: OK\n",
      "\n",
      "ğŸ‰ RÃ‰PARATION RÃ‰USSIE !\n"
     ]
    }
   ],
   "source": [
    "# CELLULE: RÃ‰PARATION FORCE\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "def force_fix_urllib3():\n",
    "    \"\"\"Solution force pour urllib3.packages.six.moves\"\"\"\n",
    "    \n",
    "    print(\"ğŸ”¥ RÃ‰PARATION FORCE - urllib3.packages.six.moves\")\n",
    "    \n",
    "    # 1. Nettoyage brutal\n",
    "    cleanup_commands = [\n",
    "        [sys.executable, '-m', 'pip', 'uninstall', 'urllib3', '-y'],\n",
    "        [sys.executable, '-m', 'pip', 'uninstall', 'selenium', '-y'], \n",
    "        [sys.executable, '-m', 'pip', 'uninstall', 'undetected-chromedriver', '-y'],\n",
    "        [sys.executable, '-m', 'pip', 'uninstall', 'requests', '-y'],\n",
    "        [sys.executable, '-m', 'pip', 'cache', 'purge']\n",
    "    ]\n",
    "    \n",
    "    for cmd in cleanup_commands:\n",
    "        subprocess.run(cmd, capture_output=True)\n",
    "        print(f\"ğŸ—‘ï¸ Nettoyage: {' '.join(cmd[3:])}\")\n",
    "    \n",
    "    # 2. Installation versions STABLES\n",
    "    stable_packages = [\n",
    "        'urllib3==1.26.15',\n",
    "        'requests==2.28.2', \n",
    "        'selenium==4.11.2',\n",
    "        'undetected-chromedriver==3.5.3'\n",
    "    ]\n",
    "    \n",
    "    for package in stable_packages:\n",
    "        cmd = [sys.executable, '-m', 'pip', 'install', package]\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(f\"âœ… {package}\")\n",
    "        else:\n",
    "            print(f\"âŒ {package}: {result.stderr}\")\n",
    "    \n",
    "    # 3. Test immÃ©diat\n",
    "    print(\"\\nğŸ§ª TEST FINAL...\")\n",
    "    try:\n",
    "        import importlib\n",
    "        import urllib3\n",
    "        print(f\"âœ… urllib3 version: {urllib3.__version__}\")\n",
    "        \n",
    "        import selenium\n",
    "        print(f\"âœ… selenium version: {selenium.__version__}\")\n",
    "        \n",
    "        import undetected_chromedriver as uc\n",
    "        print(\"âœ… undetected_chromedriver: OK\")\n",
    "        \n",
    "        print(\"\\nğŸ‰ RÃ‰PARATION RÃ‰USSIE !\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Test Ã©chouÃ©: {e}\")\n",
    "\n",
    "# EXÃ‰CUTER LA RÃ‰PARATION FORCE\n",
    "force_fix_urllib3()\n",
    "\n",
    "import random\n",
    "import undetected_chromedriver as uc\n",
    "\n",
    "REALISTIC_USER_AGENTS = [\n",
    "    # Liste d'exemples d'agents utilisateurs rÃ©alistes\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.2 Safari/605.1.15\",\n",
    "    \"Mozilla/5.0 (iPhone; CPU iPhone OS 14_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.1 Mobile/15E148 Safari/604.1\",\n",
    "    \"Mozilla/5.0 (Linux; Android 10; SM-G973F) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Mobile Safari/537.36\",\n",
    "    # Ajoutez d'autres agents utilisateurs si nÃ©cessaire\n",
    "]\n",
    "\n",
    "PROXY_LIST = [\n",
    "    # Liste d'exemples de proxies\n",
    "    \"http://proxy1:port\",\n",
    "    \"http://proxy2:port\",\n",
    "    \"http://proxy3:port\",\n",
    "    # Ajoutez d'autres proxies si nÃ©cessaire\n",
    "]\n",
    "\n",
    "class StealthMarketplaceScraper:\n",
    "    \"\"\"\n",
    "    Scraper avancÃ© avec protection anti-dÃ©tection pour marketplaces\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, headless=True, use_proxy=False):\n",
    "        self.headless = headless\n",
    "        self.use_proxy = use_proxy\n",
    "        self.driver = None\n",
    "        self.session_duration = 0\n",
    "        \n",
    "    def _get_random_user_agent(self):\n",
    "        return random.choice(REALISTIC_USER_AGENTS)\n",
    "    \n",
    "    def _get_random_proxy(self):\n",
    "        return random.choice(PROXY_LIST) if self.use_proxy and PROXY_LIST else None\n",
    "        \n",
    "    def _setup_driver(self):\n",
    "        \"\"\"Configuration Chrome optimisÃ©e et compatible\"\"\"\n",
    "        try:\n",
    "            options = uc.ChromeOptions()\n",
    "            \n",
    "            # Options de base compatibles\n",
    "            options.add_argument('--no-sandbox')\n",
    "            options.add_argument('--disable-dev-shm-usage')\n",
    "            options.add_argument('--disable-gpu')\n",
    "            options.add_argument('--disable-extensions')\n",
    "            options.add_argument('--disable-plugins')\n",
    "            options.add_argument('--disable-images')\n",
    "            options.add_argument('--disable-javascript')\n",
    "            options.add_argument('--disable-notifications')\n",
    "            options.add_argument('--disable-popup-blocking')\n",
    "            options.add_argument('--no-first-run')\n",
    "            options.add_argument('--no-default-browser-check')\n",
    "            options.add_argument('--ignore-certificate-errors')\n",
    "            options.add_argument('--ignore-ssl-errors')\n",
    "            options.add_argument('--ignore-certificate-errors-spki-list')\n",
    "            options.add_argument('--disable-web-security')\n",
    "            \n",
    "            if self.headless:\n",
    "                options.add_argument('--headless')\n",
    "                \n",
    "            # User agent alÃ©atoire\n",
    "            user_agent = self._get_random_user_agent()\n",
    "            options.add_argument(f'--user-agent={user_agent}')\n",
    "            \n",
    "            # Proxy si activÃ©\n",
    "            proxy = self._get_random_proxy()\n",
    "            if proxy:\n",
    "                options.add_argument(f'--proxy-server={proxy}')\n",
    "                \n",
    "            # Options expÃ©rimentales compatibles (sans excludeSwitches)\n",
    "            options.add_experimental_option('useAutomationExtension', False)\n",
    "            options.add_experimental_option(\"prefs\", {\n",
    "                \"profile.default_content_setting_values.notifications\": 2,\n",
    "                \"profile.default_content_settings.popups\": 0,\n",
    "                \"profile.managed_default_content_settings.images\": 2\n",
    "            })\n",
    "            \n",
    "            # CrÃ©ation du driver avec version dÃ©tectÃ©e automatiquement\n",
    "            self.driver = uc.Chrome(options=options, version_main=None)\n",
    "            \n",
    "            # Injection anti-dÃ©tection JavaScript\n",
    "            self.driver.execute_cdp_cmd('Page.addScriptToEvaluateOnNewDocument', {\n",
    "                'source': '''\n",
    "                    Object.defineProperty(navigator, 'webdriver', {\n",
    "                        get: () => undefined,\n",
    "                    });\n",
    "                    \n",
    "                    Object.defineProperty(navigator, 'plugins', {\n",
    "                        get: () => [1, 2, 3, 4, 5],\n",
    "                    });\n",
    "                    \n",
    "                    Object.defineProperty(navigator, 'languages', {\n",
    "                        get: () => ['en-US', 'en'],\n",
    "                    });\n",
    "                    \n",
    "                    window.chrome = {\n",
    "                        runtime: {},\n",
    "                    };\n",
    "                '''\n",
    "            })\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Erreur crÃ©ation driver: {e}\")\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b76c7499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Imports terminÃ©s - SystÃ¨me anti-dÃ©tection prÃªt ! ğŸ•µï¸\n"
     ]
    }
   ],
   "source": [
    "import undetected_chromedriver as uc\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium_stealth import stealth\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from fake_useragent import UserAgent\n",
    "import logging\n",
    "from typing import List, Dict, Optional\n",
    "import os\n",
    "\n",
    "# Configuration du logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configuration globale\n",
    "ua = UserAgent()\n",
    "\n",
    "# Liste de proxies gratuits (remplacer par des proxies premium en production)\n",
    "PROXY_LIST = [\n",
    "    # \"http://proxy1:port\",\n",
    "    # \"http://proxy2:port\", \n",
    "    # Ajoutez vos proxies ici\n",
    "]\n",
    "\n",
    "# User agents rÃ©alistes\n",
    "REALISTIC_USER_AGENTS = [\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15'\n",
    "]\n",
    "\n",
    "print(\"âœ… Imports terminÃ©s - SystÃ¨me anti-dÃ©tection prÃªt ! ğŸ•µï¸\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23ef9f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 16:10:40,189 - INFO - âœ… MarketplaceScraper initialisÃ©\n",
      "2025-06-27 16:10:40,192 - INFO - ğŸ”§ Initialisation du scraper anti-dÃ©tection...\n",
      "2025-06-27 16:10:40,192 - INFO - ğŸ”§ Initialisation du scraper anti-dÃ©tection...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ Scraper initialisÃ© avec succÃ¨s !\n",
      "ğŸ•µï¸ Scraper anti-dÃ©tection initialisÃ© !\n"
     ]
    }
   ],
   "source": [
    "class MarketplaceScraper:\n",
    "    \"\"\"\n",
    "    Scraper principal pour les marketplaces et reviews.\n",
    "    Supporte diffÃ©rentes plateformes avec rotation d'User-Agent.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, delay_range=(1, 3)):\n",
    "        self.session = requests.Session()\n",
    "        self.delay_min, self.delay_max = delay_range\n",
    "        self.ua = UserAgent()\n",
    "        \n",
    "        # Headers par dÃ©faut\n",
    "        self.session.headers.update({\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "            'Accept-Language': 'fr-FR,fr;q=0.9,en;q=0.8',\n",
    "            'Accept-Encoding': 'gzip, deflate',\n",
    "            'Connection': 'keep-alive',\n",
    "            'Upgrade-Insecure-Requests': '1',\n",
    "        })\n",
    "        \n",
    "        logger.info(\"âœ… MarketplaceScraper initialisÃ©\")\n",
    "    \n",
    "    def _get_page(self, url: str, retries: int = 3) -> Optional[BeautifulSoup]:\n",
    "        \"\"\"\n",
    "        RÃ©cupÃ¨re une page web avec gestion d'erreurs et dÃ©lais.\n",
    "        \"\"\"\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                # Rotation d'User-Agent Ã  chaque requÃªte\n",
    "                self.session.headers['User-Agent'] = self.ua.random\n",
    "                \n",
    "                response = self.session.get(url, timeout=10)\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                # DÃ©lai alÃ©atoire entre requÃªtes\n",
    "                delay = random.uniform(self.delay_min, self.delay_max)\n",
    "                time.sleep(delay)\n",
    "                \n",
    "                return BeautifulSoup(response.content, 'html.parser')\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Erreur tentative {attempt + 1}/{retries} pour {url}: {e}\")\n",
    "                if attempt < retries - 1:\n",
    "                    time.sleep(2 ** attempt)  # Backoff exponentiel\n",
    "                else:\n",
    "                    logger.error(f\"Ã‰chec dÃ©finitif pour {url}\")\n",
    "                    return None\n",
    "    \n",
    "    def _extract_date(self, date_text: str) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Extrait et normalise les dates depuis diffÃ©rents formats.\n",
    "        \"\"\"\n",
    "        if not date_text:\n",
    "            return None\n",
    "            \n",
    "        date_text = date_text.strip().lower()\n",
    "        \n",
    "        # Patterns de dates franÃ§ais\n",
    "        patterns = [\n",
    "            (r'(\\d{1,2})\\s+(janvier|fÃ©vrier|mars|avril|mai|juin|juillet|aoÃ»t|septembre|octobre|novembre|dÃ©cembre)\\s+(\\d{4})', '%d %B %Y'),\n",
    "            (r'(\\d{1,2})/(\\d{1,2})/(\\d{4})', '%d/%m/%Y'),\n",
    "            (r'(\\d{4})-(\\d{1,2})-(\\d{1,2})', '%Y-%m-%d'),\n",
    "        ]\n",
    "        \n",
    "        # Mapping des mois franÃ§ais\n",
    "        months_fr = {\n",
    "            'janvier': 'january', 'fÃ©vrier': 'february', 'mars': 'march',\n",
    "            'avril': 'april', 'mai': 'may', 'juin': 'june',\n",
    "            'juillet': 'july', 'aoÃ»t': 'august', 'septembre': 'september',\n",
    "            'octobre': 'october', 'novembre': 'november', 'dÃ©cembre': 'december'\n",
    "        }\n",
    "        \n",
    "        for month_fr, month_en in months_fr.items():\n",
    "            date_text = date_text.replace(month_fr, month_en)\n",
    "        \n",
    "        for pattern, fmt in patterns:\n",
    "            match = re.search(pattern, date_text)\n",
    "            if match:\n",
    "                try:\n",
    "                    date_obj = datetime.strptime(match.group(), fmt)\n",
    "                    return date_obj.strftime('%Y-%m-%d')\n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "        return None\n",
    "\n",
    "class StealthMarketplaceScraper:\n",
    "    \"\"\"\n",
    "    Scraper anti-dÃ©tection pour marketplaces avec Selenium.\n",
    "    Inclut rotation d'IP, fake user agents, et comportement humain.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, headless=True, use_proxy=False):\n",
    "        self.headless = headless\n",
    "        self.use_proxy = use_proxy\n",
    "        self.driver = None\n",
    "        self.current_proxy = None\n",
    "        \n",
    "        logger.info(\"ğŸ”§ Initialisation du scraper anti-dÃ©tection...\")\n",
    "        \n",
    "    def _get_random_user_agent(self):\n",
    "        \"\"\"Retourne un User Agent alÃ©atoire rÃ©aliste.\"\"\"\n",
    "        return random.choice(REALISTIC_USER_AGENTS)\n",
    "    \n",
    "    def _get_random_proxy(self):\n",
    "        \"\"\"Retourne un proxy alÃ©atoire.\"\"\"\n",
    "        if PROXY_LIST:\n",
    "            return random.choice(PROXY_LIST)\n",
    "        return None\n",
    "    \n",
    "    def _setup_driver(self):\n",
    "        \"\"\"Configure le driver Chrome avec toutes les protections anti-dÃ©tection.\"\"\"\n",
    "        options = Options()\n",
    "        \n",
    "        # Configuration anti-dÃ©tection\n",
    "        options.add_argument('--no-sandbox')\n",
    "        options.add_argument('--disable-dev-shm-usage')\n",
    "        options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "        options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "        options.add_experimental_option('useAutomationExtension', False)\n",
    "        \n",
    "        # User Agent alÃ©atoire\n",
    "        user_agent = self._get_random_user_agent()\n",
    "        options.add_argument(f'--user-agent={user_agent}')\n",
    "        logger.info(f\"ğŸ­ User Agent: {user_agent[:50]}...\")\n",
    "        \n",
    "        # Proxy si activÃ©\n",
    "        if self.use_proxy:\n",
    "            proxy = self._get_random_proxy()\n",
    "            if proxy:\n",
    "                options.add_argument(f'--proxy-server={proxy}')\n",
    "                self.current_proxy = proxy\n",
    "                logger.info(f\"ğŸŒ Proxy: {proxy}\")\n",
    "        \n",
    "        # Mode headless si demandÃ©\n",
    "        if self.headless:\n",
    "            options.add_argument('--headless')\n",
    "        \n",
    "        # Taille de fenÃªtre rÃ©aliste\n",
    "        options.add_argument('--window-size=1920,1080')\n",
    "        \n",
    "        # DÃ©sactiver les images pour plus de rapiditÃ© (optionnel)\n",
    "        # options.add_argument('--blink-settings=imagesEnabled=false')\n",
    "        \n",
    "        try:\n",
    "            # Utiliser undetected-chromedriver pour Ã©viter la dÃ©tection\n",
    "            self.driver = uc.Chrome(options=options)\n",
    "            \n",
    "            # Configuration Selenium Stealth pour plus de protection\n",
    "            stealth(self.driver,\n",
    "                   languages=[\"en-US\", \"en\"],\n",
    "                   vendor=\"Google Inc.\",\n",
    "                   platform=\"Win32\",\n",
    "                   webgl_vendor=\"Intel Inc.\",\n",
    "                   renderer=\"Intel Iris OpenGL Engine\",\n",
    "                   fix_hairline=True)\n",
    "            \n",
    "            # Masquer le fait que c'est un webdriver\n",
    "            self.driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
    "            \n",
    "            logger.info(\"âœ… Driver Chrome configurÃ© avec protections anti-dÃ©tection\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"âŒ Erreur lors de la configuration du driver: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def _human_like_delay(self, min_delay=1, max_delay=3):\n",
    "        \"\"\"DÃ©lai alÃ©atoire pour simuler un comportement humain.\"\"\"\n",
    "        delay = random.uniform(min_delay, max_delay)\n",
    "        time.sleep(delay)\n",
    "    \n",
    "    def _simulate_human_behavior(self):\n",
    "        \"\"\"Simule des comportements humains alÃ©atoires.\"\"\"\n",
    "        actions = ActionChains(self.driver)\n",
    "        \n",
    "        # Mouvement alÃ©atoire de la souris\n",
    "        if random.random() < 0.3:  # 30% de chance\n",
    "            x_offset = random.randint(-100, 100)\n",
    "            y_offset = random.randint(-100, 100)\n",
    "            actions.move_by_offset(x_offset, y_offset).perform()\n",
    "            time.sleep(random.uniform(0.1, 0.5))\n",
    "        \n",
    "        # Scroll alÃ©atoire\n",
    "        if random.random() < 0.4:  # 40% de chance\n",
    "            scroll_amount = random.randint(100, 500)\n",
    "            self.driver.execute_script(f\"window.scrollBy(0, {scroll_amount});\")\n",
    "            time.sleep(random.uniform(0.5, 1.5))\n",
    "    \n",
    "    def start_session(self):\n",
    "        \"\"\"DÃ©marre une session de scraping.\"\"\"\n",
    "        self._setup_driver()\n",
    "        logger.info(\"ğŸš€ Session de scraping dÃ©marrÃ©e\")\n",
    "    \n",
    "    def close_session(self):\n",
    "        \"\"\"Ferme la session de scraping.\"\"\"\n",
    "        if self.driver:\n",
    "            self.driver.quit()\n",
    "            logger.info(\"ğŸ”š Session fermÃ©e\")\n",
    "        \n",
    "    def get_page(self, url: str, wait_time: int = 10) -> bool:\n",
    "        \"\"\"\n",
    "        Navigue vers une page avec comportement humain simulÃ©.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(f\"ğŸŒ Navigation vers: {url}\")\n",
    "            \n",
    "            # Navigation avec dÃ©lai humain\n",
    "            self.driver.get(url)\n",
    "            self._human_like_delay(2, 4)\n",
    "            \n",
    "            # Attendre que la page se charge\n",
    "            WebDriverWait(self.driver, wait_time).until(\n",
    "                EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
    "            )\n",
    "            \n",
    "            # Comportement humain alÃ©atoire\n",
    "            self._simulate_human_behavior()\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"âŒ Erreur lors de la navigation: {e}\")\n",
    "            return False\n",
    "\n",
    "# Test de la classe\n",
    "scraper = MarketplaceScraper()\n",
    "print(\"ğŸ¯ Scraper initialisÃ© avec succÃ¨s !\")\n",
    "\n",
    "# Test de la classe anti-dÃ©tection\n",
    "stealth_scraper = StealthMarketplaceScraper(headless=False)  # Visible pour le test\n",
    "print(\"ğŸ•µï¸ Scraper anti-dÃ©tection initialisÃ© !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "876bc2f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ MarketplaceProductScraper prÃªt avec anti-dÃ©tection complÃ¨te !\n"
     ]
    }
   ],
   "source": [
    "class MarketplaceProductScraper(StealthMarketplaceScraper):\n",
    "    \"\"\"\n",
    "    Scraper spÃ©cialisÃ© pour produits et reviews de marketplaces.\n",
    "    \"\"\"\n",
    "    \n",
    "    def scrape_amazon_style_products(self, search_term: str, max_pages: int = 3) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Scrape des produits type Amazon (utilise un site de dÃ©monstration).\n",
    "        \"\"\"\n",
    "        logger.info(f\"ğŸ›ï¸ Scraping produits pour: {search_term}\")\n",
    "        \n",
    "        if not self.driver:\n",
    "            self.start_session()\n",
    "        \n",
    "        products_data = []\n",
    "        \n",
    "        try:\n",
    "            # Utiliser un site de dÃ©monstration e-commerce\n",
    "            base_url = \"https://webscraper.io/test-sites/e-commerce/allinone\"\n",
    "            \n",
    "            if not self.get_page(base_url):\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            # Attendre et trouver les produits\n",
    "            products = WebDriverWait(self.driver, 10).until(\n",
    "                EC.presence_of_all_elements_located((By.CSS_SELECTOR, \".product-wrapper\"))\n",
    "            )\n",
    "            \n",
    "            for i, product in enumerate(products[:20]):  # Limiter Ã  20 produits\n",
    "                try:\n",
    "                    # Simuler un comportement humain\n",
    "                    if i % 5 == 0:\n",
    "                        self._simulate_human_behavior()\n",
    "                    \n",
    "                    # Extraire les donnÃ©es du produit\n",
    "                    title_elem = product.find_element(By.CSS_SELECTOR, \".title\")\n",
    "                    price_elem = product.find_element(By.CSS_SELECTOR, \".price\")\n",
    "                    \n",
    "                    title = title_elem.text.strip()\n",
    "                    price_text = price_elem.text.strip()\n",
    "                    \n",
    "                    # Nettoyer le prix\n",
    "                    price = re.findall(r'[\\d.]+', price_text)\n",
    "                    price = float(price[0]) if price else 0.0\n",
    "                    \n",
    "                    # Essayer de trouver la description et l'image\n",
    "                    try:\n",
    "                        desc_elem = product.find_element(By.CSS_SELECTOR, \".description\")\n",
    "                        description = desc_elem.text.strip()\n",
    "                    except:\n",
    "                        description = \"\"\n",
    "                    \n",
    "                    try:\n",
    "                        img_elem = product.find_element(By.CSS_SELECTOR, \"img\")\n",
    "                        image_url = img_elem.get_attribute(\"src\")\n",
    "                    except:\n",
    "                        image_url = \"\"\n",
    "                    \n",
    "                    products_data.append({\n",
    "                        'product_id': f\"demo_{i}\",\n",
    "                        'title': title,\n",
    "                        'price': price,\n",
    "                        'description': description,\n",
    "                        'image_url': image_url,\n",
    "                        'source': 'webscraper.io',\n",
    "                        'search_term': search_term,\n",
    "                        'scraped_at': datetime.now().isoformat(),\n",
    "                        'user_agent': self.driver.execute_script(\"return navigator.userAgent;\"),\n",
    "                        'proxy': self.current_proxy\n",
    "                    })\n",
    "                    \n",
    "                    # DÃ©lai humain entre produits\n",
    "                    self._human_like_delay(0.5, 1.5)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Erreur extraction produit {i}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            logger.info(f\"âœ… {len(products_data)} produits extraits\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"âŒ Erreur scraping produits: {e}\")\n",
    "        \n",
    "        return pd.DataFrame(products_data)\n",
    "    \n",
    "    def scrape_product_reviews(self, product_url: str, max_reviews: int = 50) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Scrape les reviews d'un produit avec dates et sentiments.\n",
    "        \"\"\"\n",
    "        logger.info(f\"ğŸ“ Scraping reviews pour: {product_url}\")\n",
    "        \n",
    "        if not self.driver:\n",
    "            self.start_session()\n",
    "        \n",
    "        reviews_data = []\n",
    "        \n",
    "        try:\n",
    "            # Aller sur la page du produit\n",
    "            if not self.get_page(product_url):\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            # Simuler des reviews (site de dÃ©monstration n'a pas de vraies reviews)\n",
    "            # En production, adapter les sÃ©lecteurs CSS selon le site cible\n",
    "            \n",
    "            for i in range(min(max_reviews, 20)):  # Simuler jusqu'Ã  20 reviews\n",
    "                # GÃ©nÃ©rer des reviews rÃ©alistes pour la dÃ©monstration\n",
    "                review_texts = [\n",
    "                    \"Great product, highly recommend!\",\n",
    "                    \"Good value for money, works as expected.\",\n",
    "                    \"Not bad but could be better quality.\",\n",
    "                    \"Excellent service and fast delivery!\",\n",
    "                    \"Product broke after a few days, disappointed.\",\n",
    "                    \"Amazing quality, will buy again!\",\n",
    "                    \"Okay product, nothing special.\",\n",
    "                    \"Love it! Exactly what I was looking for.\",\n",
    "                    \"Poor quality, would not recommend.\",\n",
    "                    \"Perfect! Exceeded my expectations.\"\n",
    "                ]\n",
    "                \n",
    "                reviewer_names = [\n",
    "                    \"John D.\", \"Sarah M.\", \"Mike K.\", \"Emma L.\", \"David R.\",\n",
    "                    \"Lisa P.\", \"Tom W.\", \"Anna S.\", \"Chris B.\", \"Maria G.\"\n",
    "                ]\n",
    "                \n",
    "                # Simuler une review\n",
    "                review_text = random.choice(review_texts)\n",
    "                reviewer = random.choice(reviewer_names)\n",
    "                rating = random.randint(1, 5)\n",
    "                \n",
    "                # GÃ©nÃ©rer une date rÃ©aliste (derniers 6 mois)\n",
    "                days_ago = random.randint(1, 180)\n",
    "                review_date = (datetime.now() - timedelta(days=days_ago)).strftime('%Y-%m-%d')\n",
    "                \n",
    "                reviews_data.append({\n",
    "                    'review_id': f\"review_{i}\",\n",
    "                    'product_url': product_url,\n",
    "                    'reviewer_name': reviewer,\n",
    "                    'review_text': review_text,\n",
    "                    'rating': rating,\n",
    "                    'review_date': review_date,\n",
    "                    'helpful_votes': random.randint(0, 50),\n",
    "                    'verified_purchase': random.choice([True, False]),\n",
    "                    'scraped_at': datetime.now().isoformat(),\n",
    "                    'source': 'demo_marketplace'\n",
    "                })\n",
    "                \n",
    "                # DÃ©lai humain\n",
    "                self._human_like_delay(0.3, 1.0)\n",
    "            \n",
    "            logger.info(f\"âœ… {len(reviews_data)} reviews gÃ©nÃ©rÃ©es (dÃ©monstration)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"âŒ Erreur scraping reviews: {e}\")\n",
    "        \n",
    "        return pd.DataFrame(reviews_data)\n",
    "    \n",
    "    def scrape_trustpilot_reviews(self, company_name: str, max_reviews: int = 100) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Scrape des reviews Trustpilot avec anti-dÃ©tection.\n",
    "        \"\"\"\n",
    "        logger.info(f\"â­ Scraping Trustpilot pour: {company_name}\")\n",
    "        \n",
    "        if not self.driver:\n",
    "            self.start_session()\n",
    "        \n",
    "        reviews_data = []\n",
    "        \n",
    "        try:\n",
    "            # URL Trustpilot\n",
    "            url = f\"https://www.trustpilot.com/review/{company_name.lower().replace(' ', '-')}\"\n",
    "            \n",
    "            if not self.get_page(url):\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            # Attendre que les reviews se chargent\n",
    "            time.sleep(3)\n",
    "            \n",
    "            # Chercher les reviews (sÃ©lecteurs peuvent changer)\n",
    "            try:\n",
    "                reviews = self.driver.find_elements(By.CSS_SELECTOR, \"[data-service-review-card-paper]\")\n",
    "                \n",
    "                for i, review in enumerate(reviews[:max_reviews]):\n",
    "                    try:\n",
    "                        # Simuler comportement humain\n",
    "                        if i % 10 == 0:\n",
    "                            self._simulate_human_behavior()\n",
    "                        \n",
    "                        # Extraire les donnÃ©es (adapter selon le HTML actuel)\n",
    "                        review_text = review.find_element(By.CSS_SELECTOR, \"[data-service-review-text-typography]\").text\n",
    "                        rating_elem = review.find_element(By.CSS_SELECTOR, \"[data-service-review-rating]\")\n",
    "                        rating = len(rating_elem.find_elements(By.CSS_SELECTOR, \"svg[data-star-fill='true']\"))\n",
    "                        \n",
    "                        # Date de la review\n",
    "                        try:\n",
    "                            date_elem = review.find_element(By.CSS_SELECTOR, \"time\")\n",
    "                            review_date = date_elem.get_attribute(\"datetime\")[:10]\n",
    "                        except:\n",
    "                            review_date = datetime.now().strftime('%Y-%m-%d')\n",
    "                        \n",
    "                        # Nom du reviewer\n",
    "                        try:\n",
    "                            name_elem = review.find_element(By.CSS_SELECTOR, \"[data-consumer-name-typography]\")\n",
    "                            reviewer_name = name_elem.text\n",
    "                        except:\n",
    "                            reviewer_name = f\"Anonymous_{i}\"\n",
    "                        \n",
    "                        reviews_data.append({\n",
    "                            'review_id': f\"trustpilot_{company_name}_{i}\",\n",
    "                            'company': company_name,\n",
    "                            'reviewer_name': reviewer_name,\n",
    "                            'review_text': review_text,\n",
    "                            'rating': rating,\n",
    "                            'review_date': review_date,\n",
    "                            'source': 'trustpilot.com',\n",
    "                            'scraped_at': datetime.now().isoformat()\n",
    "                        })\n",
    "                        \n",
    "                        self._human_like_delay(0.5, 2.0)\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"Erreur extraction review {i}: {e}\")\n",
    "                        continue\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Aucune review trouvÃ©e ou structure HTML diffÃ©rente: {e}\")\n",
    "            \n",
    "            logger.info(f\"âœ… {len(reviews_data)} reviews Trustpilot extraites\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"âŒ Erreur scraping Trustpilot: {e}\")\n",
    "        \n",
    "        return pd.DataFrame(reviews_data)\n",
    "\n",
    "# Initialisation du scraper\n",
    "print(\"ğŸ¯ MarketplaceProductScraper prÃªt avec anti-dÃ©tection complÃ¨te !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "368873ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 16:10:40,504 - INFO - ğŸ“ Dossiers de donnÃ©es crÃ©Ã©s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ Fonctions utilitaires chargÃ©es !\n"
     ]
    }
   ],
   "source": [
    "# Fonctions utilitaires pour la sauvegarde et l'analyse\n",
    "def save_scraped_data(df: pd.DataFrame, filename: str, data_dir: str = \"../data/raw\"):\n",
    "    \"\"\"Sauvegarde les donnÃ©es scrapÃ©es avec timestamp.\"\"\"\n",
    "    if df.empty:\n",
    "        logger.warning(\"Aucune donnÃ©e Ã  sauvegarder\")\n",
    "        return\n",
    "    \n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filepath = os.path.join(data_dir, f\"{timestamp}_{filename}\")\n",
    "    \n",
    "    df.to_csv(filepath, index=False, encoding='utf-8')\n",
    "    logger.info(f\"ğŸ’¾ DonnÃ©es sauvegardÃ©es: {filepath} ({len(df)} enregistrements)\")\n",
    "    \n",
    "    return filepath\n",
    "\n",
    "def analyze_scraped_data(df: pd.DataFrame):\n",
    "    \"\"\"Analyse rapide des donnÃ©es scrapÃ©es.\"\"\"\n",
    "    if df.empty:\n",
    "        print(\"âŒ Aucune donnÃ©e Ã  analyser\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Analyse des donnÃ©es scrapÃ©es:\")\n",
    "    print(f\"   Total des enregistrements: {len(df)}\")\n",
    "    print(f\"   Colonnes: {list(df.columns)}\")\n",
    "    \n",
    "    if 'source' in df.columns:\n",
    "        print(f\"   Sources: {df['source'].value_counts().to_dict()}\")\n",
    "    \n",
    "    if 'rating' in df.columns:\n",
    "        print(f\"   Note moyenne: {df['rating'].mean():.2f}\")\n",
    "        print(f\"   Distribution des notes: {df['rating'].value_counts().sort_index().to_dict()}\")\n",
    "    \n",
    "    if 'price' in df.columns:\n",
    "        print(f\"   Prix moyen: ${df['price'].mean():.2f}\")\n",
    "        print(f\"   Prix min/max: ${df['price'].min():.2f} - ${df['price'].max():.2f}\")\n",
    "    \n",
    "    print(f\"\\nğŸ” AperÃ§u des donnÃ©es:\")\n",
    "    return df.head()\n",
    "\n",
    "def setup_data_directories():\n",
    "    \"\"\"CrÃ©e les dossiers nÃ©cessaires pour les donnÃ©es.\"\"\"\n",
    "    directories = [\"../data/raw\", \"../data/processed\", \"../logs\"]\n",
    "    for directory in directories:\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "        \n",
    "    logger.info(\"ğŸ“ Dossiers de donnÃ©es crÃ©Ã©s\")\n",
    "\n",
    "# Configuration initiale\n",
    "setup_data_directories()\n",
    "print(\"ğŸ¯ Fonctions utilitaires chargÃ©es !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5bdaf40c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš¨ Pour lancer le test, exÃ©cutez: test_results = test_marketplace_scraper()\n",
      "âš ï¸  Assurez-vous d'avoir Chrome installÃ© et une connexion internet stable\n"
     ]
    }
   ],
   "source": [
    "# ğŸ§ª TEST DU SCRAPER - DÃ©monstration complÃ¨te\n",
    "def test_marketplace_scraper():\n",
    "    \"\"\"\n",
    "    Test complet du scraper avec toutes les protections anti-dÃ©tection.\n",
    "    \"\"\"\n",
    "    logger.info(\"ğŸ§ª DÃ©marrage des tests du scraper...\")\n",
    "    \n",
    "    # Initialiser le scraper (headless=False pour voir le navigateur)\n",
    "    scraper = MarketplaceProductScraper(headless=False, use_proxy=False)\n",
    "    \n",
    "    try:\n",
    "        # DÃ©marrer la session\n",
    "        scraper.start_session()\n",
    "        \n",
    "        # Test 1: Scraper des produits\n",
    "        print(\"\\nğŸ›ï¸ Test 1: Scraping de produits...\")\n",
    "        products_df = scraper.scrape_amazon_style_products(\"laptop\", max_pages=1)\n",
    "        \n",
    "        if not products_df.empty:\n",
    "            save_scraped_data(products_df, \"products_demo.csv\")\n",
    "            analyze_scraped_data(products_df)\n",
    "        \n",
    "        # Test 2: Scraper des reviews (simulÃ©es)\n",
    "        print(\"\\nğŸ“ Test 2: Scraping de reviews...\")\n",
    "        reviews_df = scraper.scrape_product_reviews(\"https://webscraper.io/test-sites/e-commerce/allinone\", max_reviews=10)\n",
    "        \n",
    "        if not reviews_df.empty:\n",
    "            save_scraped_data(reviews_df, \"reviews_demo.csv\")\n",
    "            analyze_scraped_data(reviews_df)\n",
    "        \n",
    "        # Test 3: Trustpilot (optionnel - nÃ©cessite une vraie entreprise)\n",
    "        # print(\"\\nâ­ Test 3: Scraping Trustpilot...\")\n",
    "        # trustpilot_df = scraper.scrape_trustpilot_reviews(\"amazon\", max_reviews=5)\n",
    "        \n",
    "        print(\"\\nâœ… Tests terminÃ©s avec succÃ¨s !\")\n",
    "        \n",
    "        return {\n",
    "            'products': products_df,\n",
    "            'reviews': reviews_df\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"âŒ Erreur durant les tests: {e}\")\n",
    "        return None\n",
    "        \n",
    "    finally:\n",
    "        # Toujours fermer le navigateur\n",
    "        scraper.close_session()\n",
    "\n",
    "# âš ï¸ ATTENTION: DÃ©commentez la ligne suivante pour lancer le test\n",
    "# Cela ouvrira un navigateur Chrome et commencera le scraping\n",
    "print(\"ğŸš¨ Pour lancer le test, exÃ©cutez: test_results = test_marketplace_scraper()\")\n",
    "print(\"âš ï¸  Assurez-vous d'avoir Chrome installÃ© et une connexion internet stable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ace7ab4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” DIAGNOSTIC CHROME BINARY...\n",
      "âœ… Chrome trouvÃ©: C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe\n",
      "ğŸ”§ Configuration driver corrigÃ©...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 16:10:42,867 - INFO - patching driver executable C:\\Users\\Yann\\appdata\\roaming\\undetected_chromedriver\\undetected_chromedriver.exe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Driver crÃ©Ã© avec succÃ¨s !\n",
      "ğŸ‰ TEST RAPIDE...\n",
      "âœ… Navigation fonctionne !\n",
      "âœ… Navigation fonctionne !\n"
     ]
    }
   ],
   "source": [
    "# CELLULE: FIX CHROME BINARY LOCATION\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def fix_chrome_binary():\n",
    "    \"\"\"RÃ©pare la configuration Chrome Binary\"\"\"\n",
    "    \n",
    "    print(\"ğŸ” DIAGNOSTIC CHROME BINARY...\")\n",
    "    \n",
    "    # 1. Trouver Chrome automatiquement\n",
    "    possible_chrome_paths = [\n",
    "        r\"C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe\",\n",
    "        r\"C:\\Program Files (x86)\\Google\\Chrome\\Application\\chrome.exe\",\n",
    "        r\"C:\\Users\\{}\\AppData\\Local\\Google\\Chrome\\Application\\chrome.exe\".format(os.getenv('USERNAME')),\n",
    "        r\"C:\\Program Files\\BraveSoftware\\Brave-Browser\\Application\\brave.exe\",\n",
    "        r\"C:\\Program Files (x86)\\Microsoft\\Edge\\Application\\msedge.exe\"\n",
    "    ]\n",
    "    \n",
    "    chrome_path = None\n",
    "    for path in possible_chrome_paths:\n",
    "        if os.path.exists(path):\n",
    "            chrome_path = path\n",
    "            print(f\"âœ… Chrome trouvÃ©: {path}\")\n",
    "            break\n",
    "    \n",
    "    if not chrome_path:\n",
    "        print(\"âŒ Chrome non trouvÃ© - Installation automatique...\")\n",
    "        install_chrome()\n",
    "        return\n",
    "    \n",
    "    # 2. Configuration Chrome corrigÃ©e\n",
    "    return create_fixed_driver(chrome_path)\n",
    "\n",
    "def install_chrome():\n",
    "    \"\"\"Installe Chrome automatiquement\"\"\"\n",
    "    \n",
    "    print(\"ğŸ“¥ Installation Chrome...\")\n",
    "    \n",
    "    # URL de tÃ©lÃ©chargement Chrome\n",
    "    chrome_url = \"https://dl.google.com/chrome/install/latest/chrome_installer.exe\"\n",
    "    \n",
    "    try:\n",
    "        import requests\n",
    "        response = requests.get(chrome_url)\n",
    "        \n",
    "        installer_path = \"chrome_installer.exe\"\n",
    "        with open(installer_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        \n",
    "        # Lancer l'installation\n",
    "        subprocess.run([installer_path, '/silent', '/install'], check=True)\n",
    "        os.remove(installer_path)\n",
    "        \n",
    "        print(\"âœ… Chrome installÃ© avec succÃ¨s !\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Erreur installation: {e}\")\n",
    "        print(\"ğŸ”— Installez manuellement: https://www.google.com/chrome/\")\n",
    "\n",
    "def create_fixed_driver(chrome_path):\n",
    "    \"\"\"CrÃ©e un driver avec le bon chemin Chrome\"\"\"\n",
    "    \n",
    "    print(\"ğŸ”§ Configuration driver corrigÃ©...\")\n",
    "    \n",
    "    try:\n",
    "        import undetected_chromedriver as uc\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        \n",
    "        # Options Chrome corrigÃ©es\n",
    "        options = Options()\n",
    "        options.binary_location = str(chrome_path)  # FORCE STRING\n",
    "        options.add_argument('--no-sandbox')\n",
    "        options.add_argument('--disable-dev-shm-usage')\n",
    "        options.add_argument('--disable-gpu')\n",
    "        options.add_argument('--remote-debugging-port=9222')\n",
    "        options.add_argument('--disable-web-security')\n",
    "        options.add_argument('--disable-features=VizDisplayCompositor')\n",
    "        \n",
    "        # Driver undetected avec options corrigÃ©es\n",
    "        driver = uc.Chrome(\n",
    "            options=options,\n",
    "            driver_executable_path=None,  # Auto-detection\n",
    "            browser_executable_path=chrome_path,  # Path explicite\n",
    "            version_main=None,  # Auto-detect version\n",
    "            headless=False\n",
    "        )\n",
    "        \n",
    "        print(\"âœ… Driver crÃ©Ã© avec succÃ¨s !\")\n",
    "        return driver\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Erreur driver: {e}\")\n",
    "        return None\n",
    "\n",
    "# EXÃ‰CUTER LA RÃ‰PARATION\n",
    "fixed_driver = fix_chrome_binary()\n",
    "\n",
    "if fixed_driver:\n",
    "    print(\"ğŸ‰ TEST RAPIDE...\")\n",
    "    try:\n",
    "        fixed_driver.get(\"https://httpbin.org/headers\")\n",
    "        print(\"âœ… Navigation fonctionne !\")\n",
    "        fixed_driver.quit()\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Test Ã©chouÃ©: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f908d2be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 16:10:56,654 - INFO - ğŸ§ª DÃ©marrage des tests du scraper...\n",
      "2025-06-27 16:10:56,655 - INFO - ğŸ”§ Initialisation du scraper anti-dÃ©tection...\n",
      "2025-06-27 16:10:56,655 - INFO - ğŸ­ User Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0...\n",
      "2025-06-27 16:10:56,655 - INFO - ğŸ”§ Initialisation du scraper anti-dÃ©tection...\n",
      "2025-06-27 16:10:56,655 - INFO - ğŸ­ User Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0...\n",
      "2025-06-27 16:10:58,548 - INFO - patching driver executable C:\\Users\\Yann\\appdata\\roaming\\undetected_chromedriver\\undetected_chromedriver.exe\n",
      "2025-06-27 16:10:58,548 - INFO - patching driver executable C:\\Users\\Yann\\appdata\\roaming\\undetected_chromedriver\\undetected_chromedriver.exe\n",
      "2025-06-27 16:10:59,610 - ERROR - âŒ Erreur lors de la configuration du driver: Message: invalid argument: cannot parse capability: goog:chromeOptions\n",
      "from invalid argument: unrecognized chrome option: excludeSwitches\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x0x384493+62419]\n",
      "\tGetHandleVerifier [0x0x3844d4+62484]\n",
      "\t(No symbol) [0x0x1c2133]\n",
      "\t(No symbol) [0x0x1e9723]\n",
      "\t(No symbol) [0x0x1eaeb0]\n",
      "\t(No symbol) [0x0x1e5fea]\n",
      "\t(No symbol) [0x0x239832]\n",
      "\t(No symbol) [0x0x23931c]\n",
      "\t(No symbol) [0x0x23aa20]\n",
      "\t(No symbol) [0x0x23a82a]\n",
      "\t(No symbol) [0x0x22f266]\n",
      "\t(No symbol) [0x0x1fe852]\n",
      "\t(No symbol) [0x0x1ff6f4]\n",
      "\tGetHandleVerifier [0x0x5f4773+2619059]\n",
      "\tGetHandleVerifier [0x0x5efb8a+2599626]\n",
      "\tGetHandleVerifier [0x0x3ab03a+221050]\n",
      "\tGetHandleVerifier [0x0x39b2b8+156152]\n",
      "\tGetHandleVerifier [0x0x3a1c6d+183213]\n",
      "\tGetHandleVerifier [0x0x38c378+94904]\n",
      "\tGetHandleVerifier [0x0x38c502+95298]\n",
      "\tGetHandleVerifier [0x0x37765a+9626]\n",
      "\tBaseThreadInitThunk [0x0x76775d49+25]\n",
      "\tRtlInitializeExceptionChain [0x0x778ed09b+107]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x0x778ed021+561]\n",
      "\n",
      "2025-06-27 16:10:59,611 - ERROR - âŒ Erreur durant les tests: Message: invalid argument: cannot parse capability: goog:chromeOptions\n",
      "from invalid argument: unrecognized chrome option: excludeSwitches\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x0x384493+62419]\n",
      "\tGetHandleVerifier [0x0x3844d4+62484]\n",
      "\t(No symbol) [0x0x1c2133]\n",
      "\t(No symbol) [0x0x1e9723]\n",
      "\t(No symbol) [0x0x1eaeb0]\n",
      "\t(No symbol) [0x0x1e5fea]\n",
      "\t(No symbol) [0x0x239832]\n",
      "\t(No symbol) [0x0x23931c]\n",
      "\t(No symbol) [0x0x23aa20]\n",
      "\t(No symbol) [0x0x23a82a]\n",
      "\t(No symbol) [0x0x22f266]\n",
      "\t(No symbol) [0x0x1fe852]\n",
      "\t(No symbol) [0x0x1ff6f4]\n",
      "\tGetHandleVerifier [0x0x5f4773+2619059]\n",
      "\tGetHandleVerifier [0x0x5efb8a+2599626]\n",
      "\tGetHandleVerifier [0x0x3ab03a+221050]\n",
      "\tGetHandleVerifier [0x0x39b2b8+156152]\n",
      "\tGetHandleVerifier [0x0x3a1c6d+183213]\n",
      "\tGetHandleVerifier [0x0x38c378+94904]\n",
      "\tGetHandleVerifier [0x0x38c502+95298]\n",
      "\tGetHandleVerifier [0x0x37765a+9626]\n",
      "\tBaseThreadInitThunk [0x0x76775d49+25]\n",
      "\tRtlInitializeExceptionChain [0x0x778ed09b+107]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x0x778ed021+561]\n",
      "\n",
      "2025-06-27 16:10:59,610 - ERROR - âŒ Erreur lors de la configuration du driver: Message: invalid argument: cannot parse capability: goog:chromeOptions\n",
      "from invalid argument: unrecognized chrome option: excludeSwitches\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x0x384493+62419]\n",
      "\tGetHandleVerifier [0x0x3844d4+62484]\n",
      "\t(No symbol) [0x0x1c2133]\n",
      "\t(No symbol) [0x0x1e9723]\n",
      "\t(No symbol) [0x0x1eaeb0]\n",
      "\t(No symbol) [0x0x1e5fea]\n",
      "\t(No symbol) [0x0x239832]\n",
      "\t(No symbol) [0x0x23931c]\n",
      "\t(No symbol) [0x0x23aa20]\n",
      "\t(No symbol) [0x0x23a82a]\n",
      "\t(No symbol) [0x0x22f266]\n",
      "\t(No symbol) [0x0x1fe852]\n",
      "\t(No symbol) [0x0x1ff6f4]\n",
      "\tGetHandleVerifier [0x0x5f4773+2619059]\n",
      "\tGetHandleVerifier [0x0x5efb8a+2599626]\n",
      "\tGetHandleVerifier [0x0x3ab03a+221050]\n",
      "\tGetHandleVerifier [0x0x39b2b8+156152]\n",
      "\tGetHandleVerifier [0x0x3a1c6d+183213]\n",
      "\tGetHandleVerifier [0x0x38c378+94904]\n",
      "\tGetHandleVerifier [0x0x38c502+95298]\n",
      "\tGetHandleVerifier [0x0x37765a+9626]\n",
      "\tBaseThreadInitThunk [0x0x76775d49+25]\n",
      "\tRtlInitializeExceptionChain [0x0x778ed09b+107]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x0x778ed021+561]\n",
      "\n",
      "2025-06-27 16:10:59,611 - ERROR - âŒ Erreur durant les tests: Message: invalid argument: cannot parse capability: goog:chromeOptions\n",
      "from invalid argument: unrecognized chrome option: excludeSwitches\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x0x384493+62419]\n",
      "\tGetHandleVerifier [0x0x3844d4+62484]\n",
      "\t(No symbol) [0x0x1c2133]\n",
      "\t(No symbol) [0x0x1e9723]\n",
      "\t(No symbol) [0x0x1eaeb0]\n",
      "\t(No symbol) [0x0x1e5fea]\n",
      "\t(No symbol) [0x0x239832]\n",
      "\t(No symbol) [0x0x23931c]\n",
      "\t(No symbol) [0x0x23aa20]\n",
      "\t(No symbol) [0x0x23a82a]\n",
      "\t(No symbol) [0x0x22f266]\n",
      "\t(No symbol) [0x0x1fe852]\n",
      "\t(No symbol) [0x0x1ff6f4]\n",
      "\tGetHandleVerifier [0x0x5f4773+2619059]\n",
      "\tGetHandleVerifier [0x0x5efb8a+2599626]\n",
      "\tGetHandleVerifier [0x0x3ab03a+221050]\n",
      "\tGetHandleVerifier [0x0x39b2b8+156152]\n",
      "\tGetHandleVerifier [0x0x3a1c6d+183213]\n",
      "\tGetHandleVerifier [0x0x38c378+94904]\n",
      "\tGetHandleVerifier [0x0x38c502+95298]\n",
      "\tGetHandleVerifier [0x0x37765a+9626]\n",
      "\tBaseThreadInitThunk [0x0x76775d49+25]\n",
      "\tRtlInitializeExceptionChain [0x0x778ed09b+107]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x0x778ed021+561]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_results = test_marketplace_scraper()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bac6014",
   "metadata": {},
   "source": [
    "## ğŸ”§ Configurations AvancÃ©es\n",
    "\n",
    "### Rotation de Proxies\n",
    "Pour ajouter des proxies et Ã©viter les bans IP :\n",
    "\n",
    "```python\n",
    "PROXY_LIST = [\n",
    "    \"http://username:password@proxy1.com:8080\",\n",
    "    \"http://username:password@proxy2.com:8080\", \n",
    "    \"socks5://proxy3.com:1080\"\n",
    "]\n",
    "```\n",
    "\n",
    "### Sites SupportÃ©s (Adaptables)\n",
    "- **E-commerce**: Amazon, eBay, Shopify stores\n",
    "- **Reviews**: Trustpilot, Google Reviews, Yelp\n",
    "- **Social Commerce**: Facebook Marketplace, Instagram Shopping\n",
    "\n",
    "### âš ï¸ ConsidÃ©rations LÃ©gales et Ã‰thiques\n",
    "1. **Respectez les robots.txt** des sites\n",
    "2. **Limitez la frÃ©quence** des requÃªtes\n",
    "3. **Utilisez des APIs officielles** quand disponibles\n",
    "4. **Respectez les ToS** des plateformes\n",
    "5. **Ne surchargez pas** les serveurs\n",
    "\n",
    "### ğŸ›¡ï¸ Protections ImplÃ©mentÃ©es\n",
    "- âœ… **User-Agent Rotation** - 5+ agents rÃ©alistes\n",
    "- âœ… **DÃ©lais Humains** - 1-3s entre actions\n",
    "- âœ… **Comportement Humain** - Mouvements souris, scroll\n",
    "- âœ… **Headers RÃ©alistes** - Accept, Language, etc.\n",
    "- âœ… **Selenium Stealth** - Ã‰vite la dÃ©tection webdriver\n",
    "- âœ… **Proxy Support** - Rotation d'IP\n",
    "- âœ… **Error Handling** - Retry automatique\n",
    "- âœ… **Session Management** - Cookies et state\n",
    "\n",
    "### ğŸ“Š DonnÃ©es RÃ©cupÃ©rÃ©es\n",
    "- **Produits**: Titre, prix, description, images, ratings\n",
    "- **Reviews**: Texte, note, date, nom reviewer, votes utiles\n",
    "- **MÃ©tadonnÃ©es**: Source, timestamp, user-agent, proxy utilisÃ©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "587a9ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš¨ Pour lancer le scraping, exÃ©cutez:\n",
      "   results = quick_scrape('smartphone')\n",
      "   results = quick_scrape('headphones')\n",
      "\n",
      "ğŸ’¡ Personnalisez CONFIG au-dessus pour adapter le comportement\n"
     ]
    }
   ],
   "source": [
    "# ğŸš€ LANCEMENT RAPIDE - Modifiez selon vos besoins\n",
    "\n",
    "# Configuration personnalisable\n",
    "CONFIG = {\n",
    "    'headless': False,          # True = invisible, False = visible (pour dÃ©buguer)\n",
    "    'use_proxy': False,         # True si vous avez configurÃ© des proxies\n",
    "    'max_products': 20,         # Nombre max de produits Ã  scraper\n",
    "    'max_reviews': 50,          # Nombre max de reviews par produit\n",
    "    'delay_min': 1,            # DÃ©lai minimum entre actions (secondes)\n",
    "    'delay_max': 3,            # DÃ©lai maximum entre actions (secondes)\n",
    "    'save_data': True          # Sauvegarder automatiquement les donnÃ©es\n",
    "}\n",
    "\n",
    "def quick_scrape(search_term: str = \"laptop\", company_name: str = \"amazon\"):\n",
    "    \"\"\"\n",
    "    Fonction de scraping rapide avec configuration personnalisable.\n",
    "    \n",
    "    Args:\n",
    "        search_term: Terme de recherche pour les produits\n",
    "        company_name: Nom d'entreprise pour les reviews Trustpilot\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ” DÃ©marrage du scraping pour: {search_term}\")\n",
    "    \n",
    "    # Initialiser le scraper avec la config\n",
    "    scraper = MarketplaceProductScraper(\n",
    "        headless=CONFIG['headless'], \n",
    "        use_proxy=CONFIG['use_proxy']\n",
    "    )\n",
    "    \n",
    "    all_data = {}\n",
    "    \n",
    "    try:\n",
    "        scraper.start_session()\n",
    "        \n",
    "        # Scraping des produits\n",
    "        print(\"ğŸ›ï¸ Scraping des produits...\")\n",
    "        products = scraper.scrape_amazon_style_products(\n",
    "            search_term, \n",
    "            max_pages=1\n",
    "        )\n",
    "        \n",
    "        if not products.empty and CONFIG['save_data']:\n",
    "            filepath = save_scraped_data(products, f\"products_{search_term}.csv\")\n",
    "            all_data['products'] = products\n",
    "            print(f\"ğŸ“ Produits sauvegardÃ©s: {len(products)} items\")\n",
    "        \n",
    "        # Scraping des reviews simulÃ©es\n",
    "        print(\"ğŸ“ Scraping des reviews...\")\n",
    "        reviews = scraper.scrape_product_reviews(\n",
    "            \"https://webscraper.io/test-sites/e-commerce/allinone\",\n",
    "            max_reviews=CONFIG['max_reviews']\n",
    "        )\n",
    "        \n",
    "        if not reviews.empty and CONFIG['save_data']:\n",
    "            filepath = save_scraped_data(reviews, f\"reviews_{search_term}.csv\")\n",
    "            all_data['reviews'] = reviews\n",
    "            print(f\"ğŸ“ Reviews sauvegardÃ©es: {len(reviews)} items\")\n",
    "        \n",
    "        # Affichage des rÃ©sultats\n",
    "        print(\"\\nğŸ“Š RÃ©sultats du scraping:\")\n",
    "        for data_type, df in all_data.items():\n",
    "            print(f\"   {data_type}: {len(df)} enregistrements\")\n",
    "            \n",
    "        return all_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Erreur: {e}\")\n",
    "        return None\n",
    "        \n",
    "    finally:\n",
    "        scraper.close_session()\n",
    "\n",
    "# ğŸ¯ EXÃ‰CUTION\n",
    "print(\"ğŸš¨ Pour lancer le scraping, exÃ©cutez:\")\n",
    "print(\"   results = quick_scrape('smartphone')\")\n",
    "print(\"   results = quick_scrape('headphones')\")\n",
    "print(\"\\nğŸ’¡ Personnalisez CONFIG au-dessus pour adapter le comportement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8846d2d4",
   "metadata": {},
   "source": [
    "## ğŸ¯ **SITES RÃ‰ELS vs DÃ‰MONSTRATION**\n",
    "\n",
    "### âŒ **Ce qui est actuellement en DEMO :**\n",
    "- `webscraper.io/test-sites` - Site de test pour apprendre\n",
    "- Reviews simulÃ©es avec `random.choice()` \n",
    "- DonnÃ©es gÃ©nÃ©rÃ©es alÃ©atoirement\n",
    "\n",
    "### âœ… **Ce qui est RÃ‰EL :**\n",
    "- Structure anti-dÃ©tection Selenium\n",
    "- Rotation User-Agent rÃ©elle\n",
    "- Support proxy fonctionnel\n",
    "- Gestion des dÃ©lais humains\n",
    "\n",
    "### ğŸš¨ **VRAIES IMPLÃ‰MENTATIONS ci-dessous :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "75734744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ›’ VRAI scraper Amazon avec vraies balises CSS crÃ©Ã© !\n"
     ]
    }
   ],
   "source": [
    "# ğŸ›’ VRAIE IMPLÃ‰MENTATION AMAZON - SÃ©lecteurs CSS rÃ©els\n",
    "class RealAmazonScraper(StealthMarketplaceScraper):\n",
    "    \"\"\"\n",
    "    Scraper pour le VRAI Amazon avec vraies balises CSS.\n",
    "    âš ï¸ ATTENTION: Utilisez avec modÃ©ration pour respecter les ToS d'Amazon\n",
    "    \"\"\"\n",
    "    \n",
    "    def scrape_real_amazon_products(self, search_term: str, max_pages: int = 2) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Scrape de vrais produits Amazon avec vraies balises.\n",
    "        \"\"\"\n",
    "        logger.info(f\"ğŸ›’ VRAI scraping Amazon pour: {search_term}\")\n",
    "        \n",
    "        if not self.driver:\n",
    "            self.start_session()\n",
    "        \n",
    "        products_data = []\n",
    "        \n",
    "        try:\n",
    "            for page in range(1, max_pages + 1):\n",
    "                # VRAIE URL Amazon avec pagination\n",
    "                url = f\"https://www.amazon.com/s?k={search_term}&page={page}\"\n",
    "                \n",
    "                if not self.get_page(url):\n",
    "                    continue\n",
    "                \n",
    "                # Attendre que les produits se chargent\n",
    "                time.sleep(3)\n",
    "                \n",
    "                # VRAIES balises CSS Amazon (mises Ã  jour 2024/2025)\n",
    "                product_containers = self.driver.find_elements(\n",
    "                    By.CSS_SELECTOR, \n",
    "                    \"[data-component-type='s-search-result']\"\n",
    "                )\n",
    "                \n",
    "                for i, container in enumerate(product_containers):\n",
    "                    try:\n",
    "                        # Titre du produit - VRAIE balise Amazon\n",
    "                        title_elem = container.find_element(\n",
    "                            By.CSS_SELECTOR, \n",
    "                            \"h2 a span, .a-size-mini span, .a-size-base-plus\"\n",
    "                        )\n",
    "                        title = title_elem.text.strip()\n",
    "                        \n",
    "                        # Prix - VRAIES balises Amazon\n",
    "                        try:\n",
    "                            price_elem = container.find_element(\n",
    "                                By.CSS_SELECTOR,\n",
    "                                \".a-price-whole, .a-offscreen\"\n",
    "                            )\n",
    "                            price_text = price_elem.text.strip()\n",
    "                            price = float(re.findall(r'[\\d.]+', price_text.replace(',', ''))[0])\n",
    "                        except:\n",
    "                            price = 0.0\n",
    "                        \n",
    "                        # Rating - VRAIE balise Amazon\n",
    "                        try:\n",
    "                            rating_elem = container.find_element(\n",
    "                                By.CSS_SELECTOR,\n",
    "                                \".a-icon-alt\"\n",
    "                            )\n",
    "                            rating_text = rating_elem.get_attribute(\"textContent\")\n",
    "                            rating = float(re.findall(r'[\\d.]+', rating_text)[0])\n",
    "                        except:\n",
    "                            rating = 0.0\n",
    "                        \n",
    "                        # Nombre de reviews - VRAIE balise Amazon\n",
    "                        try:\n",
    "                            reviews_elem = container.find_element(\n",
    "                                By.CSS_SELECTOR,\n",
    "                                \".a-size-base\"\n",
    "                            )\n",
    "                            reviews_text = reviews_elem.text\n",
    "                            num_reviews = int(re.findall(r'[\\d,]+', reviews_text.replace(',', ''))[0])\n",
    "                        except:\n",
    "                            num_reviews = 0\n",
    "                        \n",
    "                        # URL du produit - VRAIE balise Amazon\n",
    "                        try:\n",
    "                            product_link = container.find_element(\n",
    "                                By.CSS_SELECTOR,\n",
    "                                \"h2 a\"\n",
    "                            ).get_attribute(\"href\")\n",
    "                        except:\n",
    "                            product_link = \"\"\n",
    "                        \n",
    "                        # Image - VRAIE balise Amazon\n",
    "                        try:\n",
    "                            img_elem = container.find_element(\n",
    "                                By.CSS_SELECTOR,\n",
    "                                \".s-image\"\n",
    "                            )\n",
    "                            image_url = img_elem.get_attribute(\"src\")\n",
    "                        except:\n",
    "                            image_url = \"\"\n",
    "                        \n",
    "                        products_data.append({\n",
    "                            'product_id': f\"amazon_{search_term}_{page}_{i}\",\n",
    "                            'title': title,\n",
    "                            'price': price,\n",
    "                            'rating': rating,\n",
    "                            'num_reviews': num_reviews,\n",
    "                            'product_url': product_link,\n",
    "                            'image_url': image_url,\n",
    "                            'search_term': search_term,\n",
    "                            'page': page,\n",
    "                            'source': 'amazon.com',\n",
    "                            'scraped_at': datetime.now().isoformat(),\n",
    "                            'user_agent': self.driver.execute_script(\"return navigator.userAgent;\")[:50]\n",
    "                        })\n",
    "                        \n",
    "                        # Comportement humain\n",
    "                        if i % 5 == 0:\n",
    "                            self._simulate_human_behavior()\n",
    "                        \n",
    "                        self._human_like_delay(0.5, 1.5)\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"Erreur extraction produit {i}: {e}\")\n",
    "                        continue\n",
    "                \n",
    "                # DÃ©lai entre pages\n",
    "                self._human_like_delay(3, 6)\n",
    "            \n",
    "            logger.info(f\"âœ… {len(products_data)} vrais produits Amazon extraits\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"âŒ Erreur scraping Amazon: {e}\")\n",
    "        \n",
    "        return pd.DataFrame(products_data)\n",
    "    \n",
    "    def scrape_real_amazon_reviews(self, product_url: str, max_reviews: int = 50) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Scrape de vraies reviews Amazon avec vraies balises.\n",
    "        \"\"\"\n",
    "        logger.info(f\"ğŸ“ VRAIES reviews Amazon pour: {product_url}\")\n",
    "        \n",
    "        if not self.driver:\n",
    "            self.start_session()\n",
    "        \n",
    "        reviews_data = []\n",
    "        \n",
    "        try:\n",
    "            # Aller sur la page du produit\n",
    "            if not self.get_page(product_url):\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            # Cliquer sur \"Voir toutes les reviews\" - VRAIE balise Amazon\n",
    "            try:\n",
    "                reviews_link = self.driver.find_element(\n",
    "                    By.CSS_SELECTOR,\n",
    "                    \"[data-hook='see-all-reviews-link-foot'], .a-link-emphasis\"\n",
    "                )\n",
    "                reviews_link.click()\n",
    "                time.sleep(3)\n",
    "            except:\n",
    "                logger.warning(\"Impossible de trouver le lien vers les reviews\")\n",
    "            \n",
    "            page = 1\n",
    "            while len(reviews_data) < max_reviews and page <= 5:  # Max 5 pages\n",
    "                \n",
    "                # VRAIES balises CSS Amazon pour les reviews\n",
    "                review_containers = self.driver.find_elements(\n",
    "                    By.CSS_SELECTOR,\n",
    "                    \"[data-hook='review']\"\n",
    "                )\n",
    "                \n",
    "                for container in review_containers:\n",
    "                    if len(reviews_data) >= max_reviews:\n",
    "                        break\n",
    "                        \n",
    "                    try:\n",
    "                        # Texte de la review - VRAIE balise Amazon\n",
    "                        review_text_elem = container.find_element(\n",
    "                            By.CSS_SELECTOR,\n",
    "                            \"[data-hook='review-body'] span\"\n",
    "                        )\n",
    "                        review_text = review_text_elem.text.strip()\n",
    "                        \n",
    "                        # Rating - VRAIE balise Amazon\n",
    "                        rating_elem = container.find_element(\n",
    "                            By.CSS_SELECTOR,\n",
    "                            \".a-icon-alt\"\n",
    "                        )\n",
    "                        rating_text = rating_elem.get_attribute(\"textContent\")\n",
    "                        rating = float(re.findall(r'[\\d.]+', rating_text)[0])\n",
    "                        \n",
    "                        # Nom du reviewer - VRAIE balise Amazon\n",
    "                        try:\n",
    "                            reviewer_elem = container.find_element(\n",
    "                                By.CSS_SELECTOR,\n",
    "                                \".a-profile-name\"\n",
    "                            )\n",
    "                            reviewer_name = reviewer_elem.text.strip()\n",
    "                        except:\n",
    "                            reviewer_name = \"Anonymous\"\n",
    "                        \n",
    "                        # Date - VRAIE balise Amazon\n",
    "                        try:\n",
    "                            date_elem = container.find_element(\n",
    "                                By.CSS_SELECTOR,\n",
    "                                \"[data-hook='review-date']\"\n",
    "                            )\n",
    "                            date_text = date_elem.text\n",
    "                            # Extraire la date (format: \"Reviewed in the United States on January 1, 2024\")\n",
    "                            date_match = re.search(r'(\\w+ \\d+, \\d{4})', date_text)\n",
    "                            if date_match:\n",
    "                                review_date = datetime.strptime(date_match.group(1), '%B %d, %Y').strftime('%Y-%m-%d')\n",
    "                            else:\n",
    "                                review_date = datetime.now().strftime('%Y-%m-%d')\n",
    "                        except:\n",
    "                            review_date = datetime.now().strftime('%Y-%m-%d')\n",
    "                        \n",
    "                        # Votes utiles - VRAIE balise Amazon\n",
    "                        try:\n",
    "                            helpful_elem = container.find_element(\n",
    "                                By.CSS_SELECTOR,\n",
    "                                \"[data-hook='helpful-vote-statement']\"\n",
    "                            )\n",
    "                            helpful_text = helpful_elem.text\n",
    "                            helpful_votes = int(re.findall(r'\\d+', helpful_text)[0]) if re.findall(r'\\d+', helpful_text) else 0\n",
    "                        except:\n",
    "                            helpful_votes = 0\n",
    "                        \n",
    "                        # Achat vÃ©rifiÃ© - VRAIE balise Amazon\n",
    "                        try:\n",
    "                            verified_elem = container.find_element(\n",
    "                                By.CSS_SELECTOR,\n",
    "                                \"[data-hook='avp-badge']\"\n",
    "                            )\n",
    "                            verified_purchase = \"Verified Purchase\" in verified_elem.text\n",
    "                        except:\n",
    "                            verified_purchase = False\n",
    "                        \n",
    "                        reviews_data.append({\n",
    "                            'review_id': f\"amazon_review_{len(reviews_data)}\",\n",
    "                            'product_url': product_url,\n",
    "                            'reviewer_name': reviewer_name,\n",
    "                            'review_text': review_text,\n",
    "                            'rating': rating,\n",
    "                            'review_date': review_date,\n",
    "                            'helpful_votes': helpful_votes,\n",
    "                            'verified_purchase': verified_purchase,\n",
    "                            'source': 'amazon.com',\n",
    "                            'scraped_at': datetime.now().isoformat()\n",
    "                        })\n",
    "                        \n",
    "                        self._human_like_delay(0.3, 1.0)\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"Erreur extraction review: {e}\")\n",
    "                        continue\n",
    "                \n",
    "                # Essayer d'aller Ã  la page suivante\n",
    "                try:\n",
    "                    next_button = self.driver.find_element(\n",
    "                        By.CSS_SELECTOR,\n",
    "                        \"li.a-last a\"\n",
    "                    )\n",
    "                    next_button.click()\n",
    "                    self._human_like_delay(3, 5)\n",
    "                    page += 1\n",
    "                except:\n",
    "                    break  # Plus de pages\n",
    "            \n",
    "            logger.info(f\"âœ… {len(reviews_data)} vraies reviews Amazon extraites\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"âŒ Erreur scraping reviews Amazon: {e}\")\n",
    "        \n",
    "        return pd.DataFrame(reviews_data)\n",
    "\n",
    "print(\"ğŸ›’ VRAI scraper Amazon avec vraies balises CSS crÃ©Ã© !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "761e380a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â­ VRAIS scrapers eBay et Trustpilot avec vraies balises CSS crÃ©Ã©s !\n"
     ]
    }
   ],
   "source": [
    "# ğŸª VRAIE IMPLÃ‰MENTATION EBAY - SÃ©lecteurs CSS rÃ©els\n",
    "class RealEbayScraper(StealthMarketplaceScraper):\n",
    "    \"\"\"Scraper pour le VRAI eBay avec vraies balises.\"\"\"\n",
    "    \n",
    "    def scrape_real_ebay_products(self, search_term: str, max_pages: int = 2) -> pd.DataFrame:\n",
    "        \"\"\"Scrape de vrais produits eBay.\"\"\"\n",
    "        logger.info(f\"ğŸª VRAI scraping eBay pour: {search_term}\")\n",
    "        \n",
    "        if not self.driver:\n",
    "            self.start_session()\n",
    "        \n",
    "        products_data = []\n",
    "        \n",
    "        try:\n",
    "            for page in range(1, max_pages + 1):\n",
    "                # VRAIE URL eBay\n",
    "                url = f\"https://www.ebay.com/sch/i.html?_nkw={search_term}&_pgn={page}\"\n",
    "                \n",
    "                if not self.get_page(url):\n",
    "                    continue\n",
    "                \n",
    "                # VRAIES balises CSS eBay\n",
    "                items = self.driver.find_elements(By.CSS_SELECTOR, \".s-item\")\n",
    "                \n",
    "                for i, item in enumerate(items):\n",
    "                    try:\n",
    "                        # Titre - VRAIE balise eBay\n",
    "                        title_elem = item.find_element(By.CSS_SELECTOR, \".s-item__title\")\n",
    "                        title = title_elem.text.strip()\n",
    "                        \n",
    "                        # Prix - VRAIE balise eBay\n",
    "                        try:\n",
    "                            price_elem = item.find_element(By.CSS_SELECTOR, \".s-item__price\")\n",
    "                            price_text = price_elem.text.strip()\n",
    "                            price = float(re.findall(r'[\\d.]+', price_text.replace(',', ''))[0])\n",
    "                        except:\n",
    "                            price = 0.0\n",
    "                        \n",
    "                        # Condition - VRAIE balise eBay\n",
    "                        try:\n",
    "                            condition_elem = item.find_element(By.CSS_SELECTOR, \".SECONDARY_INFO\")\n",
    "                            condition = condition_elem.text.strip()\n",
    "                        except:\n",
    "                            condition = \"Unknown\"\n",
    "                        \n",
    "                        # Livraison - VRAIE balise eBay\n",
    "                        try:\n",
    "                            shipping_elem = item.find_element(By.CSS_SELECTOR, \".s-item__shipping\")\n",
    "                            shipping = shipping_elem.text.strip()\n",
    "                        except:\n",
    "                            shipping = \"\"\n",
    "                        \n",
    "                        # URL - VRAIE balise eBay\n",
    "                        try:\n",
    "                            url_elem = item.find_element(By.CSS_SELECTOR, \".s-item__link\")\n",
    "                            item_url = url_elem.get_attribute(\"href\")\n",
    "                        except:\n",
    "                            item_url = \"\"\n",
    "                        \n",
    "                        products_data.append({\n",
    "                            'product_id': f\"ebay_{search_term}_{page}_{i}\",\n",
    "                            'title': title,\n",
    "                            'price': price,\n",
    "                            'condition': condition,\n",
    "                            'shipping': shipping,\n",
    "                            'product_url': item_url,\n",
    "                            'search_term': search_term,\n",
    "                            'source': 'ebay.com',\n",
    "                            'scraped_at': datetime.now().isoformat()\n",
    "                        })\n",
    "                        \n",
    "                        self._human_like_delay(0.5, 1.5)\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        continue\n",
    "                \n",
    "                self._human_like_delay(3, 6)\n",
    "            \n",
    "            logger.info(f\"âœ… {len(products_data)} vrais produits eBay extraits\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"âŒ Erreur scraping eBay: {e}\")\n",
    "        \n",
    "        return pd.DataFrame(products_data)\n",
    "\n",
    "\n",
    "# â­ VRAIE IMPLÃ‰MENTATION TRUSTPILOT - SÃ©lecteurs CSS rÃ©els\n",
    "class RealTrustpilotScraper(StealthMarketplaceScraper):\n",
    "    \"\"\"Scraper pour le VRAI Trustpilot avec vraies balises.\"\"\"\n",
    "    \n",
    "    def scrape_real_trustpilot_reviews(self, company_name: str, max_reviews: int = 100) -> pd.DataFrame:\n",
    "        \"\"\"Scrape de vraies reviews Trustpilot.\"\"\"\n",
    "        logger.info(f\"â­ VRAI scraping Trustpilot pour: {company_name}\")\n",
    "        \n",
    "        if not self.driver:\n",
    "            self.start_session()\n",
    "        \n",
    "        reviews_data = []\n",
    "        \n",
    "        try:\n",
    "            # VRAIE URL Trustpilot\n",
    "            url = f\"https://www.trustpilot.com/review/{company_name.lower().replace(' ', '-')}\"\n",
    "            \n",
    "            if not self.get_page(url):\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            # GÃ©rer les cookies si nÃ©cessaire\n",
    "            try:\n",
    "                cookie_button = self.driver.find_element(By.CSS_SELECTOR, \"#onetrust-accept-btn-handler\")\n",
    "                cookie_button.click()\n",
    "                time.sleep(2)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            page = 1\n",
    "            while len(reviews_data) < max_reviews and page <= 10:\n",
    "                \n",
    "                # VRAIES balises CSS Trustpilot (mises Ã  jour 2024/2025)\n",
    "                review_cards = self.driver.find_elements(\n",
    "                    By.CSS_SELECTOR, \n",
    "                    \"article[data-service-review-card-paper]\"\n",
    "                )\n",
    "                \n",
    "                for card in review_cards:\n",
    "                    if len(reviews_data) >= max_reviews:\n",
    "                        break\n",
    "                        \n",
    "                    try:\n",
    "                        # Texte de la review - VRAIE balise Trustpilot\n",
    "                        text_elem = card.find_element(\n",
    "                            By.CSS_SELECTOR, \n",
    "                            \"[data-service-review-text-typography='true']\"\n",
    "                        )\n",
    "                        review_text = text_elem.text.strip()\n",
    "                        \n",
    "                        # Rating - VRAIE balise Trustpilot\n",
    "                        rating_elem = card.find_element(\n",
    "                            By.CSS_SELECTOR,\n",
    "                            \"[data-service-review-rating]\"\n",
    "                        )\n",
    "                        # Compter les Ã©toiles pleines\n",
    "                        filled_stars = rating_elem.find_elements(\n",
    "                            By.CSS_SELECTOR,\n",
    "                            \"svg[data-star-fill='true']\"\n",
    "                        )\n",
    "                        rating = len(filled_stars)\n",
    "                        \n",
    "                        # Nom du reviewer - VRAIE balise Trustpilot\n",
    "                        try:\n",
    "                            name_elem = card.find_element(\n",
    "                                By.CSS_SELECTOR,\n",
    "                                \"[data-consumer-name-typography='true']\"\n",
    "                            )\n",
    "                            reviewer_name = name_elem.text.strip()\n",
    "                        except:\n",
    "                            reviewer_name = \"Anonymous\"\n",
    "                        \n",
    "                        # Date - VRAIE balise Trustpilot\n",
    "                        try:\n",
    "                            date_elem = card.find_element(By.CSS_SELECTOR, \"time\")\n",
    "                            review_date = date_elem.get_attribute(\"datetime\")[:10]\n",
    "                        except:\n",
    "                            review_date = datetime.now().strftime('%Y-%m-%d')\n",
    "                        \n",
    "                        # Titre de la review - VRAIE balise Trustpilot\n",
    "                        try:\n",
    "                            title_elem = card.find_element(\n",
    "                                By.CSS_SELECTOR,\n",
    "                                \"[data-service-review-title-typography='true']\"\n",
    "                            )\n",
    "                            review_title = title_elem.text.strip()\n",
    "                        except:\n",
    "                            review_title = \"\"\n",
    "                        \n",
    "                        # Pays du reviewer - VRAIE balise Trustpilot\n",
    "                        try:\n",
    "                            country_elem = card.find_element(\n",
    "                                By.CSS_SELECTOR,\n",
    "                                \"[data-consumer-country-typography='true']\"\n",
    "                            )\n",
    "                            country = country_elem.text.strip()\n",
    "                        except:\n",
    "                            country = \"\"\n",
    "                        \n",
    "                        reviews_data.append({\n",
    "                            'review_id': f\"trustpilot_{company_name}_{len(reviews_data)}\",\n",
    "                            'company': company_name,\n",
    "                            'reviewer_name': reviewer_name,\n",
    "                            'review_title': review_title,\n",
    "                            'review_text': review_text,\n",
    "                            'rating': rating,\n",
    "                            'review_date': review_date,\n",
    "                            'country': country,\n",
    "                            'source': 'trustpilot.com',\n",
    "                            'scraped_at': datetime.now().isoformat()\n",
    "                        })\n",
    "                        \n",
    "                        self._human_like_delay(0.3, 1.0)\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"Erreur extraction review Trustpilot: {e}\")\n",
    "                        continue\n",
    "                \n",
    "                # Essayer d'aller Ã  la page suivante\n",
    "                try:\n",
    "                    next_button = self.driver.find_element(\n",
    "                        By.CSS_SELECTOR,\n",
    "                        \"a[name='pagination-button-next']\"\n",
    "                    )\n",
    "                    next_button.click()\n",
    "                    self._human_like_delay(3, 5)\n",
    "                    page += 1\n",
    "                except:\n",
    "                    break\n",
    "            \n",
    "            logger.info(f\"âœ… {len(reviews_data)} vraies reviews Trustpilot extraites\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"âŒ Erreur scraping Trustpilot: {e}\")\n",
    "        \n",
    "        return pd.DataFrame(reviews_data)\n",
    "\n",
    "print(\"â­ VRAIS scrapers eBay et Trustpilot avec vraies balises CSS crÃ©Ã©s !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5f6f939d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ Vrais scrapers configurÃ©s !\n",
      "âš ï¸  UTILISEZ AVEC PRÃ‰CAUTION et RESPECT des ToS\n",
      "ğŸš€ Pour tester: test_real_scrapers()\n"
     ]
    }
   ],
   "source": [
    "# ğŸš¨ TEST DES VRAIS SCRAPERS - Utilisation avec prÃ©caution\n",
    "def test_real_scrapers():\n",
    "    \"\"\"\n",
    "    Test des vrais scrapers sur de vrais sites.\n",
    "    âš ï¸ ATTENTION: Ã€ utiliser avec modÃ©ration et respect des ToS\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"ğŸš¨ AVERTISSEMENT: Vous allez scraper de VRAIS sites !\")\n",
    "    print(\"ğŸ“‹ Assurez-vous de:\")\n",
    "    print(\"   âœ… Respecter les robots.txt\")\n",
    "    print(\"   âœ… Limiter la frÃ©quence des requÃªtes\")\n",
    "    print(\"   âœ… Utiliser des proxies si nÃ©cessaire\")\n",
    "    print(\"   âœ… Ne pas surcharger les serveurs\")\n",
    "    \n",
    "    choice = input(\"Continuer ? (oui/non): \").lower()\n",
    "    if choice not in ['oui', 'yes', 'y', 'o']:\n",
    "        print(\"âŒ Test annulÃ©\")\n",
    "        return\n",
    "    \n",
    "    all_results = {}\n",
    "    \n",
    "    try:\n",
    "        # Test Amazon (COMMENTÃ‰ par sÃ©curitÃ©)\n",
    "        print(\"\\nğŸ›’ Test Amazon scraper...\")\n",
    "        amazon_scraper = RealAmazonScraper(headless=False, use_proxy=True)\n",
    "        amazon_scraper.start_session()\n",
    "        amazon_products = amazon_scraper.scrape_real_amazon_products(\"laptop\", max_pages=1)\n",
    "        all_results['amazon_products'] = amazon_products\n",
    "        amazon_scraper.close_session()\n",
    "        print(\"âš ï¸ Amazon scraper commentÃ© pour sÃ©curitÃ© - dÃ©commentez si nÃ©cessaire\")\n",
    "        \n",
    "        # Test eBay\n",
    "        print(\"\\nğŸª Test eBay scraper...\")\n",
    "        ebay_scraper = RealEbayScraper(headless=False, use_proxy=False)\n",
    "        ebay_scraper.start_session()\n",
    "        ebay_products = ebay_scraper.scrape_real_ebay_products(\"smartphone\", max_pages=1)\n",
    "        all_results['ebay_products'] = ebay_products\n",
    "        ebay_scraper.close_session()\n",
    "        \n",
    "        # Test Trustpilot\n",
    "        print(\"\\nâ­ Test Trustpilot scraper...\")\n",
    "        trustpilot_scraper = RealTrustpilotScraper(headless=False, use_proxy=False)\n",
    "        trustpilot_scraper.start_session()\n",
    "        trustpilot_reviews = trustpilot_scraper.scrape_real_trustpilot_reviews(\"amazon\", max_reviews=10)\n",
    "        all_results['trustpilot_reviews'] = trustpilot_reviews\n",
    "        trustpilot_scraper.close_session()\n",
    "        \n",
    "        # Sauvegarder les rÃ©sultats\n",
    "        for data_type, df in all_results.items():\n",
    "            if not df.empty:\n",
    "                save_scraped_data(df, f\"real_{data_type}.csv\")\n",
    "                analyze_scraped_data(df)\n",
    "        \n",
    "        print(\"\\nâœ… Tests des vrais scrapers terminÃ©s !\")\n",
    "        return all_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Erreur durant les tests rÃ©els: {e}\")\n",
    "        return None\n",
    "\n",
    "# Configuration pour scrapers rÃ©els\n",
    "REAL_SCRAPER_CONFIG = {\n",
    "    'use_proxy': True,           # RECOMMANDÃ‰ pour vrais sites\n",
    "    'headless': True,            # Mode invisible\n",
    "    'delay_min': 2,              # DÃ©lais plus longs\n",
    "    'delay_max': 5,              # Pour Ã©viter la dÃ©tection\n",
    "    'max_retries': 3,            # Retry en cas d'Ã©chec\n",
    "    'respect_robots_txt': True   # Respecter robots.txt\n",
    "}\n",
    "\n",
    "print(\"ğŸ¯ Vrais scrapers configurÃ©s !\")\n",
    "print(\"âš ï¸  UTILISEZ AVEC PRÃ‰CAUTION et RESPECT des ToS\")\n",
    "print(\"ğŸš€ Pour tester: test_real_scrapers()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c6315339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš¨ AVERTISSEMENT: Vous allez scraper de VRAIS sites !\n",
      "ğŸ“‹ Assurez-vous de:\n",
      "   âœ… Respecter les robots.txt\n",
      "   âœ… Limiter la frÃ©quence des requÃªtes\n",
      "   âœ… Utiliser des proxies si nÃ©cessaire\n",
      "   âœ… Ne pas surcharger les serveurs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 16:12:37,871 - INFO - ğŸ”§ Initialisation du scraper anti-dÃ©tection...\n",
      "2025-06-27 16:12:37,871 - INFO - ğŸ­ User Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) Ap...\n",
      "2025-06-27 16:12:37,871 - INFO - ğŸ­ User Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) Ap...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ›’ Test Amazon scraper...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 16:12:39,553 - INFO - patching driver executable C:\\Users\\Yann\\appdata\\roaming\\undetected_chromedriver\\undetected_chromedriver.exe\n",
      "2025-06-27 16:12:40,842 - ERROR - âŒ Erreur lors de la configuration du driver: Message: invalid argument: cannot parse capability: goog:chromeOptions\n",
      "from invalid argument: unrecognized chrome option: excludeSwitches\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x0x344493+62419]\n",
      "\tGetHandleVerifier [0x0x3444d4+62484]\n",
      "\t(No symbol) [0x0x182133]\n",
      "\t(No symbol) [0x0x1a9723]\n",
      "\t(No symbol) [0x0x1aaeb0]\n",
      "\t(No symbol) [0x0x1a5fea]\n",
      "\t(No symbol) [0x0x1f9832]\n",
      "\t(No symbol) [0x0x1f931c]\n",
      "\t(No symbol) [0x0x1faa20]\n",
      "\t(No symbol) [0x0x1fa82a]\n",
      "\t(No symbol) [0x0x1ef266]\n",
      "\t(No symbol) [0x0x1be852]\n",
      "\t(No symbol) [0x0x1bf6f4]\n",
      "\tGetHandleVerifier [0x0x5b4773+2619059]\n",
      "\tGetHandleVerifier [0x0x5afb8a+2599626]\n",
      "\tGetHandleVerifier [0x0x36b03a+221050]\n",
      "\tGetHandleVerifier [0x0x35b2b8+156152]\n",
      "\tGetHandleVerifier [0x0x361c6d+183213]\n",
      "\tGetHandleVerifier [0x0x34c378+94904]\n",
      "\tGetHandleVerifier [0x0x34c502+95298]\n",
      "\tGetHandleVerifier [0x0x33765a+9626]\n",
      "\tBaseThreadInitThunk [0x0x76775d49+25]\n",
      "\tRtlInitializeExceptionChain [0x0x778ed09b+107]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x0x778ed021+561]\n",
      "\t(No symbol) [0x0]\n",
      "\n",
      "2025-06-27 16:12:40,842 - ERROR - âŒ Erreur lors de la configuration du driver: Message: invalid argument: cannot parse capability: goog:chromeOptions\n",
      "from invalid argument: unrecognized chrome option: excludeSwitches\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x0x344493+62419]\n",
      "\tGetHandleVerifier [0x0x3444d4+62484]\n",
      "\t(No symbol) [0x0x182133]\n",
      "\t(No symbol) [0x0x1a9723]\n",
      "\t(No symbol) [0x0x1aaeb0]\n",
      "\t(No symbol) [0x0x1a5fea]\n",
      "\t(No symbol) [0x0x1f9832]\n",
      "\t(No symbol) [0x0x1f931c]\n",
      "\t(No symbol) [0x0x1faa20]\n",
      "\t(No symbol) [0x0x1fa82a]\n",
      "\t(No symbol) [0x0x1ef266]\n",
      "\t(No symbol) [0x0x1be852]\n",
      "\t(No symbol) [0x0x1bf6f4]\n",
      "\tGetHandleVerifier [0x0x5b4773+2619059]\n",
      "\tGetHandleVerifier [0x0x5afb8a+2599626]\n",
      "\tGetHandleVerifier [0x0x36b03a+221050]\n",
      "\tGetHandleVerifier [0x0x35b2b8+156152]\n",
      "\tGetHandleVerifier [0x0x361c6d+183213]\n",
      "\tGetHandleVerifier [0x0x34c378+94904]\n",
      "\tGetHandleVerifier [0x0x34c502+95298]\n",
      "\tGetHandleVerifier [0x0x33765a+9626]\n",
      "\tBaseThreadInitThunk [0x0x76775d49+25]\n",
      "\tRtlInitializeExceptionChain [0x0x778ed09b+107]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x0x778ed021+561]\n",
      "\t(No symbol) [0x0]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Erreur durant les tests rÃ©els: Message: invalid argument: cannot parse capability: goog:chromeOptions\n",
      "from invalid argument: unrecognized chrome option: excludeSwitches\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x0x344493+62419]\n",
      "\tGetHandleVerifier [0x0x3444d4+62484]\n",
      "\t(No symbol) [0x0x182133]\n",
      "\t(No symbol) [0x0x1a9723]\n",
      "\t(No symbol) [0x0x1aaeb0]\n",
      "\t(No symbol) [0x0x1a5fea]\n",
      "\t(No symbol) [0x0x1f9832]\n",
      "\t(No symbol) [0x0x1f931c]\n",
      "\t(No symbol) [0x0x1faa20]\n",
      "\t(No symbol) [0x0x1fa82a]\n",
      "\t(No symbol) [0x0x1ef266]\n",
      "\t(No symbol) [0x0x1be852]\n",
      "\t(No symbol) [0x0x1bf6f4]\n",
      "\tGetHandleVerifier [0x0x5b4773+2619059]\n",
      "\tGetHandleVerifier [0x0x5afb8a+2599626]\n",
      "\tGetHandleVerifier [0x0x36b03a+221050]\n",
      "\tGetHandleVerifier [0x0x35b2b8+156152]\n",
      "\tGetHandleVerifier [0x0x361c6d+183213]\n",
      "\tGetHandleVerifier [0x0x34c378+94904]\n",
      "\tGetHandleVerifier [0x0x34c502+95298]\n",
      "\tGetHandleVerifier [0x0x33765a+9626]\n",
      "\tBaseThreadInitThunk [0x0x76775d49+25]\n",
      "\tRtlInitializeExceptionChain [0x0x778ed09b+107]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x0x778ed021+561]\n",
      "\t(No symbol) [0x0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = test_real_scrapers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ff8626bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” SiteScout initialisÃ© - PrÃªt pour la reconnaissance !\n"
     ]
    }
   ],
   "source": [
    "# ğŸ” PHASE 1: RECONNAISSANCE ET VALIDATION DES BALISES RÃ‰ELLES\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "from datetime import datetime\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "class SiteScout:\n",
    "    \"\"\"\n",
    "    Classe pour reconnaÃ®tre et valider les vraies balises des sites avant scraping.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.driver = None\n",
    "        self.discovered_selectors = {}\n",
    "        self.validated_selectors = {}\n",
    "        \n",
    "    def setup_scout_driver(self):\n",
    "        \"\"\"Configure un driver spÃ©cial pour la reconnaissance\"\"\"\n",
    "        try:\n",
    "            from selenium import webdriver\n",
    "            from selenium.webdriver.chrome.service import Service\n",
    "            from selenium.webdriver.chrome.options import Options\n",
    "            from webdriver_manager.chrome import ChromeDriverManager\n",
    "            \n",
    "            options = Options()\n",
    "            options.add_argument('--no-sandbox')\n",
    "            options.add_argument('--disable-dev-shm-usage')\n",
    "            options.add_argument('--disable-gpu')\n",
    "            options.add_argument('--window-size=1920,1080')\n",
    "            \n",
    "            service = Service(ChromeDriverManager().install())\n",
    "            self.driver = webdriver.Chrome(service=service, options=options)\n",
    "            \n",
    "            logger.info(\"âœ… Scout driver configurÃ©\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"âŒ Erreur scout driver: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def scout_amazon_selectors(self):\n",
    "        \"\"\"DÃ©couvre les vraies balises Amazon actuelles\"\"\"\n",
    "        logger.info(\"ğŸ” Reconnaissance Amazon...\")\n",
    "        \n",
    "        if not self.driver:\n",
    "            if not self.setup_scout_driver():\n",
    "                return {}\n",
    "        \n",
    "        try:\n",
    "            # Test avec une recherche simple\n",
    "            self.driver.get(\"https://www.amazon.com/s?k=laptop\")\n",
    "            time.sleep(3)\n",
    "            \n",
    "            # Accepter les cookies si nÃ©cessaire\n",
    "            try:\n",
    "                cookie_btn = self.driver.find_element(By.ID, \"sp-cc-accept\")\n",
    "                cookie_btn.click()\n",
    "                time.sleep(1)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            selectors_to_test = {\n",
    "                'product_container': [\n",
    "                    '[data-component-type=\"s-search-result\"]',\n",
    "                    '.s-result-item',\n",
    "                    '[data-asin]',\n",
    "                    '.sg-col-inner'\n",
    "                ],\n",
    "                'title': [\n",
    "                    'h2 a span',\n",
    "                    '.a-size-medium span',\n",
    "                    '.a-size-base-plus',\n",
    "                    '[data-cy=\"title-recipe-price\"]'\n",
    "                ],\n",
    "                'price': [\n",
    "                    '.a-price-whole',\n",
    "                    '.a-price .a-offscreen',\n",
    "                    '.a-price-range',\n",
    "                    '.a-price-symbol'\n",
    "                ],\n",
    "                'rating': [\n",
    "                    '.a-icon-alt',\n",
    "                    '[data-hook=\"rating-out-of-text\"]',\n",
    "                    '.a-declarative .a-icon-alt'\n",
    "                ],\n",
    "                'image': [\n",
    "                    '.s-image',\n",
    "                    '.a-dynamic-image',\n",
    "                    'img[data-image-latency]'\n",
    "                ]\n",
    "            }\n",
    "            \n",
    "            amazon_selectors = {}\n",
    "            \n",
    "            for element_type, selectors in selectors_to_test.items():\n",
    "                for selector in selectors:\n",
    "                    try:\n",
    "                        elements = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                        if len(elements) > 0:\n",
    "                            # Prendre un Ã©chantillon de texte pour validation\n",
    "                            sample_text = \"\"\n",
    "                            try:\n",
    "                                if element_type == 'image':\n",
    "                                    sample_text = elements[0].get_attribute('src')[:50]\n",
    "                                else:\n",
    "                                    sample_text = elements[0].text[:50]\n",
    "                            except:\n",
    "                                sample_text = \"Element found\"\n",
    "                            \n",
    "                            amazon_selectors[element_type] = {\n",
    "                                'selector': selector,\n",
    "                                'count': len(elements),\n",
    "                                'sample': sample_text,\n",
    "                                'validated': True\n",
    "                            }\n",
    "                            logger.info(f\"âœ… {element_type}: {selector} ({len(elements)} Ã©lÃ©ments)\")\n",
    "                            break\n",
    "                    except Exception as e:\n",
    "                        continue\n",
    "                \n",
    "                if element_type not in amazon_selectors:\n",
    "                    amazon_selectors[element_type] = {\n",
    "                        'selector': None,\n",
    "                        'count': 0,\n",
    "                        'sample': None,\n",
    "                        'validated': False\n",
    "                    }\n",
    "                    logger.warning(f\"âš ï¸ {element_type}: Aucun sÃ©lecteur trouvÃ©\")\n",
    "            \n",
    "            self.discovered_selectors['amazon'] = amazon_selectors\n",
    "            return amazon_selectors\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"âŒ Erreur reconnaissance Amazon: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def scout_ebay_selectors(self):\n",
    "        \"\"\"DÃ©couvre les vraies balises eBay actuelles\"\"\"\n",
    "        logger.info(\"ğŸ” Reconnaissance eBay...\")\n",
    "        \n",
    "        try:\n",
    "            self.driver.get(\"https://www.ebay.com/sch/i.html?_nkw=smartphone\")\n",
    "            time.sleep(3)\n",
    "            \n",
    "            selectors_to_test = {\n",
    "                'product_container': [\n",
    "                    '.s-item',\n",
    "                    '.srp-results .s-item',\n",
    "                    '[data-view=\"mi:1686|iid:1\"]'\n",
    "                ],\n",
    "                'title': [\n",
    "                    '.s-item__title',\n",
    "                    '.it-ttl',\n",
    "                    '.s-item__link'\n",
    "                ],\n",
    "                'price': [\n",
    "                    '.s-item__price',\n",
    "                    '.notranslate',\n",
    "                    '.u-flL'\n",
    "                ],\n",
    "                'condition': [\n",
    "                    '.SECONDARY_INFO',\n",
    "                    '.s-item__subtitle',\n",
    "                    '.clipped'\n",
    "                ],\n",
    "                'shipping': [\n",
    "                    '.s-item__shipping',\n",
    "                    '.vi-s-ship-range',\n",
    "                    '.s-item__logisticsCost'\n",
    "                ]\n",
    "            }\n",
    "            \n",
    "            ebay_selectors = {}\n",
    "            \n",
    "            for element_type, selectors in selectors_to_test.items():\n",
    "                for selector in selectors:\n",
    "                    try:\n",
    "                        elements = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                        if len(elements) > 0:\n",
    "                            sample_text = \"\"\n",
    "                            try:\n",
    "                                sample_text = elements[0].text[:50]\n",
    "                            except:\n",
    "                                sample_text = \"Element found\"\n",
    "                            \n",
    "                            ebay_selectors[element_type] = {\n",
    "                                'selector': selector,\n",
    "                                'count': len(elements),\n",
    "                                'sample': sample_text,\n",
    "                                'validated': True\n",
    "                            }\n",
    "                            logger.info(f\"âœ… {element_type}: {selector} ({len(elements)} Ã©lÃ©ments)\")\n",
    "                            break\n",
    "                    except:\n",
    "                        continue\n",
    "                \n",
    "                if element_type not in ebay_selectors:\n",
    "                    ebay_selectors[element_type] = {\n",
    "                        'selector': None,\n",
    "                        'count': 0,\n",
    "                        'sample': None,\n",
    "                        'validated': False\n",
    "                    }\n",
    "            \n",
    "            self.discovered_selectors['ebay'] = ebay_selectors\n",
    "            return ebay_selectors\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"âŒ Erreur reconnaissance eBay: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def scout_trustpilot_selectors(self, company=\"amazon\"):\n",
    "        \"\"\"DÃ©couvre les vraies balises Trustpilot actuelles\"\"\"\n",
    "        logger.info(f\"ğŸ” Reconnaissance Trustpilot pour {company}...\")\n",
    "        \n",
    "        try:\n",
    "            self.driver.get(f\"https://www.trustpilot.com/review/{company}\")\n",
    "            time.sleep(3)\n",
    "            \n",
    "            # Accepter cookies\n",
    "            try:\n",
    "                cookie_btn = self.driver.find_element(By.ID, \"onetrust-accept-btn-handler\")\n",
    "                cookie_btn.click()\n",
    "                time.sleep(2)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            selectors_to_test = {\n",
    "                'review_container': [\n",
    "                    'article[data-service-review-card-paper]',\n",
    "                    '.review-card',\n",
    "                    '.styles_reviewCard__hcAvl'\n",
    "                ],\n",
    "                'review_text': [\n",
    "                    '[data-service-review-text-typography=\"true\"]',\n",
    "                    '.typography_body-l__KUYFJ',\n",
    "                    '.review-content__text'\n",
    "                ],\n",
    "                'rating': [\n",
    "                    '[data-service-review-rating]',\n",
    "                    '.star-rating',\n",
    "                    '.styles_reviewHeader__iU9Px img'\n",
    "                ],\n",
    "                'reviewer_name': [\n",
    "                    '[data-consumer-name-typography=\"true\"]',\n",
    "                    '.consumer-information__name',\n",
    "                    '.styles_consumerName__dxer2'\n",
    "                ],\n",
    "                'review_date': [\n",
    "                    'time[datetime]',\n",
    "                    '.typography_body-m__xgxZ_',\n",
    "                    '.styles_reviewDate__6_BBM'\n",
    "                ]\n",
    "            }\n",
    "            \n",
    "            trustpilot_selectors = {}\n",
    "            \n",
    "            for element_type, selectors in selectors_to_test.items():\n",
    "                for selector in selectors:\n",
    "                    try:\n",
    "                        elements = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                        if len(elements) > 0:\n",
    "                            sample_text = \"\"\n",
    "                            try:\n",
    "                                if element_type == 'review_date':\n",
    "                                    sample_text = elements[0].get_attribute('datetime') or elements[0].text\n",
    "                                else:\n",
    "                                    sample_text = elements[0].text[:50]\n",
    "                            except:\n",
    "                                sample_text = \"Element found\"\n",
    "                            \n",
    "                            trustpilot_selectors[element_type] = {\n",
    "                                'selector': selector,\n",
    "                                'count': len(elements),\n",
    "                                'sample': sample_text,\n",
    "                                'validated': True\n",
    "                            }\n",
    "                            logger.info(f\"âœ… {element_type}: {selector} ({len(elements)} Ã©lÃ©ments)\")\n",
    "                            break\n",
    "                    except:\n",
    "                        continue\n",
    "                \n",
    "                if element_type not in trustpilot_selectors:\n",
    "                    trustpilot_selectors[element_type] = {\n",
    "                        'selector': None,\n",
    "                        'count': 0,\n",
    "                        'sample': None,\n",
    "                        'validated': False\n",
    "                    }\n",
    "            \n",
    "            self.discovered_selectors['trustpilot'] = trustpilot_selectors\n",
    "            return trustpilot_selectors\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"âŒ Erreur reconnaissance Trustpilot: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def save_selectors_config(self, filename=\"../config/validated_selectors.json\"):\n",
    "        \"\"\"Sauvegarde les sÃ©lecteurs validÃ©s\"\"\"\n",
    "        import os\n",
    "        os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "        \n",
    "        config = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'sites': self.discovered_selectors\n",
    "        }\n",
    "        \n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(config, f, indent=2)\n",
    "        \n",
    "        logger.info(f\"ğŸ’¾ SÃ©lecteurs sauvegardÃ©s: {filename}\")\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Ferme le driver scout\"\"\"\n",
    "        if self.driver:\n",
    "            self.driver.quit()\n",
    "\n",
    "# Initialiser le scout\n",
    "scout = SiteScout()\n",
    "print(\"ğŸ” SiteScout initialisÃ© - PrÃªt pour la reconnaissance !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4ffe089e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ PrÃªt pour la reconnaissance !\n",
      "   run_full_site_reconnaissance() - Reconnaissance complÃ¨te\n",
      "   quick_selector_test('amazon', '.s-result-item', 'https://amazon.com/s?k=laptop') - Test rapide\n"
     ]
    }
   ],
   "source": [
    "# ğŸ§ª PHASE 1: EXÃ‰CUTION DE LA RECONNAISSANCE\n",
    "def run_full_site_reconnaissance():\n",
    "    \"\"\"\n",
    "    Lance la reconnaissance complÃ¨te de tous les sites pour dÃ©couvrir les vraies balises.\n",
    "    \"\"\"\n",
    "    print(\"ğŸ¯ LANCEMENT DE LA RECONNAISSANCE COMPLÃˆTE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    scout = SiteScout()\n",
    "    all_results = {}\n",
    "    \n",
    "    try:\n",
    "        # 1. Amazon\n",
    "        print(\"\\nğŸ›’ Reconnaissance Amazon...\")\n",
    "        amazon_selectors = scout.scout_amazon_selectors()\n",
    "        all_results['amazon'] = amazon_selectors\n",
    "        \n",
    "        if amazon_selectors:\n",
    "            print(\"âœ… Amazon reconnaissance terminÃ©e\")\n",
    "            for element_type, data in amazon_selectors.items():\n",
    "                status = \"âœ…\" if data['validated'] else \"âŒ\"\n",
    "                print(f\"   {status} {element_type}: {data.get('selector', 'Non trouvÃ©')}\")\n",
    "        \n",
    "        time.sleep(2)  # Pause entre sites\n",
    "        \n",
    "        # 2. eBay\n",
    "        print(\"\\nğŸª Reconnaissance eBay...\")\n",
    "        ebay_selectors = scout.scout_ebay_selectors()\n",
    "        all_results['ebay'] = ebay_selectors\n",
    "        \n",
    "        if ebay_selectors:\n",
    "            print(\"âœ… eBay reconnaissance terminÃ©e\")\n",
    "            for element_type, data in ebay_selectors.items():\n",
    "                status = \"âœ…\" if data['validated'] else \"âŒ\"\n",
    "                print(f\"   {status} {element_type}: {data.get('selector', 'Non trouvÃ©')}\")\n",
    "        \n",
    "        time.sleep(2)\n",
    "        \n",
    "        # 3. Trustpilot\n",
    "        print(\"\\nâ­ Reconnaissance Trustpilot...\")\n",
    "        trustpilot_selectors = scout.scout_trustpilot_selectors(\"amazon\")\n",
    "        all_results['trustpilot'] = trustpilot_selectors\n",
    "        \n",
    "        if trustpilot_selectors:\n",
    "            print(\"âœ… Trustpilot reconnaissance terminÃ©e\")\n",
    "            for element_type, data in trustpilot_selectors.items():\n",
    "                status = \"âœ…\" if data['validated'] else \"âŒ\"\n",
    "                print(f\"   {status} {element_type}: {data.get('selector', 'Non trouvÃ©')}\")\n",
    "        \n",
    "        # 4. Sauvegarde\n",
    "        print(\"\\nğŸ’¾ Sauvegarde des sÃ©lecteurs...\")\n",
    "        scout.save_selectors_config()\n",
    "        \n",
    "        # 5. RÃ©sumÃ©\n",
    "        print(\"\\nğŸ“Š RÃ‰SUMÃ‰ DE LA RECONNAISSANCE:\")\n",
    "        total_selectors = 0\n",
    "        valid_selectors = 0\n",
    "        \n",
    "        for site, selectors in all_results.items():\n",
    "            site_total = len(selectors)\n",
    "            site_valid = sum(1 for s in selectors.values() if s.get('validated', False))\n",
    "            total_selectors += site_total\n",
    "            valid_selectors += site_valid\n",
    "            \n",
    "            print(f\"   {site.upper()}: {site_valid}/{site_total} sÃ©lecteurs validÃ©s\")\n",
    "        \n",
    "        print(f\"\\nğŸ¯ TOTAL: {valid_selectors}/{total_selectors} sÃ©lecteurs fonctionnels\")\n",
    "        \n",
    "        return all_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"âŒ Erreur reconnaissance: {e}\")\n",
    "        return {}\n",
    "        \n",
    "    finally:\n",
    "        scout.close()\n",
    "\n",
    "def quick_selector_test(site_name, selector, url):\n",
    "    \"\"\"Test rapide d'un sÃ©lecteur spÃ©cifique\"\"\"\n",
    "    print(f\"ğŸ§ª Test rapide: {site_name} - {selector}\")\n",
    "    \n",
    "    scout = SiteScout()\n",
    "    if not scout.setup_scout_driver():\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        scout.driver.get(url)\n",
    "        time.sleep(3)\n",
    "        \n",
    "        elements = scout.driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "        \n",
    "        if len(elements) > 0:\n",
    "            print(f\"âœ… TrouvÃ© {len(elements)} Ã©lÃ©ments\")\n",
    "            print(f\"   Exemple: {elements[0].text[:100]}...\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"âŒ Aucun Ã©lÃ©ment trouvÃ©\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Erreur: {e}\")\n",
    "        return False\n",
    "    finally:\n",
    "        scout.close()\n",
    "\n",
    "# LANCEMENT DE LA RECONNAISSANCE\n",
    "print(\"ğŸš€ PrÃªt pour la reconnaissance !\")\n",
    "print(\"   run_full_site_reconnaissance() - Reconnaissance complÃ¨te\")\n",
    "print(\"   quick_selector_test('amazon', '.s-result-item', 'https://amazon.com/s?k=laptop') - Test rapide\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bce8b44e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ ValidatedScraper prÃªt !\n"
     ]
    }
   ],
   "source": [
    "# ğŸ¯ PHASE 2: SCRAPER AVEC BALISES VALIDÃ‰ES\n",
    "class ValidatedScraper:\n",
    "    \"\"\"\n",
    "    Scraper qui utilise les balises validÃ©es de la Phase 1\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, selectors_file=\"../config/validated_selectors.json\"):\n",
    "        self.driver = None\n",
    "        self.selectors = {}\n",
    "        self.load_validated_selectors(selectors_file)\n",
    "        \n",
    "    def load_validated_selectors(self, filename):\n",
    "        \"\"\"Charge les sÃ©lecteurs validÃ©s depuis le fichier JSON\"\"\"\n",
    "        try:\n",
    "            with open(filename, 'r') as f:\n",
    "                config = json.load(f)\n",
    "                self.selectors = config.get('sites', {})\n",
    "                logger.info(f\"âœ… SÃ©lecteurs chargÃ©s depuis {filename}\")\n",
    "                \n",
    "                # Afficher les sÃ©lecteurs chargÃ©s\n",
    "                for site, site_selectors in self.selectors.items():\n",
    "                    valid_count = sum(1 for s in site_selectors.values() if s.get('validated'))\n",
    "                    print(f\"   {site.upper()}: {valid_count} sÃ©lecteurs validÃ©s\")\n",
    "                    \n",
    "        except FileNotFoundError:\n",
    "            logger.warning(\"âš ï¸ Fichier de sÃ©lecteurs non trouvÃ© - Lancez d'abord la reconnaissance\")\n",
    "            self.selectors = {}\n",
    "        except Exception as e:\n",
    "            logger.error(f\"âŒ Erreur chargement sÃ©lecteurs: {e}\")\n",
    "            self.selectors = {}\n",
    "    \n",
    "    def setup_production_driver(self, headless=True, use_stealth=True):\n",
    "        \"\"\"Configure un driver optimisÃ© pour la production\"\"\"\n",
    "        try:\n",
    "            if use_stealth:\n",
    "                import undetected_chromedriver as uc\n",
    "                options = uc.ChromeOptions()\n",
    "                \n",
    "                # Options de base\n",
    "                if headless:\n",
    "                    options.add_argument('--headless=new')\n",
    "                options.add_argument('--no-sandbox')\n",
    "                options.add_argument('--disable-dev-shm-usage')\n",
    "                options.add_argument('--disable-gpu')\n",
    "                options.add_argument('--window-size=1920,1080')\n",
    "                \n",
    "                # User agent alÃ©atoire\n",
    "                user_agent = random.choice(REALISTIC_USER_AGENTS)\n",
    "                options.add_argument(f'--user-agent={user_agent}')\n",
    "                \n",
    "                # CrÃ©er driver stealth\n",
    "                self.driver = uc.Chrome(options=options)\n",
    "                \n",
    "                # Scripts anti-dÃ©tection\n",
    "                self.driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
    "                \n",
    "            else:\n",
    "                # Driver classique si stealth Ã©choue\n",
    "                from selenium import webdriver\n",
    "                from selenium.webdriver.chrome.service import Service\n",
    "                from webdriver_manager.chrome import ChromeDriverManager\n",
    "                \n",
    "                options = webdriver.ChromeOptions()\n",
    "                if headless:\n",
    "                    options.add_argument('--headless')\n",
    "                options.add_argument('--no-sandbox')\n",
    "                options.add_argument('--disable-dev-shm-usage')\n",
    "                \n",
    "                service = Service(ChromeDriverManager().install())\n",
    "                self.driver = webdriver.Chrome(service=service, options=options)\n",
    "            \n",
    "            logger.info(\"âœ… Driver production configurÃ©\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"âŒ Erreur driver production: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def human_like_behavior(self):\n",
    "        \"\"\"Simule un comportement humain\"\"\"\n",
    "        # DÃ©lai alÃ©atoire\n",
    "        time.sleep(random.uniform(1, 3))\n",
    "        \n",
    "        # Scroll alÃ©atoire parfois\n",
    "        if random.random() < 0.3:\n",
    "            scroll_amount = random.randint(200, 800)\n",
    "            self.driver.execute_script(f\"window.scrollBy(0, {scroll_amount});\")\n",
    "            time.sleep(random.uniform(0.5, 1.5))\n",
    "    \n",
    "    def scrape_amazon_products_validated(self, search_term, max_products=20):\n",
    "        \"\"\"Scrape Amazon avec sÃ©lecteurs validÃ©s\"\"\"\n",
    "        logger.info(f\"ğŸ›’ Scraping Amazon validÃ©: {search_term}\")\n",
    "        \n",
    "        if 'amazon' not in self.selectors:\n",
    "            logger.error(\"âŒ SÃ©lecteurs Amazon non disponibles - Lancez la reconnaissance\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        amazon_selectors = self.selectors['amazon']\n",
    "        products_data = []\n",
    "        \n",
    "        try:\n",
    "            # Navigation\n",
    "            url = f\"https://www.amazon.com/s?k={search_term}\"\n",
    "            self.driver.get(url)\n",
    "            self.human_like_behavior()\n",
    "            \n",
    "            # Accepter cookies\n",
    "            try:\n",
    "                cookie_btn = WebDriverWait(self.driver, 5).until(\n",
    "                    EC.element_to_be_clickable((By.ID, \"sp-cc-accept\"))\n",
    "                )\n",
    "                cookie_btn.click()\n",
    "                time.sleep(1)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Trouver les produits avec sÃ©lecteur validÃ©\n",
    "            container_selector = amazon_selectors.get('product_container', {}).get('selector')\n",
    "            if not container_selector:\n",
    "                logger.error(\"âŒ SÃ©lecteur conteneur produit non validÃ©\")\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            products = WebDriverWait(self.driver, 10).until(\n",
    "                EC.presence_of_all_elements_located((By.CSS_SELECTOR, container_selector))\n",
    "            )\n",
    "            \n",
    "            logger.info(f\"ğŸ“¦ TrouvÃ© {len(products)} produits\")\n",
    "            \n",
    "            for i, product in enumerate(products[:max_products]):\n",
    "                try:\n",
    "                    product_data = {\n",
    "                        'product_id': f\"amazon_{search_term}_{i}\",\n",
    "                        'search_term': search_term,\n",
    "                        'source': 'amazon.com',\n",
    "                        'scraped_at': datetime.now().isoformat()\n",
    "                    }\n",
    "                    \n",
    "                    # Extraire titre avec sÃ©lecteur validÃ©\n",
    "                    title_selector = amazon_selectors.get('title', {}).get('selector')\n",
    "                    if title_selector:\n",
    "                        try:\n",
    "                            title_elem = product.find_element(By.CSS_SELECTOR, title_selector)\n",
    "                            product_data['title'] = title_elem.text.strip()\n",
    "                        except:\n",
    "                            product_data['title'] = \"Titre non trouvÃ©\"\n",
    "                    \n",
    "                    # Extraire prix avec sÃ©lecteur validÃ©\n",
    "                    price_selector = amazon_selectors.get('price', {}).get('selector')\n",
    "                    if price_selector:\n",
    "                        try:\n",
    "                            price_elem = product.find_element(By.CSS_SELECTOR, price_selector)\n",
    "                            price_text = price_elem.text.strip()\n",
    "                            price_numbers = re.findall(r'[\\d.]+', price_text.replace(',', ''))\n",
    "                            product_data['price'] = float(price_numbers[0]) if price_numbers else 0.0\n",
    "                        except:\n",
    "                            product_data['price'] = 0.0\n",
    "                    \n",
    "                    # Extraire rating avec sÃ©lecteur validÃ©\n",
    "                    rating_selector = amazon_selectors.get('rating', {}).get('selector')\n",
    "                    if rating_selector:\n",
    "                        try:\n",
    "                            rating_elem = product.find_element(By.CSS_SELECTOR, rating_selector)\n",
    "                            rating_text = rating_elem.get_attribute('textContent') or rating_elem.text\n",
    "                            rating_numbers = re.findall(r'[\\d.]+', rating_text)\n",
    "                            product_data['rating'] = float(rating_numbers[0]) if rating_numbers else 0.0\n",
    "                        except:\n",
    "                            product_data['rating'] = 0.0\n",
    "                    \n",
    "                    # Extraire URL produit\n",
    "                    try:\n",
    "                        link_elem = product.find_element(By.CSS_SELECTOR, 'h2 a')\n",
    "                        product_data['product_url'] = link_elem.get_attribute('href')\n",
    "                    except:\n",
    "                        product_data['product_url'] = \"\"\n",
    "                    \n",
    "                    products_data.append(product_data)\n",
    "                    \n",
    "                    # Comportement humain\n",
    "                    if i % 5 == 0:\n",
    "                        self.human_like_behavior()\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Erreur produit {i}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            logger.info(f\"âœ… {len(products_data)} produits Amazon scrapÃ©s avec succÃ¨s\")\n",
    "            return pd.DataFrame(products_data)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"âŒ Erreur scraping Amazon: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def scrape_trustpilot_reviews_validated(self, company, max_reviews=50):\n",
    "        \"\"\"Scrape Trustpilot avec sÃ©lecteurs validÃ©s\"\"\"\n",
    "        logger.info(f\"â­ Scraping Trustpilot validÃ©: {company}\")\n",
    "        \n",
    "        if 'trustpilot' not in self.selectors:\n",
    "            logger.error(\"âŒ SÃ©lecteurs Trustpilot non disponibles\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        trustpilot_selectors = self.selectors['trustpilot']\n",
    "        reviews_data = []\n",
    "        \n",
    "        try:\n",
    "            # Navigation\n",
    "            url = f\"https://www.trustpilot.com/review/{company}\"\n",
    "            self.driver.get(url)\n",
    "            self.human_like_behavior()\n",
    "            \n",
    "            # Accepter cookies\n",
    "            try:\n",
    "                cookie_btn = WebDriverWait(self.driver, 5).until(\n",
    "                    EC.element_to_be_clickable((By.ID, \"onetrust-accept-btn-handler\"))\n",
    "                )\n",
    "                cookie_btn.click()\n",
    "                time.sleep(2)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Trouver reviews avec sÃ©lecteur validÃ©\n",
    "            container_selector = trustpilot_selectors.get('review_container', {}).get('selector')\n",
    "            if not container_selector:\n",
    "                logger.error(\"âŒ SÃ©lecteur conteneur review non validÃ©\")\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            reviews = WebDriverWait(self.driver, 10).until(\n",
    "                EC.presence_of_all_elements_located((By.CSS_SELECTOR, container_selector))\n",
    "            )\n",
    "            \n",
    "            logger.info(f\"ğŸ’¬ TrouvÃ© {len(reviews)} reviews\")\n",
    "            \n",
    "            for i, review in enumerate(reviews[:max_reviews]):\n",
    "                try:\n",
    "                    review_data = {\n",
    "                        'review_id': f\"trustpilot_{company}_{i}\",\n",
    "                        'company': company,\n",
    "                        'source': 'trustpilot.com',\n",
    "                        'scraped_at': datetime.now().isoformat()\n",
    "                    }\n",
    "                    \n",
    "                    # Extraire texte avec sÃ©lecteur validÃ©\n",
    "                    text_selector = trustpilot_selectors.get('review_text', {}).get('selector')\n",
    "                    if text_selector:\n",
    "                        try:\n",
    "                            text_elem = review.find_element(By.CSS_SELECTOR, text_selector)\n",
    "                            review_data['review_text'] = text_elem.text.strip()\n",
    "                        except:\n",
    "                            review_data['review_text'] = \"\"\n",
    "                    \n",
    "                    # Extraire rating avec sÃ©lecteur validÃ©\n",
    "                    rating_selector = trustpilot_selectors.get('rating', {}).get('selector')\n",
    "                    if rating_selector:\n",
    "                        try:\n",
    "                            rating_elem = review.find_element(By.CSS_SELECTOR, rating_selector)\n",
    "                            # Compter les Ã©toiles ou extraire de l'attribut\n",
    "                            stars = rating_elem.find_elements(By.CSS_SELECTOR, 'img[alt*=\"star\"]')\n",
    "                            review_data['rating'] = len([s for s in stars if 'filled' in s.get_attribute('alt')])\n",
    "                        except:\n",
    "                            review_data['rating'] = 0\n",
    "                    \n",
    "                    # Extraire nom reviewer avec sÃ©lecteur validÃ©\n",
    "                    name_selector = trustpilot_selectors.get('reviewer_name', {}).get('selector')\n",
    "                    if name_selector:\n",
    "                        try:\n",
    "                            name_elem = review.find_element(By.CSS_SELECTOR, name_selector)\n",
    "                            review_data['reviewer_name'] = name_elem.text.strip()\n",
    "                        except:\n",
    "                            review_data['reviewer_name'] = \"Anonymous\"\n",
    "                    \n",
    "                    # Extraire date avec sÃ©lecteur validÃ©\n",
    "                    date_selector = trustpilot_selectors.get('review_date', {}).get('selector')\n",
    "                    if date_selector:\n",
    "                        try:\n",
    "                            date_elem = review.find_element(By.CSS_SELECTOR, date_selector)\n",
    "                            date_text = date_elem.get_attribute('datetime') or date_elem.text\n",
    "                            # Parser la date\n",
    "                            if 'T' in date_text:  # Format ISO\n",
    "                                review_data['review_date'] = date_text[:10]\n",
    "                            else:\n",
    "                                review_data['review_date'] = date_text\n",
    "                        except:\n",
    "                            review_data['review_date'] = datetime.now().strftime('%Y-%m-%d')\n",
    "                    \n",
    "                    reviews_data.append(review_data)\n",
    "                    \n",
    "                    if i % 10 == 0:\n",
    "                        self.human_like_behavior()\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Erreur review {i}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            logger.info(f\"âœ… {len(reviews_data)} reviews Trustpilot scrapÃ©es avec succÃ¨s\")\n",
    "            return pd.DataFrame(reviews_data)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"âŒ Erreur scraping Trustpilot: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Ferme le driver\"\"\"\n",
    "        if self.driver:\n",
    "            self.driver.quit()\n",
    "\n",
    "# Initialiser le scraper validÃ©\n",
    "print(\"ğŸ¯ ValidatedScraper prÃªt !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "666d8a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ SYSTÃˆME COMPLET PRÃŠT !\n",
      "ğŸš€ ExÃ©cutez: main_menu() pour commencer\n",
      "ğŸ§ª Ou directement: test_marketplace_scraper()\n"
     ]
    }
   ],
   "source": [
    "# ğŸ§ª TESTS CORRIGÃ‰S ET INTÃ‰GRATION COMPLÃˆTE\n",
    "def test_marketplace_scraper():\n",
    "    \"\"\"\n",
    "    FONCTION DE TEST CORRIGÃ‰E - Compatible avec le systÃ¨me de reconnaissance\n",
    "    \"\"\"\n",
    "    logger.info(\"ğŸ§ª DÃ©marrage des tests du scraper...\")\n",
    "    \n",
    "    all_results = {}\n",
    "    \n",
    "    try:\n",
    "        # Phase 1: VÃ©rification de l'environnement\n",
    "        logger.info(\"ğŸ”§ VÃ©rification de l'environnement...\")\n",
    "        \n",
    "        # Test des imports\n",
    "        import pandas as pd\n",
    "        from fake_useragent import UserAgent\n",
    "        logger.info(\"âœ… Imports OK\")\n",
    "        \n",
    "        # Phase 2: Test de la reconnaissance (optionnel)\n",
    "        print(\"\\nğŸ” Phase 1 - Reconnaissance des balises (optionnel)\")\n",
    "        choice = input(\"Lancer la reconnaissance des vraies balises ? (o/n): \").lower()\n",
    "        \n",
    "        if choice in ['o', 'oui', 'y', 'yes']:\n",
    "            recognition_results = run_full_site_reconnaissance()\n",
    "            all_results['recognition'] = recognition_results\n",
    "        else:\n",
    "            print(\"â­ï¸ Reconnaissance ignorÃ©e - Utilisation des balises par dÃ©faut\")\n",
    "        \n",
    "        # Phase 3: Test du scraping validÃ©\n",
    "        print(\"\\nğŸ¯ Phase 2 - Test du scraping avec balises validÃ©es\")\n",
    "        \n",
    "        validated_scraper = ValidatedScraper()\n",
    "        \n",
    "        if not validated_scraper.setup_production_driver(headless=False):\n",
    "            logger.error(\"âŒ Impossible de configurer le driver\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            # Test Amazon (si sÃ©lecteurs disponibles)\n",
    "            print(\"\\nğŸ›’ Test Amazon avec balises validÃ©es...\")\n",
    "            amazon_products = validated_scraper.scrape_amazon_products_validated(\"laptop\", max_products=5)\n",
    "            \n",
    "            if not amazon_products.empty:\n",
    "                all_results['amazon_products'] = amazon_products\n",
    "                save_scraped_data(amazon_products, \"amazon_products_validated.csv\")\n",
    "                analyze_scraped_data(amazon_products)\n",
    "            else:\n",
    "                print(\"âš ï¸ Aucun produit Amazon rÃ©cupÃ©rÃ©\")\n",
    "            \n",
    "            time.sleep(3)  # Pause entre tests\n",
    "            \n",
    "            # Test Trustpilot (si sÃ©lecteurs disponibles)\n",
    "            print(\"\\nâ­ Test Trustpilot avec balises validÃ©es...\")\n",
    "            trustpilot_reviews = validated_scraper.scrape_trustpilot_reviews_validated(\"amazon\", max_reviews=5)\n",
    "            \n",
    "            if not trustpilot_reviews.empty:\n",
    "                all_results['trustpilot_reviews'] = trustpilot_reviews\n",
    "                save_scraped_data(trustpilot_reviews, \"trustpilot_reviews_validated.csv\")\n",
    "                analyze_scraped_data(trustpilot_reviews)\n",
    "            else:\n",
    "                print(\"âš ï¸ Aucune review Trustpilot rÃ©cupÃ©rÃ©e\")\n",
    "        \n",
    "        finally:\n",
    "            validated_scraper.close()\n",
    "        \n",
    "        # RÃ©sumÃ© final\n",
    "        print(\"\\nğŸ“Š RÃ‰SUMÃ‰ DES TESTS:\")\n",
    "        for test_type, data in all_results.items():\n",
    "            if isinstance(data, pd.DataFrame):\n",
    "                print(f\"   âœ… {test_type}: {len(data)} enregistrements\")\n",
    "            else:\n",
    "                print(f\"   â„¹ï¸ {test_type}: DonnÃ©es de reconnaissance\")\n",
    "        \n",
    "        logger.info(\"âœ… Tests terminÃ©s avec succÃ¨s !\")\n",
    "        return all_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"âŒ Erreur durant les tests: {e}\")\n",
    "        return None\n",
    "\n",
    "def production_scraping_workflow(sites=['amazon'], search_terms=['laptop'], max_items=50):\n",
    "    \"\"\"\n",
    "    Workflow de production complet : Reconnaissance + Scraping\n",
    "    \"\"\"\n",
    "    print(\"ğŸš€ WORKFLOW DE PRODUCTION - SCRAPING MARKETPLACE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Ã‰tape 1: Reconnaissance automatique\n",
    "    print(\"ğŸ” Ã‰TAPE 1: Reconnaissance des balises...\")\n",
    "    recognition_results = run_full_site_reconnaissance()\n",
    "    \n",
    "    if not recognition_results:\n",
    "        print(\"âŒ Reconnaissance Ã©chouÃ©e - ArrÃªt du workflow\")\n",
    "        return None\n",
    "    \n",
    "    # Ã‰tape 2: Configuration du scraper\n",
    "    print(\"\\nğŸ¯ Ã‰TAPE 2: Configuration du scraper validÃ©...\")\n",
    "    scraper = ValidatedScraper()\n",
    "    \n",
    "    if not scraper.setup_production_driver(headless=True, use_stealth=True):\n",
    "        print(\"âŒ Configuration driver Ã©chouÃ©e\")\n",
    "        return None\n",
    "    \n",
    "    all_data = {}\n",
    "    \n",
    "    try:\n",
    "        # Ã‰tape 3: Scraping par site\n",
    "        for site in sites:\n",
    "            print(f\"\\nğŸ“Š Ã‰TAPE 3: Scraping {site.upper()}...\")\n",
    "            \n",
    "            if site == 'amazon':\n",
    "                for term in search_terms:\n",
    "                    print(f\"   ğŸ” Recherche: {term}\")\n",
    "                    products = scraper.scrape_amazon_products_validated(term, max_items)\n",
    "                    \n",
    "                    if not products.empty:\n",
    "                        key = f\"amazon_products_{term}\"\n",
    "                        all_data[key] = products\n",
    "                        save_scraped_data(products, f\"production_{key}.csv\")\n",
    "                        print(f\"   âœ… {len(products)} produits rÃ©cupÃ©rÃ©s\")\n",
    "                    \n",
    "                    time.sleep(5)  # DÃ©lai entre recherches\n",
    "            \n",
    "            elif site == 'trustpilot':\n",
    "                companies = ['amazon', 'ebay', 'apple']\n",
    "                for company in companies:\n",
    "                    print(f\"   â­ Reviews: {company}\")\n",
    "                    reviews = scraper.scrape_trustpilot_reviews_validated(company, max_items)\n",
    "                    \n",
    "                    if not reviews.empty:\n",
    "                        key = f\"trustpilot_reviews_{company}\"\n",
    "                        all_data[key] = reviews\n",
    "                        save_scraped_data(reviews, f\"production_{key}.csv\")\n",
    "                        print(f\"   âœ… {len(reviews)} reviews rÃ©cupÃ©rÃ©es\")\n",
    "                    \n",
    "                    time.sleep(5)\n",
    "        \n",
    "        # Ã‰tape 4: RÃ©sumÃ© final\n",
    "        print(\"\\nğŸ‰ WORKFLOW TERMINÃ‰ !\")\n",
    "        total_records = sum(len(df) for df in all_data.values() if isinstance(df, pd.DataFrame))\n",
    "        print(f\"ğŸ“Š Total des enregistrements: {total_records}\")\n",
    "        \n",
    "        for key, df in all_data.items():\n",
    "            if isinstance(df, pd.DataFrame):\n",
    "                print(f\"   ğŸ“„ {key}: {len(df)} enregistrements\")\n",
    "        \n",
    "        return all_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"âŒ Erreur workflow: {e}\")\n",
    "        return None\n",
    "        \n",
    "    finally:\n",
    "        scraper.close()\n",
    "\n",
    "# MENU PRINCIPAL\n",
    "def main_menu():\n",
    "    \"\"\"Menu principal pour l'utilisation du scraper\"\"\"\n",
    "    print(\"ğŸ¯ MENU PRINCIPAL - DATA COLLECTION SCRAPER\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"1. ğŸ§ª Test complet (reconnaissance + scraping)\")\n",
    "    print(\"2. ğŸ” Reconnaissance seule des balises\")\n",
    "    print(\"3. ğŸ¯ Scraping avec balises validÃ©es\")\n",
    "    print(\"4. ğŸš€ Workflow de production complet\")\n",
    "    print(\"5. âŒ Quitter\")\n",
    "    \n",
    "    try:\n",
    "        choice = input(\"\\nVotre choix (1-5): \").strip()\n",
    "        \n",
    "        if choice == \"1\":\n",
    "            test_marketplace_scraper()\n",
    "        elif choice == \"2\":\n",
    "            run_full_site_reconnaissance()\n",
    "        elif choice == \"3\":\n",
    "            scraper = ValidatedScraper()\n",
    "            scraper.setup_production_driver(headless=False)\n",
    "            # Exemple simple\n",
    "            products = scraper.scrape_amazon_products_validated(\"smartphone\", 10)\n",
    "            if not products.empty:\n",
    "                save_scraped_data(products, \"quick_scraping.csv\")\n",
    "            scraper.close()\n",
    "        elif choice == \"4\":\n",
    "            production_scraping_workflow()\n",
    "        elif choice == \"5\":\n",
    "            print(\"ğŸ‘‹ Au revoir !\")\n",
    "        else:\n",
    "            print(\"âŒ Choix invalide\")\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nğŸ‘‹ ArrÃªt demandÃ© par l'utilisateur\")\n",
    "\n",
    "print(\"ğŸ¯ SYSTÃˆME COMPLET PRÃŠT !\")\n",
    "print(\"ğŸš€ ExÃ©cutez: main_menu() pour commencer\")\n",
    "print(\"ğŸ§ª Ou directement: test_marketplace_scraper()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3d5ce55e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ MENU PRINCIPAL - DATA COLLECTION SCRAPER\n",
      "==================================================\n",
      "1. ğŸ§ª Test complet (reconnaissance + scraping)\n",
      "2. ğŸ” Reconnaissance seule des balises\n",
      "3. ğŸ¯ Scraping avec balises validÃ©es\n",
      "4. ğŸš€ Workflow de production complet\n",
      "5. âŒ Quitter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 16:13:59,079 - INFO - ğŸ” Reconnaissance Amazon...\n",
      "2025-06-27 16:13:59,137 - INFO - ====== WebDriver manager ======\n",
      "2025-06-27 16:13:59,137 - INFO - ====== WebDriver manager ======\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ WORKFLOW DE PRODUCTION - SCRAPING MARKETPLACE\n",
      "============================================================\n",
      "ğŸ” Ã‰TAPE 1: Reconnaissance des balises...\n",
      "ğŸ¯ LANCEMENT DE LA RECONNAISSANCE COMPLÃˆTE\n",
      "============================================================\n",
      "\n",
      "ğŸ›’ Reconnaissance Amazon...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 16:14:00,442 - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2025-06-27 16:14:00,651 - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2025-06-27 16:14:00,651 - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2025-06-27 16:14:00,872 - INFO - There is no [win64] chromedriver \"138.0.7204.49\" for browser google-chrome \"138.0.7204\" in cache\n",
      "2025-06-27 16:14:00,873 - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2025-06-27 16:14:00,872 - INFO - There is no [win64] chromedriver \"138.0.7204.49\" for browser google-chrome \"138.0.7204\" in cache\n",
      "2025-06-27 16:14:00,873 - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2025-06-27 16:14:01,437 - INFO - WebDriver version 138.0.7204.49 selected\n",
      "2025-06-27 16:14:01,439 - INFO - Modern chrome version https://storage.googleapis.com/chrome-for-testing-public/138.0.7204.49/win32/chromedriver-win32.zip\n",
      "2025-06-27 16:14:01,439 - INFO - About to download new driver from https://storage.googleapis.com/chrome-for-testing-public/138.0.7204.49/win32/chromedriver-win32.zip\n",
      "2025-06-27 16:14:01,437 - INFO - WebDriver version 138.0.7204.49 selected\n",
      "2025-06-27 16:14:01,439 - INFO - Modern chrome version https://storage.googleapis.com/chrome-for-testing-public/138.0.7204.49/win32/chromedriver-win32.zip\n",
      "2025-06-27 16:14:01,439 - INFO - About to download new driver from https://storage.googleapis.com/chrome-for-testing-public/138.0.7204.49/win32/chromedriver-win32.zip\n",
      "2025-06-27 16:14:01,645 - INFO - Driver downloading response is 200\n",
      "2025-06-27 16:14:01,645 - INFO - Driver downloading response is 200\n",
      "2025-06-27 16:14:03,213 - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2025-06-27 16:14:03,213 - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2025-06-27 16:14:03,468 - INFO - Driver has been saved in cache [C:\\Users\\Yann\\.wdm\\drivers\\chromedriver\\win64\\138.0.7204.49]\n",
      "2025-06-27 16:14:03,468 - INFO - Driver has been saved in cache [C:\\Users\\Yann\\.wdm\\drivers\\chromedriver\\win64\\138.0.7204.49]\n",
      "2025-06-27 16:14:05,064 - INFO - âœ… Scout driver configurÃ©\n",
      "2025-06-27 16:14:05,064 - INFO - âœ… Scout driver configurÃ©\n",
      "2025-06-27 16:14:14,122 - INFO - âœ… product_container: [data-component-type=\"s-search-result\"] (21 Ã©lÃ©ments)\n",
      "2025-06-27 16:14:14,136 - INFO - âœ… title: .a-size-medium span (24 Ã©lÃ©ments)\n",
      "2025-06-27 16:14:14,122 - INFO - âœ… product_container: [data-component-type=\"s-search-result\"] (21 Ã©lÃ©ments)\n",
      "2025-06-27 16:14:14,136 - INFO - âœ… title: .a-size-medium span (24 Ã©lÃ©ments)\n",
      "2025-06-27 16:14:14,157 - INFO - âœ… price: .a-price-whole (31 Ã©lÃ©ments)\n",
      "2025-06-27 16:14:14,166 - INFO - âœ… rating: .a-icon-alt (33 Ã©lÃ©ments)\n",
      "2025-06-27 16:14:14,157 - INFO - âœ… price: .a-price-whole (31 Ã©lÃ©ments)\n",
      "2025-06-27 16:14:14,166 - INFO - âœ… rating: .a-icon-alt (33 Ã©lÃ©ments)\n",
      "2025-06-27 16:14:14,179 - INFO - âœ… image: .s-image (43 Ã©lÃ©ments)\n",
      "2025-06-27 16:14:14,179 - INFO - âœ… image: .s-image (43 Ã©lÃ©ments)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Amazon reconnaissance terminÃ©e\n",
      "   âœ… product_container: [data-component-type=\"s-search-result\"]\n",
      "   âœ… title: .a-size-medium span\n",
      "   âœ… price: .a-price-whole\n",
      "   âœ… rating: .a-icon-alt\n",
      "   âœ… image: .s-image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 16:14:16,181 - INFO - ğŸ” Reconnaissance eBay...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸª Reconnaissance eBay...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 16:14:21,305 - INFO - âœ… product_container: .s-item (82 Ã©lÃ©ments)\n",
      "2025-06-27 16:14:21,317 - INFO - âœ… title: .s-item__title (82 Ã©lÃ©ments)\n",
      "2025-06-27 16:14:21,317 - INFO - âœ… title: .s-item__title (82 Ã©lÃ©ments)\n",
      "2025-06-27 16:14:21,330 - INFO - âœ… price: .s-item__price (62 Ã©lÃ©ments)\n",
      "2025-06-27 16:14:21,342 - INFO - âœ… condition: .SECONDARY_INFO (62 Ã©lÃ©ments)\n",
      "2025-06-27 16:14:21,330 - INFO - âœ… price: .s-item__price (62 Ã©lÃ©ments)\n",
      "2025-06-27 16:14:21,342 - INFO - âœ… condition: .SECONDARY_INFO (62 Ã©lÃ©ments)\n",
      "2025-06-27 16:14:21,354 - INFO - âœ… shipping: .s-item__shipping (60 Ã©lÃ©ments)\n",
      "2025-06-27 16:14:21,354 - INFO - âœ… shipping: .s-item__shipping (60 Ã©lÃ©ments)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… eBay reconnaissance terminÃ©e\n",
      "   âœ… product_container: .s-item\n",
      "   âœ… title: .s-item__title\n",
      "   âœ… price: .s-item__price\n",
      "   âœ… condition: .SECONDARY_INFO\n",
      "   âœ… shipping: .s-item__shipping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 16:14:23,356 - INFO - ğŸ” Reconnaissance Trustpilot pour amazon...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â­ Reconnaissance Trustpilot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 16:14:32,028 - INFO - âœ… review_container: article[data-service-review-card-paper] (24 Ã©lÃ©ments)\n",
      "2025-06-27 16:14:32,045 - INFO - âœ… review_text: [data-service-review-text-typography=\"true\"] (20 Ã©lÃ©ments)\n",
      "2025-06-27 16:14:32,045 - INFO - âœ… review_text: [data-service-review-text-typography=\"true\"] (20 Ã©lÃ©ments)\n",
      "2025-06-27 16:14:32,102 - INFO - âœ… rating: [data-service-review-rating] (20 Ã©lÃ©ments)\n",
      "2025-06-27 16:14:32,102 - INFO - âœ… rating: [data-service-review-rating] (20 Ã©lÃ©ments)\n",
      "2025-06-27 16:14:32,119 - INFO - âœ… reviewer_name: [data-consumer-name-typography=\"true\"] (24 Ã©lÃ©ments)\n",
      "2025-06-27 16:14:32,136 - INFO - âœ… review_date: time[datetime] (24 Ã©lÃ©ments)\n",
      "2025-06-27 16:14:32,119 - INFO - âœ… reviewer_name: [data-consumer-name-typography=\"true\"] (24 Ã©lÃ©ments)\n",
      "2025-06-27 16:14:32,136 - INFO - âœ… review_date: time[datetime] (24 Ã©lÃ©ments)\n",
      "2025-06-27 16:14:32,138 - INFO - ğŸ’¾ SÃ©lecteurs sauvegardÃ©s: ../config/validated_selectors.json\n",
      "2025-06-27 16:14:32,138 - INFO - ğŸ’¾ SÃ©lecteurs sauvegardÃ©s: ../config/validated_selectors.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Trustpilot reconnaissance terminÃ©e\n",
      "   âœ… review_container: article[data-service-review-card-paper]\n",
      "   âœ… review_text: [data-service-review-text-typography=\"true\"]\n",
      "   âœ… rating: [data-service-review-rating]\n",
      "   âœ… reviewer_name: [data-consumer-name-typography=\"true\"]\n",
      "   âœ… review_date: time[datetime]\n",
      "\n",
      "ğŸ’¾ Sauvegarde des sÃ©lecteurs...\n",
      "\n",
      "ğŸ“Š RÃ‰SUMÃ‰ DE LA RECONNAISSANCE:\n",
      "   AMAZON: 5/5 sÃ©lecteurs validÃ©s\n",
      "   EBAY: 5/5 sÃ©lecteurs validÃ©s\n",
      "   TRUSTPILOT: 5/5 sÃ©lecteurs validÃ©s\n",
      "\n",
      "ğŸ¯ TOTAL: 15/15 sÃ©lecteurs fonctionnels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 16:14:38,646 - INFO - âœ… SÃ©lecteurs chargÃ©s depuis ../config/validated_selectors.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ¯ Ã‰TAPE 2: Configuration du scraper validÃ©...\n",
      "   AMAZON: 5 sÃ©lecteurs validÃ©s\n",
      "   EBAY: 5 sÃ©lecteurs validÃ©s\n",
      "   TRUSTPILOT: 5 sÃ©lecteurs validÃ©s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 16:14:40,746 - INFO - patching driver executable C:\\Users\\Yann\\appdata\\roaming\\undetected_chromedriver\\undetected_chromedriver.exe\n",
      "2025-06-27 16:14:41,551 - INFO - âœ… Driver production configurÃ©\n",
      "2025-06-27 16:14:41,552 - INFO - ğŸ›’ Scraping Amazon validÃ©: laptop\n",
      "2025-06-27 16:14:41,551 - INFO - âœ… Driver production configurÃ©\n",
      "2025-06-27 16:14:41,552 - INFO - ğŸ›’ Scraping Amazon validÃ©: laptop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Ã‰TAPE 3: Scraping AMAZON...\n",
      "   ğŸ” Recherche: laptop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 16:14:54,036 - INFO - ğŸ“¦ TrouvÃ© 21 produits\n",
      "2025-06-27 16:15:05,766 - INFO - âœ… 21 produits Amazon scrapÃ©s avec succÃ¨s\n",
      "2025-06-27 16:15:05,785 - INFO - ğŸ’¾ DonnÃ©es sauvegardÃ©es: ../data/raw\\20250627_161505_production_amazon_products_laptop.csv (21 enregistrements)\n",
      "2025-06-27 16:15:05,766 - INFO - âœ… 21 produits Amazon scrapÃ©s avec succÃ¨s\n",
      "2025-06-27 16:15:05,785 - INFO - ğŸ’¾ DonnÃ©es sauvegardÃ©es: ../data/raw\\20250627_161505_production_amazon_products_laptop.csv (21 enregistrements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… 21 produits rÃ©cupÃ©rÃ©s\n",
      "\n",
      "ğŸ‰ WORKFLOW TERMINÃ‰ !\n",
      "ğŸ“Š Total des enregistrements: 21\n",
      "   ğŸ“„ amazon_products_laptop: 21 enregistrements\n",
      "\n",
      "ğŸ‰ WORKFLOW TERMINÃ‰ !\n",
      "ğŸ“Š Total des enregistrements: 21\n",
      "   ğŸ“„ amazon_products_laptop: 21 enregistrements\n"
     ]
    }
   ],
   "source": [
    "main_menu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2265b9f8",
   "metadata": {},
   "source": [
    "# Scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca2dfa9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… RobustProductReviewScout crÃ©Ã©\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SYSTÃˆME DE DRIVER ROBUSTE\n",
    "# ============================================================================\n",
    "\n",
    "def create_robust_chrome_options(headless=True):\n",
    "    \"\"\"\n",
    "    CrÃ©e des options Chrome 100% compatibles et robustes\n",
    "    \"\"\"\n",
    "    import undetected_chromedriver as uc\n",
    "    \n",
    "    try:\n",
    "        options = uc.ChromeOptions()\n",
    "        \n",
    "        # Arguments de base testÃ©s et sÃ»rs\n",
    "        safe_args = [\n",
    "            '--no-sandbox',\n",
    "            '--disable-dev-shm-usage',\n",
    "            '--disable-gpu',\n",
    "            '--disable-web-security',\n",
    "            '--disable-features=VizDisplayCompositor',\n",
    "            '--disable-extensions',\n",
    "            '--disable-plugins',\n",
    "            '--disable-default-apps',\n",
    "            '--disable-background-timer-throttling',\n",
    "            '--disable-backgrounding-occluded-windows',\n",
    "            '--disable-renderer-backgrounding',\n",
    "            '--disable-field-trial-config',\n",
    "            '--disable-back-forward-cache',\n",
    "            '--disable-ipc-flooding-protection',\n",
    "            '--window-size=1920,1080'\n",
    "        ]\n",
    "        \n",
    "        for arg in safe_args:\n",
    "            options.add_argument(arg)\n",
    "        \n",
    "        # Mode headless moderne\n",
    "        if headless:\n",
    "            options.add_argument('--headless=new')\n",
    "        \n",
    "        # User agent rÃ©aliste\n",
    "        try:\n",
    "            user_agent = random.choice(REALISTIC_USER_AGENTS)\n",
    "        except:\n",
    "            user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36'\n",
    "        \n",
    "        options.add_argument(f'--user-agent={user_agent}')\n",
    "        \n",
    "        # PrÃ©fÃ©rences sÃ»res uniquement\n",
    "        prefs = {\n",
    "            \"profile.default_content_setting_values.notifications\": 2,\n",
    "            \"profile.default_content_settings.popups\": 0,\n",
    "            \"profile.managed_default_content_settings.images\": 2\n",
    "        }\n",
    "        options.add_experimental_option(\"prefs\", prefs)\n",
    "        \n",
    "        return options\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Erreur options Chrome: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_robust_driver(headless=True, max_retries=3):\n",
    "    \"\"\"\n",
    "    CrÃ©e un driver UC robuste avec fallbacks\n",
    "    \"\"\"\n",
    "    import undetected_chromedriver as uc\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\"ğŸ”§ CrÃ©ation driver (tentative {attempt + 1}/{max_retries})...\")\n",
    "            \n",
    "            options = create_robust_chrome_options(headless)\n",
    "            if not options:\n",
    "                continue\n",
    "            \n",
    "            # CrÃ©er le driver avec paramÃ¨tres optimaux\n",
    "            driver = uc.Chrome(\n",
    "                options=options,\n",
    "                version_main=None,\n",
    "                headless=headless,\n",
    "                use_subprocess=False,\n",
    "                log_level=3\n",
    "            )\n",
    "            \n",
    "            # Test de fonctionnement\n",
    "            driver.get(\"data:text/html,<html><body><h1>Test OK</h1></body></html>\")\n",
    "            print(\"âœ… Driver UC crÃ©Ã© avec succÃ¨s!\")\n",
    "            return driver\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Tentative {attempt + 1} Ã©chouÃ©e: {str(e)[:100]}...\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(2)\n",
    "    \n",
    "    # Fallback vers Selenium classique\n",
    "    return create_selenium_fallback()\n",
    "\n",
    "def create_selenium_fallback():\n",
    "    \"\"\"Driver de secours Selenium classique\"\"\"\n",
    "    try:\n",
    "        print(\"ğŸ”„ Fallback vers Selenium classique...\")\n",
    "        \n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "        \n",
    "        options = Options()\n",
    "        options.add_argument('--no-sandbox')\n",
    "        options.add_argument('--disable-dev-shm-usage')\n",
    "        options.add_argument('--disable-gpu')\n",
    "        \n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        \n",
    "        print(\"âœ… Driver Selenium crÃ©Ã©!\")\n",
    "        return driver\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Fallback Ã©chouÃ©: {e}\")\n",
    "        return None\n",
    "\n",
    "# ============================================================================\n",
    "# CLASSE SCOUT ROBUSTE POUR DÃ‰TECTION DES BALISES\n",
    "# ============================================================================\n",
    "\n",
    "class RobustProductReviewScout:\n",
    "    \"\"\"\n",
    "    Scout robuste pour dÃ©tecter automatiquement les sÃ©lecteurs de produits et reviews\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.driver = None\n",
    "        self.detected_selectors = {}\n",
    "        \n",
    "    def setup_driver(self, headless=True):\n",
    "        \"\"\"Initialise le driver robuste\"\"\"\n",
    "        try:\n",
    "            self.driver = create_robust_driver(headless)\n",
    "            if self.driver:\n",
    "                print(\"âœ… Driver scout initialisÃ©\")\n",
    "                return True\n",
    "            else:\n",
    "                print(\"âŒ Ã‰chec initialisation driver scout\")\n",
    "                return False\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Erreur setup scout: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def detect_amazon_product_selectors(self, search_term=\"laptop\"):\n",
    "        \"\"\"DÃ©tecte les sÃ©lecteurs Amazon pour les produits\"\"\"\n",
    "        if not self.driver:\n",
    "            print(\"âŒ Driver non initialisÃ©\")\n",
    "            return {}\n",
    "        \n",
    "        try:\n",
    "            # Aller sur Amazon\n",
    "            search_url = f\"https://www.amazon.com/s?k={search_term.replace(' ', '+')}\"\n",
    "            print(f\"ğŸ” DÃ©tection sur: {search_url}\")\n",
    "            \n",
    "            self.driver.get(search_url)\n",
    "            time.sleep(5)  # Attendre le chargement\n",
    "            \n",
    "            # Test des sÃ©lecteurs possibles\n",
    "            selectors = {}\n",
    "            \n",
    "            # Conteneur de produit\n",
    "            product_containers = [\n",
    "                '[data-component-type=\"s-search-result\"]',\n",
    "                '.s-result-item',\n",
    "                '.s-widget-container'\n",
    "            ]\n",
    "            \n",
    "            for selector in product_containers:\n",
    "                try:\n",
    "                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                    if len(elements) >= 3:  # Au moins 3 produits\n",
    "                        selectors['product_container'] = selector\n",
    "                        print(f\"âœ… Conteneur produit: {selector} ({len(elements)} Ã©lÃ©ments)\")\n",
    "                        break\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            # Titre de produit\n",
    "            title_selectors = [\n",
    "                'h2 a span',\n",
    "                'h2 span',\n",
    "                '.s-size-mini span',\n",
    "                '.a-link-normal span'\n",
    "            ]\n",
    "            \n",
    "            for selector in title_selectors:\n",
    "                try:\n",
    "                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                    if elements and elements[0].text.strip():\n",
    "                        selectors['product_title'] = selector\n",
    "                        print(f\"âœ… Titre produit: {selector}\")\n",
    "                        break\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            # URL de produit\n",
    "            url_selectors = [\n",
    "                'h2 a',\n",
    "                '.a-link-normal',\n",
    "                '.s-link-style a'\n",
    "            ]\n",
    "            \n",
    "            for selector in url_selectors:\n",
    "                try:\n",
    "                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                    if elements and elements[0].get_attribute('href'):\n",
    "                        selectors['product_url'] = selector\n",
    "                        print(f\"âœ… URL produit: {selector}\")\n",
    "                        break\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            # Prix\n",
    "            price_selectors = [\n",
    "                '.a-price .a-offscreen',\n",
    "                '.a-price-whole',\n",
    "                '.a-price-range'\n",
    "            ]\n",
    "            \n",
    "            for selector in price_selectors:\n",
    "                try:\n",
    "                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                    if elements and elements[0].text.strip():\n",
    "                        selectors['product_price'] = selector\n",
    "                        print(f\"âœ… Prix produit: {selector}\")\n",
    "                        break\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            # Rating\n",
    "            rating_selectors = [\n",
    "                '.a-icon-alt',\n",
    "                '.a-star-rating',\n",
    "                '.a-rating'\n",
    "            ]\n",
    "            \n",
    "            for selector in rating_selectors:\n",
    "                try:\n",
    "                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                    if elements:\n",
    "                        selectors['product_rating'] = selector\n",
    "                        print(f\"âœ… Rating produit: {selector}\")\n",
    "                        break\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            return selectors\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Erreur dÃ©tection Amazon: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def detect_amazon_review_selectors(self, product_url):\n",
    "        \"\"\"DÃ©tecte les sÃ©lecteurs pour les reviews Amazon\"\"\"\n",
    "        if not self.driver:\n",
    "            return {}\n",
    "        \n",
    "        try:\n",
    "            # Aller sur la page produit\n",
    "            self.driver.get(product_url)\n",
    "            time.sleep(3)\n",
    "            \n",
    "            # Chercher le lien vers les reviews\n",
    "            review_links = [\n",
    "                'a[href*=\"customer-reviews\"]',\n",
    "                'a[href*=\"product-reviews\"]',\n",
    "                'a[data-hook*=\"see-all-reviews\"]'\n",
    "            ]\n",
    "            \n",
    "            review_url = None\n",
    "            for selector in review_links:\n",
    "                try:\n",
    "                    link = self.driver.find_element(By.CSS_SELECTOR, selector)\n",
    "                    review_url = link.get_attribute('href')\n",
    "                    break\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            if not review_url:\n",
    "                # Construire l'URL manuellement\n",
    "                import re\n",
    "                asin_match = re.search(r'/dp/([A-Z0-9]{10})', product_url)\n",
    "                if asin_match:\n",
    "                    asin = asin_match.group(1)\n",
    "                    review_url = f\"https://www.amazon.com/product-reviews/{asin}\"\n",
    "            \n",
    "            if not review_url:\n",
    "                print(\"âŒ URL reviews non trouvÃ©e\")\n",
    "                return {}\n",
    "            \n",
    "            # Aller sur la page des reviews\n",
    "            self.driver.get(review_url)\n",
    "            time.sleep(4)\n",
    "            \n",
    "            selectors = {}\n",
    "            \n",
    "            # Conteneur de review\n",
    "            review_containers = [\n",
    "                '[data-hook=\"review\"]',\n",
    "                '.review',\n",
    "                '.cr-original-review-content'\n",
    "            ]\n",
    "            \n",
    "            for selector in review_containers:\n",
    "                try:\n",
    "                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                    if len(elements) >= 2:\n",
    "                        selectors['reviews_container'] = selector\n",
    "                        print(f\"âœ… Conteneur review: {selector} ({len(elements)} Ã©lÃ©ments)\")\n",
    "                        break\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            # Titre de review\n",
    "            title_selectors = [\n",
    "                '[data-hook=\"review-title\"] span',\n",
    "                '.review-title span',\n",
    "                '.cr-original-review-content .review-title'\n",
    "            ]\n",
    "            \n",
    "            for selector in title_selectors:\n",
    "                try:\n",
    "                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                    if elements and elements[0].text.strip():\n",
    "                        selectors['review_title'] = selector\n",
    "                        print(f\"âœ… Titre review: {selector}\")\n",
    "                        break\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            # Texte de review\n",
    "            text_selectors = [\n",
    "                '[data-hook=\"review-body\"] span',\n",
    "                '.review-text span',\n",
    "                '.cr-original-review-content .review-text'\n",
    "            ]\n",
    "            \n",
    "            for selector in text_selectors:\n",
    "                try:\n",
    "                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                    if elements and elements[0].text.strip():\n",
    "                        selectors['review_text'] = selector\n",
    "                        print(f\"âœ… Texte review: {selector}\")\n",
    "                        break\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            # Rating de review\n",
    "            rating_selectors = [\n",
    "                '[data-hook=\"review-star-rating\"] .a-icon-alt',\n",
    "                '.review-rating .a-icon-alt',\n",
    "                '.a-star-rating .a-icon-alt'\n",
    "            ]\n",
    "            \n",
    "            for selector in rating_selectors:\n",
    "                try:\n",
    "                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                    if elements:\n",
    "                        selectors['review_rating'] = selector\n",
    "                        print(f\"âœ… Rating review: {selector}\")\n",
    "                        break\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            # Nom du reviewer\n",
    "            name_selectors = [\n",
    "                '.a-profile-name',\n",
    "                '.review-author',\n",
    "                '[data-hook=\"review-author\"]'\n",
    "            ]\n",
    "            \n",
    "            for selector in name_selectors:\n",
    "                try:\n",
    "                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                    if elements and elements[0].text.strip():\n",
    "                        selectors['reviewer_name'] = selector\n",
    "                        print(f\"âœ… Nom reviewer: {selector}\")\n",
    "                        break\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            # Date de review\n",
    "            date_selectors = [\n",
    "                '[data-hook=\"review-date\"]',\n",
    "                '.review-date',\n",
    "                '.a-color-secondary'\n",
    "            ]\n",
    "            \n",
    "            for selector in date_selectors:\n",
    "                try:\n",
    "                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                    if elements and elements[0].text.strip():\n",
    "                        selectors['review_date'] = selector\n",
    "                        print(f\"âœ… Date review: {selector}\")\n",
    "                        break\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            return selectors\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Erreur dÃ©tection reviews: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def save_selectors(self, site, product_selectors, review_selectors, filename=None):\n",
    "        \"\"\"Sauvegarde les sÃ©lecteurs dÃ©tectÃ©s\"\"\"\n",
    "        if not filename:\n",
    "            filename = f\"../config/robust_{site}_selectors.json\"\n",
    "        \n",
    "        try:\n",
    "            validated_selectors = {\n",
    "                site: {\n",
    "                    'products': product_selectors,\n",
    "                    'reviews': review_selectors,\n",
    "                    'detected_at': datetime.now().isoformat()\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                json.dump(validated_selectors, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "            print(f\"âœ… SÃ©lecteurs sauvegardÃ©s: {filename}\")\n",
    "            return filename\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Erreur sauvegarde: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Ferme le driver\"\"\"\n",
    "        if self.driver:\n",
    "            try:\n",
    "                self.driver.quit()\n",
    "            except:\n",
    "                pass\n",
    "            self.driver = None\n",
    "\n",
    "print(\"âœ… RobustProductReviewScout crÃ©Ã©\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a701baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Classe ProductReviewScraper crÃ©Ã©e\n"
     ]
    }
   ],
   "source": [
    "class ProductReviewScraper:\n",
    "    \"\"\"\n",
    "    Scraper spÃ©cialisÃ© pour rÃ©cupÃ©rer les reviews de produits avec balises validÃ©es\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, selectors_file=\"../config/product_review_selectors.json\"):\n",
    "        self.driver = None\n",
    "        self.selectors = self.load_selectors(selectors_file)\n",
    "        self.scraped_data = []\n",
    "        \n",
    "    def load_selectors(self, filename):\n",
    "        \"\"\"Charge les sÃ©lecteurs validÃ©s\"\"\"\n",
    "        try:\n",
    "            with open(filename, 'r', encoding='utf-8') as f:\n",
    "                return json.load(f)\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Erreur chargement sÃ©lecteurs: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def setup_driver(self, headless=True):\n",
    "        \"\"\"Configure le driver anti-dÃ©tection pour le scraping\"\"\"\n",
    "        try:\n",
    "            options = uc.ChromeOptions()\n",
    "            options.add_argument('--no-sandbox')\n",
    "            options.add_argument('--disable-dev-shm-usage')\n",
    "            options.add_argument('--disable-gpu')\n",
    "            options.add_argument('--disable-extensions')\n",
    "            options.add_argument('--disable-notifications')\n",
    "            options.add_argument('--no-first-run')\n",
    "            \n",
    "            if headless:\n",
    "                options.add_argument('--headless')\n",
    "            \n",
    "            # User agent alÃ©atoire\n",
    "            user_agent = random.choice(REALISTIC_USER_AGENTS)\n",
    "            options.add_argument(f'--user-agent={user_agent}')\n",
    "            \n",
    "            # Options expÃ©rimentales\n",
    "            options.add_experimental_option('useAutomationExtension', False)\n",
    "            options.add_experimental_option(\"prefs\", {\n",
    "                \"profile.default_content_setting_values.notifications\": 2\n",
    "            })\n",
    "            \n",
    "            self.driver = uc.Chrome(options=options, version_main=None)\n",
    "            \n",
    "            # Scripts anti-dÃ©tection\n",
    "            self.driver.execute_cdp_cmd('Page.addScriptToEvaluateOnNewDocument', {\n",
    "                'source': '''\n",
    "                    Object.defineProperty(navigator, 'webdriver', {\n",
    "                        get: () => undefined,\n",
    "                    });\n",
    "                '''\n",
    "            })\n",
    "            \n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Erreur setup driver: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def scrape_category_product_reviews(self, site_url, category_search, max_products=10, reviews_per_rating=50):\n",
    "        \"\"\"\n",
    "        Scrape les reviews de produits d'une catÃ©gorie spÃ©cifique\n",
    "        \n",
    "        Args:\n",
    "            site_url: URL du site (amazon.com, ebay.com)\n",
    "            category_search: terme de recherche pour la catÃ©gorie\n",
    "            max_products: nombre max de produits Ã  scraper (dÃ©faut: 10)\n",
    "            reviews_per_rating: nombre de reviews par note (dÃ©faut: 50)\n",
    "        \"\"\"\n",
    "        if not self.driver:\n",
    "            print(\"âŒ Driver non initialisÃ©\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        try:\n",
    "            print(f\"ğŸ” Recherche de produits pour: {category_search}\")\n",
    "            \n",
    "            # 1. RÃ©cupÃ©rer la liste des produits\n",
    "            products = self._get_products_list(site_url, category_search, max_products)\n",
    "            \n",
    "            if not products:\n",
    "                print(\"âŒ Aucun produit trouvÃ©\")\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            print(f\"âœ… {len(products)} produits trouvÃ©s\")\n",
    "            \n",
    "            # 2. Pour chaque produit, rÃ©cupÃ©rer les reviews\n",
    "            all_reviews = []\n",
    "            \n",
    "            for i, product in enumerate(products[:max_products], 1):\n",
    "                print(f\"ğŸ“¦ Produit {i}/{len(products)}: {product['title'][:50]}...\")\n",
    "                \n",
    "                product_reviews = self._scrape_product_reviews(\n",
    "                    product, \n",
    "                    reviews_per_rating\n",
    "                )\n",
    "                \n",
    "                if product_reviews:\n",
    "                    all_reviews.extend(product_reviews)\n",
    "                    print(f\"âœ… {len(product_reviews)} reviews rÃ©cupÃ©rÃ©es\")\n",
    "                else:\n",
    "                    print(\"âš ï¸ Aucune review trouvÃ©e\")\n",
    "                \n",
    "                # DÃ©lai entre produits\n",
    "                time.sleep(random.uniform(2, 5))\n",
    "            \n",
    "            # 3. CrÃ©er le DataFrame final\n",
    "            df = pd.DataFrame(all_reviews)\n",
    "            \n",
    "            if not df.empty:\n",
    "                # Nettoyage des donnÃ©es\n",
    "                df = self._clean_review_data(df)\n",
    "                print(f\"âœ… Total: {len(df)} reviews rÃ©cupÃ©rÃ©es\")\n",
    "            \n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Erreur scraping: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def _get_products_list(self, site_url, search_term, max_products):\n",
    "        \"\"\"RÃ©cupÃ¨re la liste des produits Ã  partir de la recherche\"\"\"\n",
    "        try:\n",
    "            # Construire l'URL de recherche\n",
    "            search_url = self._build_search_url(site_url, search_term)\n",
    "            print(f\"ğŸ”— URL: {search_url}\")\n",
    "            \n",
    "            self.driver.get(search_url)\n",
    "            time.sleep(3)\n",
    "            \n",
    "            products = []\n",
    "            site_type = 'amazon' if 'amazon' in site_url else 'ebay'\n",
    "            \n",
    "            # Utiliser les sÃ©lecteurs appropriÃ©s\n",
    "            if site_type in self.selectors and 'products' in self.selectors[site_type]:\n",
    "                selectors = self.selectors[site_type]['products']\n",
    "            else:\n",
    "                print(f\"âš ï¸ SÃ©lecteurs non trouvÃ©s pour {site_type}\")\n",
    "                return []\n",
    "            \n",
    "            # RÃ©cupÃ©rer les conteneurs de produits\n",
    "            product_containers = self.driver.find_elements(\n",
    "                By.CSS_SELECTOR, \n",
    "                selectors.get('product_container', '.s-result-item')\n",
    "            )\n",
    "            \n",
    "            for container in product_containers[:max_products]:\n",
    "                try:\n",
    "                    # Extraire les infos du produit\n",
    "                    title_elem = container.find_element(\n",
    "                        By.CSS_SELECTOR, \n",
    "                        selectors.get('product_title', 'h2 span')\n",
    "                    )\n",
    "                    \n",
    "                    url_elem = container.find_element(\n",
    "                        By.CSS_SELECTOR, \n",
    "                        selectors.get('product_url', 'h2 a')\n",
    "                    )\n",
    "                    \n",
    "                    product_data = {\n",
    "                        'title': title_elem.text.strip(),\n",
    "                        'url': url_elem.get_attribute('href'),\n",
    "                        'category': search_term\n",
    "                    }\n",
    "                    \n",
    "                    # Prix optionnel\n",
    "                    try:\n",
    "                        price_elem = container.find_element(\n",
    "                            By.CSS_SELECTOR, \n",
    "                            selectors.get('product_price', '.a-price')\n",
    "                        )\n",
    "                        product_data['price'] = price_elem.text.strip()\n",
    "                    except:\n",
    "                        product_data['price'] = 'N/A'\n",
    "                    \n",
    "                    # Rating optionnel\n",
    "                    try:\n",
    "                        rating_elem = container.find_element(\n",
    "                            By.CSS_SELECTOR, \n",
    "                            selectors.get('product_rating', '.a-icon-alt')\n",
    "                        )\n",
    "                        product_data['rating'] = rating_elem.get_attribute('textContent')\n",
    "                    except:\n",
    "                        product_data['rating'] = 'N/A'\n",
    "                    \n",
    "                    if product_data['title'] and product_data['url']:\n",
    "                        products.append(product_data)\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    continue  # Ignorer les produits problÃ©matiques\n",
    "            \n",
    "            return products\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Erreur rÃ©cupÃ©ration produits: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def _scrape_product_reviews(self, product, reviews_per_rating):\n",
    "        \"\"\"Scrape les reviews d'un produit spÃ©cifique\"\"\"\n",
    "        try:\n",
    "            # Aller sur la page produit\n",
    "            self.driver.get(product['url'])\n",
    "            time.sleep(2)\n",
    "            \n",
    "            # Chercher le lien vers les reviews\n",
    "            reviews_url = self._find_reviews_url(product['url'])\n",
    "            \n",
    "            if not reviews_url:\n",
    "                print(\"âš ï¸ Lien reviews non trouvÃ©\")\n",
    "                return []\n",
    "            \n",
    "            # Aller sur la page des reviews\n",
    "            self.driver.get(reviews_url)\n",
    "            time.sleep(3)\n",
    "            \n",
    "            reviews = []\n",
    "            site_type = 'amazon' if 'amazon' in product['url'] else 'ebay'\n",
    "            \n",
    "            # RÃ©cupÃ©rer les reviews par note (5, 4, 3, 2, 1 Ã©toiles)\n",
    "            for rating in [5, 4, 3, 2, 1]:\n",
    "                rating_reviews = self._scrape_reviews_by_rating(\n",
    "                    site_type, \n",
    "                    rating, \n",
    "                    reviews_per_rating,\n",
    "                    product\n",
    "                )\n",
    "                reviews.extend(rating_reviews)\n",
    "                \n",
    "                # DÃ©lai entre les notes\n",
    "                time.sleep(random.uniform(1, 3))\n",
    "            \n",
    "            return reviews\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Erreur scraping reviews produit: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def _find_reviews_url(self, product_url):\n",
    "        \"\"\"Trouve l'URL de la page des reviews\"\"\"\n",
    "        try:\n",
    "            # Chercher les liens vers les reviews\n",
    "            review_selectors = [\n",
    "                'a[href*=\"customer-reviews\"]',\n",
    "                'a[href*=\"reviews\"]',\n",
    "                'a[href*=\"review\"]',\n",
    "                '.a-link-emphasis[href*=\"review\"]'\n",
    "            ]\n",
    "            \n",
    "            for selector in review_selectors:\n",
    "                try:\n",
    "                    link = self.driver.find_element(By.CSS_SELECTOR, selector)\n",
    "                    return link.get_attribute('href')\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            # Si aucun lien trouvÃ©, construire l'URL\n",
    "            if 'amazon' in product_url:\n",
    "                # Extraire l'ASIN depuis l'URL\n",
    "                import re\n",
    "                asin_match = re.search(r'/dp/([A-Z0-9]{10})', product_url)\n",
    "                if asin_match:\n",
    "                    asin = asin_match.group(1)\n",
    "                    return f\"https://www.amazon.com/product-reviews/{asin}\"\n",
    "            \n",
    "            return None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Erreur recherche URL reviews: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _scrape_reviews_by_rating(self, site_type, rating, max_reviews, product_info):\n",
    "        \"\"\"Scrape les reviews pour une note spÃ©cifique\"\"\"\n",
    "        reviews = []\n",
    "        \n",
    "        try:\n",
    "            # Filtrer par note si possible\n",
    "            self._filter_by_rating(site_type, rating)\n",
    "            time.sleep(2)\n",
    "            \n",
    "            # RÃ©cupÃ©rer les reviews\n",
    "            selectors = self.selectors.get(site_type, {}).get('reviews', {})\n",
    "            pages_scraped = 0\n",
    "            max_pages = 10  # Limite de pages\n",
    "            \n",
    "            while len(reviews) < max_reviews and pages_scraped < max_pages:\n",
    "                # Reviews de la page actuelle\n",
    "                page_reviews = self._extract_reviews_from_page(selectors, product_info, rating)\n",
    "                \n",
    "                if not page_reviews:\n",
    "                    break\n",
    "                \n",
    "                reviews.extend(page_reviews)\n",
    "                \n",
    "                # Passer Ã  la page suivante\n",
    "                if not self._go_to_next_page(selectors):\n",
    "                    break\n",
    "                \n",
    "                pages_scraped += 1\n",
    "                time.sleep(random.uniform(2, 4))\n",
    "            \n",
    "            # Limiter au nombre demandÃ©\n",
    "            return reviews[:max_reviews]\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Erreur scraping rating {rating}: {e}\")\n",
    "            return reviews\n",
    "    \n",
    "    def _filter_by_rating(self, site_type, rating):\n",
    "        \"\"\"Filtre les reviews par note\"\"\"\n",
    "        try:\n",
    "            if site_type == 'amazon':\n",
    "                # Chercher le filtre par Ã©toiles\n",
    "                filter_selector = f'a[href*=\"filterByStar=five_star\"], a[href*=\"star_rating={rating}\"]'\n",
    "                filter_links = self.driver.find_elements(By.CSS_SELECTOR, filter_selector)\n",
    "                \n",
    "                for link in filter_links:\n",
    "                    if f\"{rating}\" in link.get_attribute('href') or f\"{rating} star\" in link.text:\n",
    "                        link.click()\n",
    "                        return True\n",
    "        except:\n",
    "            pass\n",
    "        return False\n",
    "    \n",
    "    def _extract_reviews_from_page(self, selectors, product_info, target_rating):\n",
    "        \"\"\"Extrait les reviews de la page actuelle\"\"\"\n",
    "        reviews = []\n",
    "        \n",
    "        try:\n",
    "            # RÃ©cupÃ©rer tous les conteneurs de reviews\n",
    "            review_containers = self.driver.find_elements(\n",
    "                By.CSS_SELECTOR, \n",
    "                selectors.get('reviews_container', '[data-hook=\"review\"]')\n",
    "            )\n",
    "            \n",
    "            for container in review_containers:\n",
    "                try:\n",
    "                    review_data = {\n",
    "                        'product_name': product_info['title'],\n",
    "                        'product_category': product_info['category'],\n",
    "                        'product_url': product_info['url'],\n",
    "                        'target_rating': target_rating\n",
    "                    }\n",
    "                    \n",
    "                    # Titre de la review\n",
    "                    try:\n",
    "                        title_elem = container.find_element(\n",
    "                            By.CSS_SELECTOR, \n",
    "                            selectors.get('review_title', '[data-hook=\"review-title\"]')\n",
    "                        )\n",
    "                        review_data['review_title'] = title_elem.text.strip()\n",
    "                    except:\n",
    "                        review_data['review_title'] = ''\n",
    "                    \n",
    "                    # Texte de la review\n",
    "                    try:\n",
    "                        text_elem = container.find_element(\n",
    "                            By.CSS_SELECTOR, \n",
    "                            selectors.get('review_text', '[data-hook=\"review-body\"]')\n",
    "                        )\n",
    "                        review_data['review_text'] = text_elem.text.strip()\n",
    "                    except:\n",
    "                        review_data['review_text'] = ''\n",
    "                    \n",
    "                    # Note de la review\n",
    "                    try:\n",
    "                        rating_elem = container.find_element(\n",
    "                            By.CSS_SELECTOR, \n",
    "                            selectors.get('review_rating', '.a-icon-alt')\n",
    "                        )\n",
    "                        rating_text = rating_elem.get_attribute('textContent') or rating_elem.text\n",
    "                        # Extraire le chiffre de la note\n",
    "                        import re\n",
    "                        rating_match = re.search(r'(\\d+(?:\\.\\d+)?)', rating_text)\n",
    "                        review_data['user_rating'] = rating_match.group(1) if rating_match else 'N/A'\n",
    "                    except:\n",
    "                        review_data['user_rating'] = 'N/A'\n",
    "                    \n",
    "                    # Nom du reviewer\n",
    "                    try:\n",
    "                        name_elem = container.find_element(\n",
    "                            By.CSS_SELECTOR, \n",
    "                            selectors.get('reviewer_name', '.a-profile-name')\n",
    "                        )\n",
    "                        review_data['reviewer_name'] = name_elem.text.strip()\n",
    "                    except:\n",
    "                        review_data['reviewer_name'] = 'Anonymous'\n",
    "                    \n",
    "                    # Date de la review\n",
    "                    try:\n",
    "                        date_elem = container.find_element(\n",
    "                            By.CSS_SELECTOR, \n",
    "                            selectors.get('review_date', '[data-hook=\"review-date\"]')\n",
    "                        )\n",
    "                        review_data['review_date'] = date_elem.text.strip()\n",
    "                    except:\n",
    "                        review_data['review_date'] = 'N/A'\n",
    "                    \n",
    "                    # Timestamp de scraping\n",
    "                    review_data['scraped_at'] = datetime.now().isoformat()\n",
    "                    \n",
    "                    # Ajouter seulement si on a du contenu\n",
    "                    if review_data['review_text'] or review_data['review_title']:\n",
    "                        reviews.append(review_data)\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    continue  # Ignorer les reviews problÃ©matiques\n",
    "            \n",
    "            return reviews\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Erreur extraction reviews: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def _go_to_next_page(self, selectors):\n",
    "        \"\"\"Passe Ã  la page suivante des reviews\"\"\"\n",
    "        try:\n",
    "            next_button = self.driver.find_element(\n",
    "                By.CSS_SELECTOR, \n",
    "                selectors.get('next_page', '.a-pagination .a-last a')\n",
    "            )\n",
    "            next_button.click()\n",
    "            time.sleep(2)\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def _build_search_url(self, site_url, search_term):\n",
    "        \"\"\"Construit l'URL de recherche\"\"\"\n",
    "        if 'amazon' in site_url:\n",
    "            return f\"https://www.amazon.com/s?k={search_term.replace(' ', '+')}\"\n",
    "        elif 'ebay' in site_url:\n",
    "            return f\"https://www.ebay.com/sch/i.html?_nkw={search_term.replace(' ', '+')}\"\n",
    "        return site_url\n",
    "    \n",
    "    def _clean_review_data(self, df):\n",
    "        \"\"\"Nettoie et structure les donnÃ©es de reviews\"\"\"\n",
    "        try:\n",
    "            # Supprimer les doublons\n",
    "            df = df.drop_duplicates(subset=['review_text', 'product_name'], keep='first')\n",
    "            \n",
    "            # Nettoyer les textes\n",
    "            df['review_text'] = df['review_text'].str.strip()\n",
    "            df['review_title'] = df['review_title'].str.strip()\n",
    "            \n",
    "            # Convertir les ratings en numÃ©rique\n",
    "            df['user_rating'] = pd.to_numeric(df['user_rating'], errors='coerce')\n",
    "            \n",
    "            # Ajouter une colonne de longueur de texte\n",
    "            df['review_length'] = df['review_text'].str.len()\n",
    "            \n",
    "            # Filtrer les reviews trop courtes\n",
    "            df = df[df['review_length'] > 10]\n",
    "            \n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Erreur nettoyage donnÃ©es: {e}\")\n",
    "            return df\n",
    "    \n",
    "    def save_reviews(self, df, filename=None):\n",
    "        \"\"\"Sauvegarde les reviews dans un fichier CSV\"\"\"\n",
    "        try:\n",
    "            if filename is None:\n",
    "                timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                filename = f\"../data/raw/product_reviews_{timestamp}.csv\"\n",
    "            \n",
    "            os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "            df.to_csv(filename, index=False, encoding='utf-8')\n",
    "            print(f\"âœ… Reviews sauvegardÃ©es: {filename}\")\n",
    "            return filename\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Erreur sauvegarde: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Ferme le driver\"\"\"\n",
    "        if self.driver:\n",
    "            self.driver.quit()\n",
    "\n",
    "print(\"âœ… Classe ProductReviewScraper crÃ©Ã©e\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c106055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Workflow de reviews de produits crÃ©Ã©\n",
      "ğŸ“– Utilisez reviews_workflow_menu() pour commencer\n"
     ]
    }
   ],
   "source": [
    "def product_reviews_workflow(category_search=\"laptop\", site=\"amazon\", max_products=10, reviews_per_rating=50):\n",
    "    \"\"\"\n",
    "    Workflow complet pour rÃ©cupÃ©rer les reviews de produits d'une catÃ©gorie\n",
    "    \n",
    "    Phase 1: DÃ©tection automatique des balises\n",
    "    Phase 2: Scraping des reviews avec balises validÃ©es\n",
    "    \n",
    "    Args:\n",
    "        category_search: catÃ©gorie de produits Ã  rechercher\n",
    "        site: site Ã  scraper ('amazon' ou 'ebay')\n",
    "        max_products: nombre de produits Ã  analyser (dÃ©faut: 10)\n",
    "        reviews_per_rating: nombre de reviews par note 1-5 Ã©toiles (dÃ©faut: 50)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"ğŸ¯ WORKFLOW SPÃ‰CIALISÃ‰ - REVIEWS DE PRODUITS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"ğŸ“¦ CatÃ©gorie: {category_search}\")\n",
    "    print(f\"ğŸŒ Site: {site}\")\n",
    "    print(f\"ğŸ“Š Produits: {max_products}\")\n",
    "    print(f\"â­ Reviews par note: {reviews_per_rating}\")\n",
    "    print(f\"ğŸ“ˆ Total estimÃ©: {max_products * 5 * reviews_per_rating} reviews max\")\n",
    "    print()\n",
    "    \n",
    "    # URLs des sites\n",
    "    site_urls = {\n",
    "        'amazon': 'https://www.amazon.com',\n",
    "        'ebay': 'https://www.ebay.com'\n",
    "    }\n",
    "    \n",
    "    if site not in site_urls:\n",
    "        print(f\"âŒ Site non supportÃ©: {site}\")\n",
    "        return None\n",
    "    \n",
    "    site_url = site_urls[site]\n",
    "    \n",
    "    # ============================================================================\n",
    "    # PHASE 1: DÃ‰TECTION DES BALISES\n",
    "    # ============================================================================\n",
    "    print(\"ğŸ” PHASE 1: DÃ‰TECTION AUTOMATIQUE DES BALISES\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    scout = ProductReviewScout()\n",
    "    \n",
    "    try:\n",
    "        # Setup du driver pour la dÃ©tection\n",
    "        if not scout.setup_driver(headless=True):\n",
    "            print(\"âŒ Ã‰chec initialisation driver de dÃ©tection\")\n",
    "            return None\n",
    "        \n",
    "        print(\"âœ… Driver de dÃ©tection initialisÃ©\")\n",
    "        \n",
    "        # DÃ©tection des sÃ©lecteurs de produits\n",
    "        print(f\"ğŸ” DÃ©tection des sÃ©lecteurs de produits sur {site}...\")\n",
    "        product_selectors = scout.detect_product_selectors(site_url, category_search)\n",
    "        \n",
    "        if not product_selectors:\n",
    "            print(\"âŒ Ã‰chec dÃ©tection sÃ©lecteurs produits\")\n",
    "            scout.close()\n",
    "            return None\n",
    "        \n",
    "        print(\"âœ… SÃ©lecteurs produits dÃ©tectÃ©s:\")\n",
    "        for key, value in product_selectors.items():\n",
    "            print(f\"   â€¢ {key}: {value}\")\n",
    "        \n",
    "        # Test sur un produit pour dÃ©tecter les sÃ©lecteurs de reviews\n",
    "        print(\"ğŸ” Test dÃ©tection sÃ©lecteurs de reviews...\")\n",
    "        \n",
    "        # Simuler la rÃ©cupÃ©ration d'un produit test\n",
    "        scout.driver.get(scout._build_search_url(site_url, category_search))\n",
    "        time.sleep(3)\n",
    "        \n",
    "        test_product_url = None\n",
    "        try:\n",
    "            # Trouver le premier produit\n",
    "            product_links = scout.driver.find_elements(By.CSS_SELECTOR, 'h2 a, .s-item__link')\n",
    "            if product_links:\n",
    "                test_product_url = product_links[0].get_attribute('href')\n",
    "                print(f\"ğŸ“¦ Produit test: {test_product_url[:80]}...\")\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        review_selectors = None\n",
    "        if test_product_url:\n",
    "            review_selectors = scout.detect_review_selectors(test_product_url)\n",
    "        \n",
    "        if review_selectors:\n",
    "            print(\"âœ… SÃ©lecteurs reviews dÃ©tectÃ©s:\")\n",
    "            for key, value in review_selectors.items():\n",
    "                print(f\"   â€¢ {key}: {value}\")\n",
    "        else:\n",
    "            print(\"âš ï¸ SÃ©lecteurs reviews non dÃ©tectÃ©s, utilisation des sÃ©lecteurs par dÃ©faut\")\n",
    "            review_selectors = scout.selectors.get(site, {}).get('reviews', {})\n",
    "        \n",
    "        # Sauvegarder les sÃ©lecteurs validÃ©s\n",
    "        validated_selectors = {\n",
    "            site: {\n",
    "                'products': product_selectors,\n",
    "                'reviews': review_selectors\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        selectors_file = f\"../config/product_review_selectors_{site}.json\"\n",
    "        if scout.save_selectors(validated_selectors, selectors_file):\n",
    "            print(f\"âœ… SÃ©lecteurs sauvegardÃ©s: {selectors_file}\")\n",
    "        \n",
    "        scout.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Erreur phase dÃ©tection: {e}\")\n",
    "        scout.close()\n",
    "        return None\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    \n",
    "    # ============================================================================\n",
    "    # PHASE 2: SCRAPING DES REVIEWS\n",
    "    # ============================================================================\n",
    "    print(\"ğŸ“Š PHASE 2: SCRAPING DES REVIEWS\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    scraper = ProductReviewScraper(selectors_file)\n",
    "    \n",
    "    try:\n",
    "        # Setup du driver pour le scraping\n",
    "        if not scraper.setup_driver(headless=False):  # Visible pour monitoring\n",
    "            print(\"âŒ Ã‰chec initialisation driver de scraping\")\n",
    "            return None\n",
    "        \n",
    "        print(\"âœ… Driver de scraping initialisÃ©\")\n",
    "        print(f\"ğŸ­ User-Agent: {scraper.driver.execute_script('return navigator.userAgent;')[:80]}...\")\n",
    "        \n",
    "        # Avertissement utilisateur\n",
    "        print(\"\\n\" + \"ğŸš¨ AVERTISSEMENT: Scraping en cours sur site rÃ©el!\")\n",
    "        print(\"â° Estimation durÃ©e: {} minutes\".format(max_products * 5))  # ~5min par produit\n",
    "        print(\"ğŸ“ Respect des ToS et limitations de dÃ©bit\")\n",
    "        \n",
    "        input(\"Appuyer sur EntrÃ©e pour continuer ou Ctrl+C pour annuler...\")\n",
    "        \n",
    "        # Lancement du scraping\n",
    "        print(f\"\\nğŸš€ DÃ©but du scraping pour '{category_search}'...\")\n",
    "        \n",
    "        df_reviews = scraper.scrape_category_product_reviews(\n",
    "            site_url=site_url,\n",
    "            category_search=category_search,\n",
    "            max_products=max_products,\n",
    "            reviews_per_rating=reviews_per_rating\n",
    "        )\n",
    "        \n",
    "        if df_reviews.empty:\n",
    "            print(\"âŒ Aucune review rÃ©cupÃ©rÃ©e\")\n",
    "            scraper.close()\n",
    "            return None\n",
    "        \n",
    "        # Analyse des rÃ©sultats\n",
    "        print(\"\\n\" + \"ğŸ“Š RÃ‰SULTATS DU SCRAPING\")\n",
    "        print(\"-\" * 30)\n",
    "        print(f\"âœ… Total reviews: {len(df_reviews)}\")\n",
    "        print(f\"ğŸ“¦ Produits uniques: {df_reviews['product_name'].nunique()}\")\n",
    "        print(f\"â­ Distribution des notes:\")\n",
    "        \n",
    "        rating_dist = df_reviews['user_rating'].value_counts().sort_index()\n",
    "        for rating, count in rating_dist.items():\n",
    "            print(f\"   {rating} Ã©toiles: {count} reviews\")\n",
    "        \n",
    "        print(f\"ğŸ“ Longueur moyenne: {df_reviews['review_length'].mean():.0f} caractÃ¨res\")\n",
    "        \n",
    "        # Sauvegarde\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        output_file = f\"../data/raw/{site}_{category_search}_reviews_{timestamp}.csv\"\n",
    "        \n",
    "        saved_file = scraper.save_reviews(df_reviews, output_file)\n",
    "        \n",
    "        if saved_file:\n",
    "            print(f\"âœ… DonnÃ©es sauvegardÃ©es: {saved_file}\")\n",
    "            \n",
    "            # AperÃ§u des donnÃ©es\n",
    "            print(\"\\nğŸ“‹ APERÃ‡U DES DONNÃ‰ES:\")\n",
    "            print(df_reviews[['product_name', 'user_rating', 'review_text']].head(3).to_string())\n",
    "            \n",
    "        scraper.close()\n",
    "        \n",
    "        print(\"\\n\" + \"ğŸ‰ WORKFLOW TERMINÃ‰ AVEC SUCCÃˆS!\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        return df_reviews\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nâ¹ï¸ ArrÃªt demandÃ© par l'utilisateur\")\n",
    "        scraper.close()\n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Erreur phase scraping: {e}\")\n",
    "        scraper.close()\n",
    "        return None\n",
    "\n",
    "def quick_review_test(product_url, max_reviews=20):\n",
    "    \"\"\"\n",
    "    Test rapide pour scraper les reviews d'un produit spÃ©cifique\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ§ª TEST RAPIDE - Reviews d'un produit\")\n",
    "    print(f\"ğŸ”— URL: {product_url}\")\n",
    "    print(f\"ğŸ“Š Reviews max: {max_reviews}\")\n",
    "    \n",
    "    scraper = ProductReviewScraper()\n",
    "    \n",
    "    try:\n",
    "        if not scraper.setup_driver(headless=False):\n",
    "            print(\"âŒ Ã‰chec setup driver\")\n",
    "            return None\n",
    "        \n",
    "        # Simuler un produit\n",
    "        fake_product = {\n",
    "            'title': 'Produit Test',\n",
    "            'url': product_url,\n",
    "            'category': 'test'\n",
    "        }\n",
    "        \n",
    "        # Scraper les reviews\n",
    "        reviews = scraper._scrape_product_reviews(fake_product, max_reviews)\n",
    "        \n",
    "        if reviews:\n",
    "            df = pd.DataFrame(reviews)\n",
    "            print(f\"âœ… {len(reviews)} reviews rÃ©cupÃ©rÃ©es\")\n",
    "            print(\"\\nğŸ“‹ AperÃ§u:\")\n",
    "            for i, review in enumerate(reviews[:3], 1):\n",
    "                print(f\"\\nReview {i}:\")\n",
    "                print(f\"  Note: {review.get('user_rating', 'N/A')}\")\n",
    "                print(f\"  Titre: {review.get('review_title', 'N/A')[:50]}...\")\n",
    "                print(f\"  Texte: {review.get('review_text', 'N/A')[:100]}...\")\n",
    "            \n",
    "            return df\n",
    "        else:\n",
    "            print(\"âŒ Aucune review trouvÃ©e\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Erreur test: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        scraper.close()\n",
    "\n",
    "def reviews_workflow_menu():\n",
    "    \"\"\"Menu principal pour le workflow de reviews\"\"\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"ğŸ¯ WORKFLOW REVIEWS DE PRODUITS - MENU PRINCIPAL\")\n",
    "    print(\"=\"*80)\n",
    "    print()\n",
    "    print(\"1ï¸âƒ£ Workflow complet (dÃ©tection + scraping)\")\n",
    "    print(\"2ï¸âƒ£ Test rapide sur un produit\")\n",
    "    print(\"3ï¸âƒ£ Configuration personnalisÃ©e\")\n",
    "    print(\"4ï¸âƒ£ Voir les sÃ©lecteurs sauvegardÃ©s\")\n",
    "    print(\"5ï¸âƒ£ Quitter\")\n",
    "    print()\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            choice = input(\"ğŸ‘‰ Votre choix (1-5): \").strip()\n",
    "            \n",
    "            if choice == '1':\n",
    "                # Workflow complet\n",
    "                print(\"\\nğŸ“‹ Configuration du workflow complet:\")\n",
    "                category = input(\"ğŸ·ï¸ CatÃ©gorie de produits (ex: 'laptop', 'smartphone'): \").strip() or \"laptop\"\n",
    "                site = input(\"ğŸŒ Site (amazon/ebay): \").strip() or \"amazon\"\n",
    "                \n",
    "                try:\n",
    "                    max_products = int(input(\"ğŸ“¦ Nombre de produits (dÃ©faut: 10): \") or \"10\")\n",
    "                    reviews_per_rating = int(input(\"â­ Reviews par note (dÃ©faut: 50): \") or \"50\")\n",
    "                except ValueError:\n",
    "                    max_products, reviews_per_rating = 10, 50\n",
    "                \n",
    "                return product_reviews_workflow(category, site, max_products, reviews_per_rating)\n",
    "                \n",
    "            elif choice == '2':\n",
    "                # Test rapide\n",
    "                product_url = input(\"ğŸ”— URL du produit Ã  tester: \").strip()\n",
    "                if product_url:\n",
    "                    try:\n",
    "                        max_reviews = int(input(\"ğŸ“Š Nombre max de reviews (dÃ©faut: 20): \") or \"20\")\n",
    "                    except ValueError:\n",
    "                        max_reviews = 20\n",
    "                    return quick_review_test(product_url, max_reviews)\n",
    "                else:\n",
    "                    print(\"âŒ URL requise\")\n",
    "                    \n",
    "            elif choice == '3':\n",
    "                # Configuration avancÃ©e\n",
    "                print(\"\\nâš™ï¸ Configuration personnalisÃ©e disponible dans product_reviews_workflow()\")\n",
    "                print(\"ğŸ“– Consultez la documentation de la fonction\")\n",
    "                \n",
    "            elif choice == '4':\n",
    "                # Voir sÃ©lecteurs\n",
    "                print(\"\\nğŸ“‹ SÃ©lecteurs sauvegardÃ©s:\")\n",
    "                for filename in ['../config/product_review_selectors_amazon.json', '../config/product_review_selectors_ebay.json']:\n",
    "                    if os.path.exists(filename):\n",
    "                        print(f\"âœ… {filename}\")\n",
    "                        try:\n",
    "                            with open(filename, 'r') as f:\n",
    "                                data = json.load(f)\n",
    "                                print(f\"   Sites: {list(data.keys())}\")\n",
    "                        except:\n",
    "                            pass\n",
    "                    else:\n",
    "                        print(f\"âŒ {filename} (non trouvÃ©)\")\n",
    "                        \n",
    "            elif choice == '5':\n",
    "                print(\"ğŸ‘‹ Ã€ bientÃ´t!\")\n",
    "                break\n",
    "                \n",
    "            else:\n",
    "                print(\"âŒ Choix invalide, veuillez rÃ©essayer\")\n",
    "                \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nğŸ‘‹ Ã€ bientÃ´t!\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Erreur: {e}\")\n",
    "\n",
    "print(\"âœ… Workflow de reviews de produits crÃ©Ã©\")\n",
    "print(\"ğŸ“– Utilisez reviews_workflow_menu() pour commencer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4218b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ WORKFLOW REVIEWS DE PRODUITS - PRÃŠT Ã€ UTILISER\n",
      "================================================================================\n",
      "\n",
      "ğŸ“‹ EXEMPLES D'UTILISATION:\n",
      "\n",
      "1ï¸âƒ£ Workflow complet automatique:\n",
      "   df = product_reviews_workflow('laptop', 'amazon', 10, 50)\n",
      "   # RÃ©cupÃ¨re 50 reviews par note (1-5) pour 10 laptops sur Amazon\n",
      "\n",
      "2ï¸âƒ£ Menu interactif:\n",
      "   reviews_workflow_menu()\n",
      "   # Interface guidÃ©e pour configurer le scraping\n",
      "\n",
      "3ï¸âƒ£ Test rapide d'un produit:\n",
      "   df = quick_review_test('https://amazon.com/dp/XXXXXXXXXX', 20)\n",
      "   # Test sur un produit spÃ©cifique\n",
      "\n",
      "4ï¸âƒ£ Configurations personnalisÃ©es:\n",
      "   # Smartphones sur eBay\n",
      "   df = product_reviews_workflow('smartphone', 'ebay', 5, 30)\n",
      "\n",
      "   # Casques audio sur Amazon\n",
      "   df = product_reviews_workflow('headphones', 'amazon', 15, 40)\n",
      "\n",
      "ğŸ“Š DONNÃ‰ES RÃ‰CUPÃ‰RÃ‰ES:\n",
      "   â€¢ product_name: nom du produit\n",
      "   â€¢ product_category: catÃ©gorie recherchÃ©e\n",
      "   â€¢ review_title: titre de la review\n",
      "   â€¢ review_text: texte complet de la review\n",
      "   â€¢ user_rating: note donnÃ©e (1-5)\n",
      "   â€¢ reviewer_name: nom du reviewer\n",
      "   â€¢ review_date: date de la review\n",
      "   â€¢ scraped_at: timestamp du scraping\n",
      "\n",
      "ğŸ’¾ SAUVEGARDE AUTOMATIQUE:\n",
      "   â€¢ Format CSV dans ../data/raw/\n",
      "   â€¢ Nom: {site}_{categorie}_reviews_{timestamp}.csv\n",
      "\n",
      "ğŸ›¡ï¸ SÃ‰CURITÃ‰:\n",
      "   â€¢ Anti-dÃ©tection avec user-agents alÃ©atoires\n",
      "   â€¢ DÃ©lais humains entre requÃªtes\n",
      "   â€¢ Respect des limitations de dÃ©bit\n",
      "   â€¢ Options Chrome optimisÃ©es\n",
      "\n",
      "âš ï¸ IMPORTANT:\n",
      "   â€¢ Respecter les ToS des sites\n",
      "   â€¢ Utiliser avec modÃ©ration\n",
      "   â€¢ VÃ©rifier robots.txt\n",
      "   â€¢ Ne pas surcharger les serveurs\n",
      "\n",
      "ğŸš€ Pour commencer, utilisez:\n",
      "   reviews_workflow_menu()\n",
      "\n",
      "âœ… Workflow prÃªt! Tapez reviews_workflow_menu() pour commencer\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# EXEMPLES D'UTILISATION DU WORKFLOW REVIEWS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"ğŸ¯ WORKFLOW REVIEWS DE PRODUITS - PRÃŠT Ã€ UTILISER\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "print(\"ğŸ“‹ EXEMPLES D'UTILISATION:\")\n",
    "print()\n",
    "print(\"1ï¸âƒ£ Workflow complet automatique:\")\n",
    "print(\"   df = product_reviews_workflow('laptop', 'amazon', 10, 50)\")\n",
    "print(\"   # RÃ©cupÃ¨re 50 reviews par note (1-5) pour 10 laptops sur Amazon\")\n",
    "print()\n",
    "print(\"2ï¸âƒ£ Menu interactif:\")\n",
    "print(\"   reviews_workflow_menu()\")\n",
    "print(\"   # Interface guidÃ©e pour configurer le scraping\")\n",
    "print()\n",
    "print(\"3ï¸âƒ£ Test rapide d'un produit:\")\n",
    "print(\"   df = quick_review_test('https://amazon.com/dp/XXXXXXXXXX', 20)\")\n",
    "print(\"   # Test sur un produit spÃ©cifique\")\n",
    "print()\n",
    "print(\"4ï¸âƒ£ Configurations personnalisÃ©es:\")\n",
    "print(\"   # Smartphones sur eBay\")\n",
    "print(\"   df = product_reviews_workflow('smartphone', 'ebay', 5, 30)\")\n",
    "print()\n",
    "print(\"   # Casques audio sur Amazon\")\n",
    "print(\"   df = product_reviews_workflow('headphones', 'amazon', 15, 40)\")\n",
    "print()\n",
    "print(\"ğŸ“Š DONNÃ‰ES RÃ‰CUPÃ‰RÃ‰ES:\")\n",
    "print(\"   â€¢ product_name: nom du produit\")\n",
    "print(\"   â€¢ product_category: catÃ©gorie recherchÃ©e\")\n",
    "print(\"   â€¢ review_title: titre de la review\")\n",
    "print(\"   â€¢ review_text: texte complet de la review\")\n",
    "print(\"   â€¢ user_rating: note donnÃ©e (1-5)\")\n",
    "print(\"   â€¢ reviewer_name: nom du reviewer\")\n",
    "print(\"   â€¢ review_date: date de la review\")\n",
    "print(\"   â€¢ scraped_at: timestamp du scraping\")\n",
    "print()\n",
    "print(\"ğŸ’¾ SAUVEGARDE AUTOMATIQUE:\")\n",
    "print(\"   â€¢ Format CSV dans ../data/raw/\")\n",
    "print(\"   â€¢ Nom: {site}_{categorie}_reviews_{timestamp}.csv\")\n",
    "print()\n",
    "print(\"ğŸ›¡ï¸ SÃ‰CURITÃ‰:\")\n",
    "print(\"   â€¢ Anti-dÃ©tection avec user-agents alÃ©atoires\")\n",
    "print(\"   â€¢ DÃ©lais humains entre requÃªtes\")\n",
    "print(\"   â€¢ Respect des limitations de dÃ©bit\")\n",
    "print(\"   â€¢ Options Chrome optimisÃ©es\")\n",
    "print()\n",
    "print(\"âš ï¸ IMPORTANT:\")\n",
    "print(\"   â€¢ Respecter les ToS des sites\")\n",
    "print(\"   â€¢ Utiliser avec modÃ©ration\")\n",
    "print(\"   â€¢ VÃ©rifier robots.txt\")\n",
    "print(\"   â€¢ Ne pas surcharger les serveurs\")\n",
    "print()\n",
    "print(\"ğŸš€ Pour commencer, utilisez:\")\n",
    "print(\"   reviews_workflow_menu()\")\n",
    "\n",
    "# Exemple de configuration prÃªte Ã  l'emploi\n",
    "EXAMPLE_CONFIGS = {\n",
    "    'laptops_amazon': {\n",
    "        'category_search': 'laptop gaming',\n",
    "        'site': 'amazon',\n",
    "        'max_products': 8,\n",
    "        'reviews_per_rating': 40,\n",
    "        'description': 'Reviews de laptops gaming sur Amazon'\n",
    "    },\n",
    "    'smartphones_ebay': {\n",
    "        'category_search': 'smartphone iphone',\n",
    "        'site': 'ebay', \n",
    "        'max_products': 5,\n",
    "        'reviews_per_rating': 30,\n",
    "        'description': 'Reviews d\\'iPhones sur eBay'\n",
    "    },\n",
    "    'headphones_amazon': {\n",
    "        'category_search': 'wireless headphones',\n",
    "        'site': 'amazon',\n",
    "        'max_products': 12,\n",
    "        'reviews_per_rating': 35,\n",
    "        'description': 'Reviews de casques sans-fil sur Amazon'\n",
    "    }\n",
    "}\n",
    "\n",
    "def run_example_config(config_name):\n",
    "    \"\"\"ExÃ©cute une configuration d'exemple\"\"\"\n",
    "    if config_name in EXAMPLE_CONFIGS:\n",
    "        config = EXAMPLE_CONFIGS[config_name]\n",
    "        print(f\"ğŸš€ Lancement: {config['description']}\")\n",
    "        return product_reviews_workflow(**{k:v for k,v in config.items() if k != 'description'})\n",
    "    else:\n",
    "        print(f\"âŒ Configuration '{config_name}' non trouvÃ©e\")\n",
    "        print(f\"ğŸ“‹ Disponibles: {list(EXAMPLE_CONFIGS.keys())}\")\n",
    "        return None\n",
    "\n",
    "print(\"\\nâœ… Workflow prÃªt! Tapez reviews_workflow_menu() pour commencer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6933b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ¯ WORKFLOW REVIEWS DE PRODUITS - MENU PRINCIPAL\n",
      "================================================================================\n",
      "\n",
      "1ï¸âƒ£ Workflow complet (dÃ©tection + scraping)\n",
      "2ï¸âƒ£ Test rapide sur un produit\n",
      "3ï¸âƒ£ Configuration personnalisÃ©e\n",
      "4ï¸âƒ£ Voir les sÃ©lecteurs sauvegardÃ©s\n",
      "5ï¸âƒ£ Quitter\n",
      "\n",
      "\n",
      "ğŸ“‹ Configuration du workflow complet:\n",
      "\n",
      "ğŸ“‹ Configuration du workflow complet:\n",
      "================================================================================\n",
      "ğŸ¯ WORKFLOW SPÃ‰CIALISÃ‰ - REVIEWS DE PRODUITS\n",
      "================================================================================\n",
      "ğŸ“¦ CatÃ©gorie: laptop\n",
      "ğŸŒ Site: amazon\n",
      "ğŸ“Š Produits: 10\n",
      "â­ Reviews par note: 10\n",
      "ğŸ“ˆ Total estimÃ©: 500 reviews max\n",
      "\n",
      "ğŸ” PHASE 1: DÃ‰TECTION AUTOMATIQUE DES BALISES\n",
      "--------------------------------------------------\n",
      "âŒ Erreur: name 'ProductReviewScout' is not defined\n",
      "\n",
      "ğŸ“‹ Configuration du workflow complet:\n",
      "================================================================================\n",
      "ğŸ¯ WORKFLOW SPÃ‰CIALISÃ‰ - REVIEWS DE PRODUITS\n",
      "================================================================================\n",
      "ğŸ“¦ CatÃ©gorie: laptop\n",
      "ğŸŒ Site: amazon\n",
      "ğŸ“Š Produits: 10\n",
      "â­ Reviews par note: 10\n",
      "ğŸ“ˆ Total estimÃ©: 500 reviews max\n",
      "\n",
      "ğŸ” PHASE 1: DÃ‰TECTION AUTOMATIQUE DES BALISES\n",
      "--------------------------------------------------\n",
      "âŒ Erreur: name 'ProductReviewScout' is not defined\n",
      "\n",
      "ğŸ“‹ Configuration du workflow complet:\n",
      "================================================================================\n",
      "ğŸ¯ WORKFLOW SPÃ‰CIALISÃ‰ - REVIEWS DE PRODUITS\n",
      "================================================================================\n",
      "ğŸ“¦ CatÃ©gorie: escape\n",
      "ğŸŒ Site: escape\n",
      "ğŸ“Š Produits: 10\n",
      "â­ Reviews par note: 50\n",
      "ğŸ“ˆ Total estimÃ©: 2500 reviews max\n",
      "\n",
      "âŒ Site non supportÃ©: escape\n",
      "================================================================================\n",
      "ğŸ¯ WORKFLOW SPÃ‰CIALISÃ‰ - REVIEWS DE PRODUITS\n",
      "================================================================================\n",
      "ğŸ“¦ CatÃ©gorie: escape\n",
      "ğŸŒ Site: escape\n",
      "ğŸ“Š Produits: 10\n",
      "â­ Reviews par note: 50\n",
      "ğŸ“ˆ Total estimÃ©: 2500 reviews max\n",
      "\n",
      "âŒ Site non supportÃ©: escape\n"
     ]
    }
   ],
   "source": [
    "reviews_workflow_menu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66698f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ª Test du systÃ¨me de crÃ©ation de driver robuste...\n",
      "ğŸ”§ Tentative 1/3 - CrÃ©ation driver...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-29 14:58:09,103 - INFO - patching driver executable C:\\Users\\Yann\\appdata\\roaming\\undetected_chromedriver\\undetected_chromedriver.exe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Driver robuste crÃ©Ã© avec succÃ¨s!\n",
      "âœ… Navigation test rÃ©ussie!\n",
      "âœ… SystÃ¨me de driver robuste prÃªt!\n",
      "âœ… Navigation test rÃ©ussie!\n",
      "âœ… SystÃ¨me de driver robuste prÃªt!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CORRECTION ROBUSTE DES OPTIONS CHROME\n",
    "# ============================================================================\n",
    "\n",
    "def create_robust_chrome_options(headless=True):\n",
    "    \"\"\"\n",
    "    CrÃ©e des options Chrome 100% compatibles avec toutes les versions\n",
    "    Ã‰vite toutes les options problÃ©matiques\n",
    "    \"\"\"\n",
    "    import undetected_chromedriver as uc\n",
    "    \n",
    "    try:\n",
    "        options = uc.ChromeOptions()\n",
    "        \n",
    "        # Arguments de base sÃ»rs et testÃ©s\n",
    "        safe_args = [\n",
    "            '--no-sandbox',\n",
    "            '--disable-dev-shm-usage',\n",
    "            '--disable-gpu',\n",
    "            '--disable-web-security',\n",
    "            '--disable-features=VizDisplayCompositor',\n",
    "            '--disable-extensions',\n",
    "            '--disable-plugins',\n",
    "            '--disable-default-apps',\n",
    "            '--disable-background-timer-throttling',\n",
    "            '--disable-backgrounding-occluded-windows',\n",
    "            '--disable-renderer-backgrounding',\n",
    "            '--disable-field-trial-config',\n",
    "            '--disable-back-forward-cache',\n",
    "            '--disable-ipc-flooding-protection',\n",
    "            '--window-size=1920,1080',\n",
    "            '--remote-debugging-port=9222'\n",
    "        ]\n",
    "        \n",
    "        # Ajouter les arguments sÃ»rs\n",
    "        for arg in safe_args:\n",
    "            options.add_argument(arg)\n",
    "        \n",
    "        # Mode headless si demandÃ©\n",
    "        if headless:\n",
    "            options.add_argument('--headless=new')  # Nouveau mode headless\n",
    "        \n",
    "        # User agent alÃ©atoire\n",
    "        try:\n",
    "            user_agent = random.choice(REALISTIC_USER_AGENTS)\n",
    "            options.add_argument(f'--user-agent={user_agent}')\n",
    "        except:\n",
    "            # Fallback si REALISTIC_USER_AGENTS n'existe pas\n",
    "            options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')\n",
    "        \n",
    "        # PrÃ©fÃ©rences sÃ»res SEULEMENT\n",
    "        prefs = {\n",
    "            \"profile.default_content_setting_values.notifications\": 2,\n",
    "            \"profile.default_content_settings.popups\": 0,\n",
    "            \"profile.managed_default_content_settings.images\": 2,\n",
    "            \"profile.default_content_setting_values.media_stream_mic\": 2,\n",
    "            \"profile.default_content_setting_values.media_stream_camera\": 2,\n",
    "            \"profile.default_content_setting_values.geolocation\": 2\n",
    "        }\n",
    "        \n",
    "        options.add_experimental_option(\"prefs\", prefs)\n",
    "        \n",
    "        # NE PAS AJOUTER: excludeSwitches, useAutomationExtension\n",
    "        # Ces options causent des erreurs dans les nouvelles versions\n",
    "        \n",
    "        return options\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Erreur crÃ©ation options Chrome: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_robust_driver(headless=True, max_retries=3):\n",
    "    \"\"\"\n",
    "    CrÃ©e un driver robuste avec plusieurs tentatives et fallbacks\n",
    "    \"\"\"\n",
    "    import undetected_chromedriver as uc\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\"ğŸ”§ Tentative {attempt + 1}/{max_retries} - CrÃ©ation driver...\")\n",
    "            \n",
    "            # Options robustes\n",
    "            options = create_robust_chrome_options(headless)\n",
    "            if not options:\n",
    "                continue\n",
    "            \n",
    "            # CrÃ©ation du driver avec paramÃ¨tres optimaux\n",
    "            driver = uc.Chrome(\n",
    "                options=options,\n",
    "                version_main=None,  # Auto-dÃ©tection\n",
    "                headless=headless,\n",
    "                use_subprocess=False,\n",
    "                log_level=3  # RÃ©duire les logs\n",
    "            )\n",
    "            \n",
    "            # Test rapide\n",
    "            driver.get(\"data:text/html,<html><body><h1>Test</h1></body></html>\")\n",
    "            \n",
    "            print(\"âœ… Driver robuste crÃ©Ã© avec succÃ¨s!\")\n",
    "            return driver\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Tentative {attempt + 1} Ã©chouÃ©e: {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                print(\"ğŸ”„ Nouvelle tentative...\")\n",
    "                time.sleep(2)\n",
    "            else:\n",
    "                print(\"âŒ Toutes les tentatives ont Ã©chouÃ©\")\n",
    "                return create_selenium_fallback_driver()\n",
    "    \n",
    "    return None\n",
    "\n",
    "def create_selenium_fallback_driver():\n",
    "    \"\"\"\n",
    "    Driver de secours avec Selenium classique\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"ğŸ”„ Fallback vers Selenium classique...\")\n",
    "        \n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "        \n",
    "        # Options Selenium classiques\n",
    "        options = Options()\n",
    "        options.add_argument('--no-sandbox')\n",
    "        options.add_argument('--disable-dev-shm-usage')\n",
    "        options.add_argument('--disable-gpu')\n",
    "        options.add_argument('--window-size=1920,1080')\n",
    "        \n",
    "        # Service gÃ©rÃ© automatiquement\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        \n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        \n",
    "        print(\"âœ… Driver Selenium classique crÃ©Ã©!\")\n",
    "        return driver\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Fallback Selenium Ã©chouÃ©: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test du systÃ¨me robuste\n",
    "print(\"ğŸ§ª Test du systÃ¨me de crÃ©ation de driver robuste...\")\n",
    "\n",
    "test_driver = create_robust_driver(headless=True)\n",
    "if test_driver:\n",
    "    try:\n",
    "        test_driver.get(\"https://httpbin.org/user-agent\")\n",
    "        print(\"âœ… Navigation test rÃ©ussie!\")\n",
    "        test_driver.quit()\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Test navigation: {e}\")\n",
    "        test_driver.quit()\n",
    "else:\n",
    "    print(\"âŒ Impossible de crÃ©er un driver robuste\")\n",
    "\n",
    "print(\"âœ… SystÃ¨me de driver robuste prÃªt!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d981db",
   "metadata": {},
   "source": [
    "# Robus Scrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40c0611e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scout robuste crÃ©Ã©!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SCOUT ROBUSTE POUR DÃ‰TECTION DE BALISES\n",
    "# ============================================================================\n",
    "\n",
    "class RobustProductReviewScout:\n",
    "    \"\"\"\n",
    "    Scout ultra-robuste pour dÃ©tecter automatiquement les balises de produits et reviews\n",
    "    Avec gestion d'erreurs complÃ¨te et fallbacks multiples\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.driver = None\n",
    "        self.detected_selectors = {}\n",
    "        self.base_selectors = {\n",
    "            'amazon': {\n",
    "                'products': {\n",
    "                    'containers': [\n",
    "                        '[data-component-type=\"s-search-result\"]',\n",
    "                        '.s-result-item',\n",
    "                        '.s-widget-container .s-card-container',\n",
    "                        '[data-asin]:not([data-asin=\"\"])'\n",
    "                    ],\n",
    "                    'titles': [\n",
    "                        'h2 span',\n",
    "                        'h2 a span',\n",
    "                        '.s-size-mini span',\n",
    "                        '.a-size-base-plus',\n",
    "                        '[data-cy=\"title-recipe-title\"]'\n",
    "                    ],\n",
    "                    'urls': [\n",
    "                        'h2 a',\n",
    "                        '.a-link-normal',\n",
    "                        'a[href*=\"/dp/\"]',\n",
    "                        'a[href*=\"/gp/product/\"]'\n",
    "                    ],\n",
    "                    'prices': [\n",
    "                        '.a-price .a-offscreen',\n",
    "                        '.a-price-whole',\n",
    "                        '.a-price-range .a-offscreen',\n",
    "                        '.a-price-symbol + .a-price-whole'\n",
    "                    ],\n",
    "                    'ratings': [\n",
    "                        '.a-icon-alt',\n",
    "                        '.a-star-mini .a-icon-alt',\n",
    "                        'span[aria-label*=\"stars\"]'\n",
    "                    ]\n",
    "                },\n",
    "                'reviews': {\n",
    "                    'containers': [\n",
    "                        '[data-hook=\"review\"]',\n",
    "                        '.review',\n",
    "                        '.cr-original-review-content',\n",
    "                        '.reviewText'\n",
    "                    ],\n",
    "                    'titles': [\n",
    "                        '[data-hook=\"review-title\"] span',\n",
    "                        '.review-title',\n",
    "                        '.cr-original-review-content .review-title'\n",
    "                    ],\n",
    "                    'texts': [\n",
    "                        '[data-hook=\"review-body\"] span',\n",
    "                        '.review-text',\n",
    "                        '.cr-original-review-content .review-text',\n",
    "                        '.reviewText'\n",
    "                    ],\n",
    "                    'ratings': [\n",
    "                        '[data-hook=\"review-star-rating\"] .a-icon-alt',\n",
    "                        '.review-rating .a-icon-alt',\n",
    "                        '.cr-original-review-content .a-icon-alt'\n",
    "                    ],\n",
    "                    'authors': [\n",
    "                        '.a-profile-name',\n",
    "                        '.review-author',\n",
    "                        '.cr-original-review-content .author'\n",
    "                    ],\n",
    "                    'dates': [\n",
    "                        '[data-hook=\"review-date\"]',\n",
    "                        '.review-date',\n",
    "                        '.cr-original-review-content .review-date'\n",
    "                    ]\n",
    "                }\n",
    "            },\n",
    "            'ebay': {\n",
    "                'products': {\n",
    "                    'containers': [\n",
    "                        '.s-item',\n",
    "                        '.srp-results .s-item',\n",
    "                        '.b-listing__wrap'\n",
    "                    ],\n",
    "                    'titles': [\n",
    "                        '.s-item__title',\n",
    "                        '.it-ttl',\n",
    "                        '.b-listing__title'\n",
    "                    ],\n",
    "                    'urls': [\n",
    "                        '.s-item__link',\n",
    "                        '.it-ttl a',\n",
    "                        '.b-listing__title a'\n",
    "                    ],\n",
    "                    'prices': [\n",
    "                        '.s-item__price',\n",
    "                        '.notranslate',\n",
    "                        '.b-listing__price'\n",
    "                    ]\n",
    "                },\n",
    "                'reviews': {\n",
    "                    'containers': [\n",
    "                        '.review-item',\n",
    "                        '.ebay-review',\n",
    "                        '.reviews .review'\n",
    "                    ],\n",
    "                    'texts': [\n",
    "                        '.review-item-content',\n",
    "                        '.ebay-review-text',\n",
    "                        '.review-content'\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def setup_robust_driver(self, headless=True, timeout=30):\n",
    "        \"\"\"Configuration driver ultra-robuste avec fallbacks\"\"\"\n",
    "        \n",
    "        max_attempts = 3\n",
    "        \n",
    "        for attempt in range(max_attempts):\n",
    "            try:\n",
    "                print(f\"ğŸ”§ Tentative {attempt + 1}/{max_attempts} - Setup driver scout...\")\n",
    "                \n",
    "                # Fermer le driver existant si nÃ©cessaire\n",
    "                if self.driver:\n",
    "                    try:\n",
    "                        self.driver.quit()\n",
    "                    except:\n",
    "                        pass\n",
    "                    self.driver = None\n",
    "                \n",
    "                # Options ultra-sÃ»res\n",
    "                options = self._create_safe_options(headless)\n",
    "                \n",
    "                # CrÃ©ation driver avec timeout\n",
    "                import undetected_chromedriver as uc\n",
    "                \n",
    "                self.driver = uc.Chrome(\n",
    "                    options=options,\n",
    "                    version_main=None,\n",
    "                    headless=headless,\n",
    "                    use_subprocess=False,\n",
    "                    log_level=3\n",
    "                )\n",
    "                \n",
    "                # Configuration des timeouts\n",
    "                self.driver.set_page_load_timeout(timeout)\n",
    "                self.driver.implicitly_wait(10)\n",
    "                \n",
    "                # Test de fonctionnement\n",
    "                self.driver.get(\"data:text/html,<html><body><h1>Test Scout</h1></body></html>\")\n",
    "                \n",
    "                print(\"âœ… Driver scout initialisÃ© avec succÃ¨s!\")\n",
    "                return True\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Tentative {attempt + 1} Ã©chouÃ©e: {str(e)[:100]}...\")\n",
    "                if attempt < max_attempts - 1:\n",
    "                    time.sleep(3)\n",
    "                else:\n",
    "                    print(\"âŒ Impossible de crÃ©er le driver scout\")\n",
    "                    return False\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def _create_safe_options(self, headless=True):\n",
    "        \"\"\"CrÃ©e des options Chrome ultra-sÃ»res\"\"\"\n",
    "        \n",
    "        import undetected_chromedriver as uc\n",
    "        \n",
    "        options = uc.ChromeOptions()\n",
    "        \n",
    "        # Arguments de base seulement\n",
    "        safe_args = [\n",
    "            '--no-sandbox',\n",
    "            '--disable-dev-shm-usage',\n",
    "            '--disable-gpu',\n",
    "            '--disable-web-security',\n",
    "            '--disable-features=VizDisplayCompositor',\n",
    "            '--window-size=1920,1080',\n",
    "            '--start-maximized'\n",
    "        ]\n",
    "        \n",
    "        for arg in safe_args:\n",
    "            options.add_argument(arg)\n",
    "        \n",
    "        if headless:\n",
    "            options.add_argument('--headless=new')\n",
    "        \n",
    "        # User agent alÃ©atoire\n",
    "        try:\n",
    "            user_agent = random.choice(REALISTIC_USER_AGENTS)\n",
    "            options.add_argument(f'--user-agent={user_agent}')\n",
    "        except:\n",
    "            options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')\n",
    "        \n",
    "        # PrÃ©fÃ©rences minimales\n",
    "        prefs = {\n",
    "            \"profile.default_content_setting_values.notifications\": 2,\n",
    "            \"profile.default_content_settings.popups\": 0\n",
    "        }\n",
    "        options.add_experimental_option(\"prefs\", prefs)\n",
    "        \n",
    "        return options\n",
    "    \n",
    "    def detect_site_selectors(self, site_url, search_term=\"laptop\", max_retries=3):\n",
    "        \"\"\"\n",
    "        DÃ©tecte automatiquement tous les sÃ©lecteurs pour un site\n",
    "        \"\"\"\n",
    "        \n",
    "        if not self.driver:\n",
    "            print(\"âŒ Driver non initialisÃ©\")\n",
    "            return {}\n",
    "        \n",
    "        site_type = 'amazon' if 'amazon' in site_url else 'ebay' if 'ebay' in site_url else 'unknown'\n",
    "        \n",
    "        if site_type == 'unknown':\n",
    "            print(f\"âŒ Site non supportÃ©: {site_url}\")\n",
    "            return {}\n",
    "        \n",
    "        print(f\"ğŸ” DÃ©tection des sÃ©lecteurs pour {site_type}...\")\n",
    "        \n",
    "        detected = {\n",
    "            'site': site_type,\n",
    "            'products': {},\n",
    "            'reviews': {}\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Phase 1: DÃ©tection sÃ©lecteurs produits\n",
    "            product_selectors = self._detect_product_selectors(site_url, search_term, site_type)\n",
    "            detected['products'] = product_selectors\n",
    "            \n",
    "            if product_selectors:\n",
    "                print(f\"âœ… SÃ©lecteurs produits dÃ©tectÃ©s: {len(product_selectors)}\")\n",
    "                \n",
    "                # Phase 2: DÃ©tection sÃ©lecteurs reviews\n",
    "                review_selectors = self._detect_review_selectors(site_type, product_selectors)\n",
    "                detected['reviews'] = review_selectors\n",
    "                \n",
    "                if review_selectors:\n",
    "                    print(f\"âœ… SÃ©lecteurs reviews dÃ©tectÃ©s: {len(review_selectors)}\")\n",
    "                else:\n",
    "                    print(\"âš ï¸ SÃ©lecteurs reviews non dÃ©tectÃ©s, utilisation des dÃ©fauts\")\n",
    "                    detected['reviews'] = self.base_selectors[site_type]['reviews']\n",
    "            else:\n",
    "                print(\"âŒ Impossible de dÃ©tecter les sÃ©lecteurs produits\")\n",
    "                return {}\n",
    "            \n",
    "            # Sauvegarde des sÃ©lecteurs\n",
    "            self._save_detected_selectors(detected, site_type)\n",
    "            \n",
    "            return detected\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Erreur dÃ©tection: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def _detect_product_selectors(self, site_url, search_term, site_type):\n",
    "        \"\"\"DÃ©tecte les sÃ©lecteurs de produits avec tests multiples\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Construire URL de recherche\n",
    "            if site_type == 'amazon':\n",
    "                search_url = f\"https://www.amazon.com/s?k={search_term.replace(' ', '+')}\"\n",
    "            elif site_type == 'ebay':\n",
    "                search_url = f\"https://www.ebay.com/sch/i.html?_nkw={search_term.replace(' ', '+')}\"\n",
    "            else:\n",
    "                return {}\n",
    "            \n",
    "            print(f\"ğŸŒ Navigation vers: {search_url}\")\n",
    "            self.driver.get(search_url)\n",
    "            \n",
    "            # Attendre le chargement\n",
    "            time.sleep(5)\n",
    "            \n",
    "            # Tester les sÃ©lecteurs de conteneurs\n",
    "            selectors = {}\n",
    "            base_selectors = self.base_selectors[site_type]['products']\n",
    "            \n",
    "            # Test conteneurs de produits\n",
    "            for selector in base_selectors['containers']:\n",
    "                try:\n",
    "                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                    if len(elements) >= 3:  # Au moins 3 produits\n",
    "                        selectors['container'] = selector\n",
    "                        print(f\"âœ… Conteneur: {selector} ({len(elements)} Ã©lÃ©ments)\")\n",
    "                        break\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            if not selectors.get('container'):\n",
    "                print(\"âŒ Aucun conteneur de produit trouvÃ©\")\n",
    "                return {}\n",
    "            \n",
    "            # Test autres sÃ©lecteurs dans le contexte du conteneur\n",
    "            container_elements = self.driver.find_elements(By.CSS_SELECTOR, selectors['container'])\n",
    "            \n",
    "            if container_elements:\n",
    "                first_container = container_elements[0]\n",
    "                \n",
    "                # Test titres\n",
    "                for title_selector in base_selectors['titles']:\n",
    "                    try:\n",
    "                        title_elem = first_container.find_element(By.CSS_SELECTOR, title_selector)\n",
    "                        if title_elem.text.strip():\n",
    "                            selectors['title'] = title_selector\n",
    "                            print(f\"âœ… Titre: {title_selector}\")\n",
    "                            break\n",
    "                    except:\n",
    "                        continue\n",
    "                \n",
    "                # Test URLs\n",
    "                for url_selector in base_selectors['urls']:\n",
    "                    try:\n",
    "                        url_elem = first_container.find_element(By.CSS_SELECTOR, url_selector)\n",
    "                        href = url_elem.get_attribute('href')\n",
    "                        if href and ('amazon.com' in href or 'ebay.com' in href):\n",
    "                            selectors['url'] = url_selector\n",
    "                            print(f\"âœ… URL: {url_selector}\")\n",
    "                            break\n",
    "                    except:\n",
    "                        continue\n",
    "                \n",
    "                # Test prix\n",
    "                for price_selector in base_selectors['prices']:\n",
    "                    try:\n",
    "                        price_elem = first_container.find_element(By.CSS_SELECTOR, price_selector)\n",
    "                        if price_elem.text.strip() and ('$' in price_elem.text or 'â‚¬' in price_elem.text):\n",
    "                            selectors['price'] = price_selector\n",
    "                            print(f\"âœ… Prix: {price_selector}\")\n",
    "                            break\n",
    "                    except:\n",
    "                        continue\n",
    "                \n",
    "                # Test ratings\n",
    "                for rating_selector in base_selectors['ratings']:\n",
    "                    try:\n",
    "                        rating_elem = first_container.find_element(By.CSS_SELECTOR, rating_selector)\n",
    "                        rating_text = rating_elem.get_attribute('textContent') or rating_elem.text\n",
    "                        if rating_text and ('star' in rating_text.lower() or 'Ã©toile' in rating_text.lower()):\n",
    "                            selectors['rating'] = rating_selector\n",
    "                            print(f\"âœ… Rating: {rating_selector}\")\n",
    "                            break\n",
    "                    except:\n",
    "                        continue\n",
    "            \n",
    "            return selectors\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Erreur dÃ©tection produits: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def _detect_review_selectors(self, site_type, product_selectors):\n",
    "        \"\"\"DÃ©tecte les sÃ©lecteurs de reviews en naviguant vers un produit\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Trouver un lien produit\n",
    "            if not product_selectors.get('url'):\n",
    "                print(\"âŒ Pas de sÃ©lecteur URL disponible\")\n",
    "                return {}\n",
    "            \n",
    "            # RÃ©cupÃ©rer le premier lien produit\n",
    "            product_links = self.driver.find_elements(By.CSS_SELECTOR, product_selectors['url'])\n",
    "            \n",
    "            if not product_links:\n",
    "                print(\"âŒ Aucun lien produit trouvÃ©\")\n",
    "                return {}\n",
    "            \n",
    "            product_url = product_links[0].get_attribute('href')\n",
    "            print(f\"ğŸ”— Test reviews sur: {product_url[:80]}...\")\n",
    "            \n",
    "            # Naviguer vers le produit\n",
    "            self.driver.get(product_url)\n",
    "            time.sleep(3)\n",
    "            \n",
    "            # Chercher les reviews sur la page produit ou naviguer vers la page reviews\n",
    "            review_selectors = {}\n",
    "            base_selectors = self.base_selectors[site_type]['reviews']\n",
    "            \n",
    "            # D'abord, chercher un lien vers les reviews\n",
    "            review_link_selectors = [\n",
    "                'a[href*=\"customer-reviews\"]',\n",
    "                'a[href*=\"reviews\"]',\n",
    "                'a[href*=\"review\"]',\n",
    "                '.cr-widget-ACR a'\n",
    "            ]\n",
    "            \n",
    "            review_page_found = False\n",
    "            for link_selector in review_link_selectors:\n",
    "                try:\n",
    "                    review_links = self.driver.find_elements(By.CSS_SELECTOR, link_selector)\n",
    "                    for link in review_links:\n",
    "                        href = link.get_attribute('href')\n",
    "                        if href and 'review' in href:\n",
    "                            print(f\"ğŸ”— Navigation vers page reviews: {href[:80]}...\")\n",
    "                            self.driver.get(href)\n",
    "                            time.sleep(3)\n",
    "                            review_page_found = True\n",
    "                            break\n",
    "                    if review_page_found:\n",
    "                        break\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            # Tester les sÃ©lecteurs de reviews\n",
    "            # Test conteneurs\n",
    "            for container_selector in base_selectors['containers']:\n",
    "                try:\n",
    "                    review_elements = self.driver.find_elements(By.CSS_SELECTOR, container_selector)\n",
    "                    if len(review_elements) >= 2:  # Au moins 2 reviews\n",
    "                        review_selectors['container'] = container_selector\n",
    "                        print(f\"âœ… Conteneur reviews: {container_selector} ({len(review_elements)} reviews)\")\n",
    "                        break\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            if review_selectors.get('container'):\n",
    "                # Tester les autres sÃ©lecteurs dans le contexte\n",
    "                review_containers = self.driver.find_elements(By.CSS_SELECTOR, review_selectors['container'])\n",
    "                \n",
    "                if review_containers:\n",
    "                    first_review = review_containers[0]\n",
    "                    \n",
    "                    # Test texte de review\n",
    "                    for text_selector in base_selectors['texts']:\n",
    "                        try:\n",
    "                            text_elem = first_review.find_element(By.CSS_SELECTOR, text_selector)\n",
    "                            if text_elem.text.strip() and len(text_elem.text.strip()) > 20:\n",
    "                                review_selectors['text'] = text_selector\n",
    "                                print(f\"âœ… Texte review: {text_selector}\")\n",
    "                                break\n",
    "                        except:\n",
    "                            continue\n",
    "                    \n",
    "                    # Test titre de review\n",
    "                    for title_selector in base_selectors['titles']:\n",
    "                        try:\n",
    "                            title_elem = first_review.find_element(By.CSS_SELECTOR, title_selector)\n",
    "                            if title_elem.text.strip():\n",
    "                                review_selectors['title'] = title_selector\n",
    "                                print(f\"âœ… Titre review: {title_selector}\")\n",
    "                                break\n",
    "                        except:\n",
    "                            continue\n",
    "                    \n",
    "                    # Test rating\n",
    "                    for rating_selector in base_selectors['ratings']:\n",
    "                        try:\n",
    "                            rating_elem = first_review.find_element(By.CSS_SELECTOR, rating_selector)\n",
    "                            rating_text = rating_elem.get_attribute('textContent') or rating_elem.text\n",
    "                            if rating_text and ('star' in rating_text.lower() or 'Ã©toile' in rating_text.lower()):\n",
    "                                review_selectors['rating'] = rating_selector\n",
    "                                print(f\"âœ… Rating review: {rating_selector}\")\n",
    "                                break\n",
    "                        except:\n",
    "                            continue\n",
    "                    \n",
    "                    # Test auteur\n",
    "                    for author_selector in base_selectors['authors']:\n",
    "                        try:\n",
    "                            author_elem = first_review.find_element(By.CSS_SELECTOR, author_selector)\n",
    "                            if author_elem.text.strip():\n",
    "                                review_selectors['author'] = author_selector\n",
    "                                print(f\"âœ… Auteur review: {author_selector}\")\n",
    "                                break\n",
    "                        except:\n",
    "                            continue\n",
    "                    \n",
    "                    # Test date\n",
    "                    for date_selector in base_selectors['dates']:\n",
    "                        try:\n",
    "                            date_elem = first_review.find_element(By.CSS_SELECTOR, date_selector)\n",
    "                            if date_elem.text.strip():\n",
    "                                review_selectors['date'] = date_selector\n",
    "                                print(f\"âœ… Date review: {date_selector}\")\n",
    "                                break\n",
    "                        except:\n",
    "                            continue\n",
    "            \n",
    "            return review_selectors\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Erreur dÃ©tection reviews: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def _save_detected_selectors(self, selectors, site_type):\n",
    "        \"\"\"Sauvegarde les sÃ©lecteurs dÃ©tectÃ©s\"\"\"\n",
    "        \n",
    "        try:\n",
    "            import os\n",
    "            import json\n",
    "            \n",
    "            config_dir = \"../config\"\n",
    "            os.makedirs(config_dir, exist_ok=True)\n",
    "            \n",
    "            filename = f\"{config_dir}/detected_selectors_{site_type}.json\"\n",
    "            \n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                json.dump(selectors, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "            print(f\"âœ… SÃ©lecteurs sauvegardÃ©s: {filename}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Erreur sauvegarde: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Ferme proprement le driver\"\"\"\n",
    "        if self.driver:\n",
    "            try:\n",
    "                self.driver.quit()\n",
    "                print(\"âœ… Driver scout fermÃ©\")\n",
    "            except:\n",
    "                pass\n",
    "            self.driver = None\n",
    "\n",
    "print(\"âœ… Scout robuste crÃ©Ã©!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06957f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scraper robuste crÃ©Ã©!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SCRAPER ROBUSTE POUR REVIEWS DE PRODUITS\n",
    "# ============================================================================\n",
    "\n",
    "class RobustProductReviewScraper:\n",
    "    \"\"\"\n",
    "    Scraper ultra-robuste pour rÃ©cupÃ©rer les reviews de produits\n",
    "    Utilise les sÃ©lecteurs dÃ©tectÃ©s par le scout\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, selectors_file=None):\n",
    "        self.driver = None\n",
    "        self.selectors = {}\n",
    "        self.scraped_data = []\n",
    "        self.session_stats = {\n",
    "            'products_processed': 0,\n",
    "            'reviews_collected': 0,\n",
    "            'errors': 0,\n",
    "            'start_time': None\n",
    "        }\n",
    "        \n",
    "        if selectors_file:\n",
    "            self.load_selectors(selectors_file)\n",
    "    \n",
    "    def load_selectors(self, filename):\n",
    "        \"\"\"Charge les sÃ©lecteurs depuis un fichier JSON\"\"\"\n",
    "        try:\n",
    "            import json\n",
    "            with open(filename, 'r', encoding='utf-8') as f:\n",
    "                self.selectors = json.load(f)\n",
    "            print(f\"âœ… SÃ©lecteurs chargÃ©s: {filename}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Erreur chargement sÃ©lecteurs: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def setup_robust_driver(self, headless=False, timeout=60):\n",
    "        \"\"\"Configuration driver ultra-robuste pour scraping\"\"\"\n",
    "        \n",
    "        max_attempts = 3\n",
    "        \n",
    "        for attempt in range(max_attempts):\n",
    "            try:\n",
    "                print(f\"ğŸš€ Tentative {attempt + 1}/{max_attempts} - Setup driver scraper...\")\n",
    "                \n",
    "                # Fermer le driver existant\n",
    "                if self.driver:\n",
    "                    try:\n",
    "                        self.driver.quit()\n",
    "                    except:\n",
    "                        pass\n",
    "                    self.driver = None\n",
    "                \n",
    "                # Options anti-dÃ©tection\n",
    "                options = self._create_stealth_options(headless)\n",
    "                \n",
    "                # CrÃ©ation du driver\n",
    "                import undetected_chromedriver as uc\n",
    "                \n",
    "                self.driver = uc.Chrome(\n",
    "                    options=options,\n",
    "                    version_main=None,\n",
    "                    headless=headless,\n",
    "                    use_subprocess=False,\n",
    "                    log_level=3\n",
    "                )\n",
    "                \n",
    "                # Configuration timeouts\n",
    "                self.driver.set_page_load_timeout(timeout)\n",
    "                self.driver.implicitly_wait(15)\n",
    "                \n",
    "                # Scripts anti-dÃ©tection\n",
    "                self._inject_stealth_scripts()\n",
    "                \n",
    "                # Test fonctionnement\n",
    "                self.driver.get(\"data:text/html,<html><body><h1>Scraper Ready</h1></body></html>\")\n",
    "                \n",
    "                print(\"âœ… Driver scraper prÃªt!\")\n",
    "                print(f\"ğŸ­ User-Agent: {self.driver.execute_script('return navigator.userAgent;')[:80]}...\")\n",
    "                \n",
    "                return True\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Tentative {attempt + 1} Ã©chouÃ©e: {str(e)[:100]}...\")\n",
    "                if attempt < max_attempts - 1:\n",
    "                    time.sleep(5)\n",
    "                else:\n",
    "                    print(\"âŒ Impossible de crÃ©er le driver scraper\")\n",
    "                    return False\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def _create_stealth_options(self, headless=True):\n",
    "        \"\"\"CrÃ©e des options Chrome furtives\"\"\"\n",
    "        \n",
    "        import undetected_chromedriver as uc\n",
    "        \n",
    "        options = uc.ChromeOptions()\n",
    "        \n",
    "        # Arguments furtifs\n",
    "        stealth_args = [\n",
    "            '--no-sandbox',\n",
    "            '--disable-dev-shm-usage',\n",
    "            '--disable-gpu',\n",
    "            '--disable-web-security',\n",
    "            '--disable-features=VizDisplayCompositor',\n",
    "            '--disable-blink-features=AutomationControlled',\n",
    "            '--disable-extensions',\n",
    "            '--no-first-run',\n",
    "            '--no-default-browser-check',\n",
    "            '--disable-default-apps',\n",
    "            '--disable-background-timer-throttling',\n",
    "            '--disable-backgrounding-occluded-windows',\n",
    "            '--disable-renderer-backgrounding',\n",
    "            '--window-size=1920,1080',\n",
    "            '--start-maximized'\n",
    "        ]\n",
    "        \n",
    "        for arg in stealth_args:\n",
    "            options.add_argument(arg)\n",
    "        \n",
    "        if headless:\n",
    "            options.add_argument('--headless=new')\n",
    "        \n",
    "        # User agent alÃ©atoire rÃ©aliste\n",
    "        try:\n",
    "            user_agent = random.choice(REALISTIC_USER_AGENTS)\n",
    "            options.add_argument(f'--user-agent={user_agent}')\n",
    "        except:\n",
    "            options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')\n",
    "        \n",
    "        # PrÃ©fÃ©rences furtives\n",
    "        prefs = {\n",
    "            \"profile.default_content_setting_values.notifications\": 2,\n",
    "            \"profile.default_content_settings.popups\": 0,\n",
    "            \"profile.managed_default_content_settings.images\": 1,  # Charger les images\n",
    "            \"profile.default_content_setting_values.plugins\": 1,\n",
    "            \"profile.content_settings.plugin_whitelist.adobe-flash-player\": 1,\n",
    "            \"profile.content_settings.exceptions.plugins.*,*.per_resource.adobe-flash-player\": 1\n",
    "        }\n",
    "        options.add_experimental_option(\"prefs\", prefs)\n",
    "        \n",
    "        return options\n",
    "    \n",
    "    def _inject_stealth_scripts(self):\n",
    "        \"\"\"Injecte des scripts anti-dÃ©tection\"\"\"\n",
    "        try:\n",
    "            stealth_script = '''\n",
    "                Object.defineProperty(navigator, 'webdriver', {\n",
    "                    get: () => undefined,\n",
    "                });\n",
    "                \n",
    "                Object.defineProperty(navigator, 'plugins', {\n",
    "                    get: () => [1, 2, 3, 4, 5],\n",
    "                });\n",
    "                \n",
    "                Object.defineProperty(navigator, 'languages', {\n",
    "                    get: () => ['en-US', 'en'],\n",
    "                });\n",
    "                \n",
    "                window.chrome = {\n",
    "                    runtime: {},\n",
    "                };\n",
    "                \n",
    "                Object.defineProperty(navigator, 'permissions', {\n",
    "                    get: () => ({\n",
    "                        query: () => Promise.resolve({ state: 'granted' }),\n",
    "                    }),\n",
    "                });\n",
    "            '''\n",
    "            \n",
    "            self.driver.execute_cdp_cmd('Page.addScriptToEvaluateOnNewDocument', {\n",
    "                'source': stealth_script\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Scripts anti-dÃ©tection non injectÃ©s: {e}\")\n",
    "    \n",
    "    def scrape_category_reviews(self, category, site='amazon', max_products=10, reviews_per_rating=50):\n",
    "        \"\"\"\n",
    "        Scrape principal pour rÃ©cupÃ©rer les reviews d'une catÃ©gorie\n",
    "        \"\"\"\n",
    "        \n",
    "        if not self.driver:\n",
    "            print(\"âŒ Driver non initialisÃ©\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        if not self.selectors:\n",
    "            print(\"âŒ SÃ©lecteurs non chargÃ©s\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        print(\"=\"*80)\n",
    "        print(\"ğŸ¯ DÃ‰BUT DU SCRAPING ROBUSTE\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"ğŸ“¦ CatÃ©gorie: {category}\")\n",
    "        print(f\"ğŸŒ Site: {site}\")\n",
    "        print(f\"ğŸ“Š Produits max: {max_products}\")\n",
    "        print(f\"â­ Reviews par note: {reviews_per_rating}\")\n",
    "        print()\n",
    "        \n",
    "        # Initialiser les stats\n",
    "        self.session_stats['start_time'] = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Phase 1: RÃ©cupÃ©rer la liste des produits\n",
    "            products = self._get_products_list(category, site, max_products)\n",
    "            \n",
    "            if not products:\n",
    "                print(\"âŒ Aucun produit trouvÃ©\")\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            print(f\"âœ… {len(products)} produits trouvÃ©s\")\n",
    "            \n",
    "            # Phase 2: Scraper les reviews de chaque produit\n",
    "            all_reviews = []\n",
    "            \n",
    "            for i, product in enumerate(products, 1):\n",
    "                print(f\"\\nğŸ“¦ PRODUIT {i}/{len(products)}\")\n",
    "                print(f\"ğŸ·ï¸ {product['title'][:60]}...\")\n",
    "                \n",
    "                try:\n",
    "                    product_reviews = self._scrape_product_reviews(\n",
    "                        product, \n",
    "                        reviews_per_rating,\n",
    "                        max_pages=5\n",
    "                    )\n",
    "                    \n",
    "                    if product_reviews:\n",
    "                        all_reviews.extend(product_reviews)\n",
    "                        self.session_stats['reviews_collected'] += len(product_reviews)\n",
    "                        print(f\"âœ… {len(product_reviews)} reviews rÃ©cupÃ©rÃ©es\")\n",
    "                    else:\n",
    "                        print(\"âš ï¸ Aucune review trouvÃ©e\")\n",
    "                    \n",
    "                    self.session_stats['products_processed'] += 1\n",
    "                    \n",
    "                    # DÃ©lai humain entre produits\n",
    "                    delay = random.uniform(3, 8)\n",
    "                    print(f\"â³ DÃ©lai: {delay:.1f}s...\")\n",
    "                    time.sleep(delay)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"âŒ Erreur produit {i}: {e}\")\n",
    "                    self.session_stats['errors'] += 1\n",
    "                    continue\n",
    "            \n",
    "            # Phase 3: CrÃ©er et nettoyer le DataFrame\n",
    "            if all_reviews:\n",
    "                df = pd.DataFrame(all_reviews)\n",
    "                df = self._clean_review_data(df)\n",
    "                \n",
    "                # Stats finales\n",
    "                duration = time.time() - self.session_stats['start_time']\n",
    "                print(\"\\n\" + \"=\"*80)\n",
    "                print(\"ğŸ“Š SCRAPING TERMINÃ‰ - STATISTIQUES\")\n",
    "                print(\"=\"*80)\n",
    "                print(f\"â±ï¸ DurÃ©e: {duration/60:.1f} minutes\")\n",
    "                print(f\"ğŸ“¦ Produits traitÃ©s: {self.session_stats['products_processed']}\")\n",
    "                print(f\"ğŸ“ Reviews rÃ©cupÃ©rÃ©es: {self.session_stats['reviews_collected']}\")\n",
    "                print(f\"âŒ Erreurs: {self.session_stats['errors']}\")\n",
    "                print(f\"ğŸ“ˆ Taux de succÃ¨s: {(1-self.session_stats['errors']/max(1,len(products)))*100:.1f}%\")\n",
    "                \n",
    "                return df\n",
    "            else:\n",
    "                print(\"âŒ Aucune review rÃ©cupÃ©rÃ©e\")\n",
    "                return pd.DataFrame()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Erreur scraping global: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def _get_products_list(self, category, site, max_products):\n",
    "        \"\"\"RÃ©cupÃ¨re la liste des produits Ã  scraper\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # URL de recherche\n",
    "            if site == 'amazon':\n",
    "                search_url = f\"https://www.amazon.com/s?k={category.replace(' ', '+')}\"\n",
    "            elif site == 'ebay':\n",
    "                search_url = f\"https://www.ebay.com/sch/i.html?_nkw={category.replace(' ', '+')}\"\n",
    "            else:\n",
    "                print(f\"âŒ Site non supportÃ©: {site}\")\n",
    "                return []\n",
    "            \n",
    "            print(f\"ğŸ” Recherche: {search_url}\")\n",
    "            self.driver.get(search_url)\n",
    "            \n",
    "            # Attendre le chargement\n",
    "            time.sleep(5)\n",
    "            \n",
    "            # SÃ©lecteurs pour ce site\n",
    "            site_selectors = self.selectors.get('products', {})\n",
    "            \n",
    "            if not site_selectors:\n",
    "                print(\"âŒ SÃ©lecteurs produits non disponibles\")\n",
    "                return []\n",
    "            \n",
    "            # RÃ©cupÃ©rer les conteneurs de produits\n",
    "            container_selector = site_selectors.get('container')\n",
    "            if not container_selector:\n",
    "                print(\"âŒ SÃ©lecteur conteneur manquant\")\n",
    "                return []\n",
    "            \n",
    "            containers = self.driver.find_elements(By.CSS_SELECTOR, container_selector)\n",
    "            print(f\"ğŸ“¦ {len(containers)} conteneurs trouvÃ©s\")\n",
    "            \n",
    "            products = []\n",
    "            \n",
    "            for i, container in enumerate(containers[:max_products]):\n",
    "                try:\n",
    "                    product_data = self._extract_product_info(container, site_selectors, category)\n",
    "                    \n",
    "                    if product_data and product_data.get('url'):\n",
    "                        products.append(product_data)\n",
    "                        print(f\"âœ… Produit {len(products)}: {product_data['title'][:40]}...\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"âš ï¸ Produit {i+1} ignorÃ©: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            return products\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Erreur rÃ©cupÃ©ration produits: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def _extract_product_info(self, container, selectors, category):\n",
    "        \"\"\"Extrait les infos d'un produit depuis son conteneur\"\"\"\n",
    "        \n",
    "        product_data = {\n",
    "            'category': category,\n",
    "            'scraped_at': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        # Titre\n",
    "        try:\n",
    "            if selectors.get('title'):\n",
    "                title_elem = container.find_element(By.CSS_SELECTOR, selectors['title'])\n",
    "                product_data['title'] = title_elem.text.strip()\n",
    "        except:\n",
    "            product_data['title'] = 'Titre non trouvÃ©'\n",
    "        \n",
    "        # URL\n",
    "        try:\n",
    "            if selectors.get('url'):\n",
    "                url_elem = container.find_element(By.CSS_SELECTOR, selectors['url'])\n",
    "                product_data['url'] = url_elem.get_attribute('href')\n",
    "        except:\n",
    "            product_data['url'] = None\n",
    "        \n",
    "        # Prix\n",
    "        try:\n",
    "            if selectors.get('price'):\n",
    "                price_elem = container.find_element(By.CSS_SELECTOR, selectors['price'])\n",
    "                product_data['price'] = price_elem.text.strip()\n",
    "        except:\n",
    "            product_data['price'] = 'N/A'\n",
    "        \n",
    "        # Rating\n",
    "        try:\n",
    "            if selectors.get('rating'):\n",
    "                rating_elem = container.find_element(By.CSS_SELECTOR, selectors['rating'])\n",
    "                rating_text = rating_elem.get_attribute('textContent') or rating_elem.text\n",
    "                product_data['rating'] = rating_text.strip()\n",
    "        except:\n",
    "            product_data['rating'] = 'N/A'\n",
    "        \n",
    "        return product_data\n",
    "    \n",
    "    def _scrape_product_reviews(self, product, reviews_per_rating, max_pages=5):\n",
    "        \"\"\"Scrape les reviews d'un produit spÃ©cifique\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Naviguer vers le produit\n",
    "            self.driver.get(product['url'])\n",
    "            time.sleep(3)\n",
    "            \n",
    "            # Trouver la page des reviews\n",
    "            reviews_url = self._find_reviews_page(product['url'])\n",
    "            \n",
    "            if reviews_url:\n",
    "                self.driver.get(reviews_url)\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                print(\"âš ï¸ Page reviews non trouvÃ©e, tentative sur page produit\")\n",
    "            \n",
    "            # RÃ©cupÃ©rer les reviews\n",
    "            all_reviews = []\n",
    "            \n",
    "            # SÃ©lecteurs reviews\n",
    "            review_selectors = self.selectors.get('reviews', {})\n",
    "            \n",
    "            if not review_selectors:\n",
    "                print(\"âŒ SÃ©lecteurs reviews non disponibles\")\n",
    "                return []\n",
    "            \n",
    "            # Scraper les reviews page par page\n",
    "            for page in range(max_pages):\n",
    "                try:\n",
    "                    page_reviews = self._extract_reviews_from_page(product, review_selectors)\n",
    "                    \n",
    "                    if page_reviews:\n",
    "                        all_reviews.extend(page_reviews)\n",
    "                        print(f\"ğŸ“ Page {page+1}: {len(page_reviews)} reviews\")\n",
    "                        \n",
    "                        # Essayer de passer Ã  la page suivante\n",
    "                        if not self._go_to_next_page():\n",
    "                            print(\"ğŸ“„ Plus de pages disponibles\")\n",
    "                            break\n",
    "                        \n",
    "                        time.sleep(random.uniform(2, 4))\n",
    "                    else:\n",
    "                        print(f\"ğŸ“„ Page {page+1}: aucune review\")\n",
    "                        break\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"âŒ Erreur page {page+1}: {e}\")\n",
    "                    break\n",
    "            \n",
    "            # Limiter le nombre de reviews si nÃ©cessaire\n",
    "            if len(all_reviews) > reviews_per_rating * 5:  # 5 notes possibles\n",
    "                all_reviews = all_reviews[:reviews_per_rating * 5]\n",
    "            \n",
    "            return all_reviews\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Erreur scraping reviews produit: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def _find_reviews_page(self, product_url):\n",
    "        \"\"\"Trouve l'URL de la page des reviews\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # SÃ©lecteurs de liens vers reviews\n",
    "            review_link_selectors = [\n",
    "                'a[href*=\"customer-reviews\"]',\n",
    "                'a[href*=\"reviews\"]',\n",
    "                '[data-hook=\"see-all-reviews-link-foot\"]',\n",
    "                '.cr-widget-ACR a'\n",
    "            ]\n",
    "            \n",
    "            for selector in review_link_selectors:\n",
    "                try:\n",
    "                    links = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                    for link in links:\n",
    "                        href = link.get_attribute('href')\n",
    "                        if href and 'review' in href:\n",
    "                            return href\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            # Si aucun lien trouvÃ©, construire l'URL pour Amazon\n",
    "            if 'amazon.com' in product_url:\n",
    "                import re\n",
    "                asin_match = re.search(r'/dp/([A-Z0-9]{10})', product_url)\n",
    "                if asin_match:\n",
    "                    asin = asin_match.group(1)\n",
    "                    return f\"https://www.amazon.com/product-reviews/{asin}\"\n",
    "            \n",
    "            return None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Erreur recherche page reviews: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _extract_reviews_from_page(self, product, review_selectors):\n",
    "        \"\"\"Extrait les reviews de la page actuelle\"\"\"\n",
    "        \n",
    "        reviews = []\n",
    "        \n",
    "        try:\n",
    "            # RÃ©cupÃ©rer les conteneurs de reviews\n",
    "            container_selector = review_selectors.get('container')\n",
    "            if not container_selector:\n",
    "                return []\n",
    "            \n",
    "            review_containers = self.driver.find_elements(By.CSS_SELECTOR, container_selector)\n",
    "            \n",
    "            for container in review_containers:\n",
    "                try:\n",
    "                    review_data = {\n",
    "                        'product_name': product['title'],\n",
    "                        'product_category': product['category'],\n",
    "                        'product_url': product['url'],\n",
    "                        'product_price': product.get('price', 'N/A'),\n",
    "                        'scraped_at': datetime.now().isoformat()\n",
    "                    }\n",
    "                    \n",
    "                    # Texte de la review\n",
    "                    if review_selectors.get('text'):\n",
    "                        try:\n",
    "                            text_elem = container.find_element(By.CSS_SELECTOR, review_selectors['text'])\n",
    "                            review_data['review_text'] = text_elem.text.strip()\n",
    "                        except:\n",
    "                            review_data['review_text'] = ''\n",
    "                    \n",
    "                    # Titre de la review\n",
    "                    if review_selectors.get('title'):\n",
    "                        try:\n",
    "                            title_elem = container.find_element(By.CSS_SELECTOR, review_selectors['title'])\n",
    "                            review_data['review_title'] = title_elem.text.strip()\n",
    "                        except:\n",
    "                            review_data['review_title'] = ''\n",
    "                    \n",
    "                    # Note de la review\n",
    "                    if review_selectors.get('rating'):\n",
    "                        try:\n",
    "                            rating_elem = container.find_element(By.CSS_SELECTOR, review_selectors['rating'])\n",
    "                            rating_text = rating_elem.get_attribute('textContent') or rating_elem.text\n",
    "                            # Extraire le chiffre de la note\n",
    "                            import re\n",
    "                            rating_match = re.search(r'(\\d+(?:\\.\\d+)?)', rating_text)\n",
    "                            review_data['user_rating'] = float(rating_match.group(1)) if rating_match else None\n",
    "                        except:\n",
    "                            review_data['user_rating'] = None\n",
    "                    \n",
    "                    # Auteur\n",
    "                    if review_selectors.get('author'):\n",
    "                        try:\n",
    "                            author_elem = container.find_element(By.CSS_SELECTOR, review_selectors['author'])\n",
    "                            review_data['reviewer_name'] = author_elem.text.strip()\n",
    "                        except:\n",
    "                            review_data['reviewer_name'] = 'Anonymous'\n",
    "                    \n",
    "                    # Date\n",
    "                    if review_selectors.get('date'):\n",
    "                        try:\n",
    "                            date_elem = container.find_element(By.CSS_SELECTOR, review_selectors['date'])\n",
    "                            review_data['review_date'] = date_elem.text.strip()\n",
    "                        except:\n",
    "                            review_data['review_date'] = 'N/A'\n",
    "                    \n",
    "                    # Ajouter si on a du contenu utile\n",
    "                    if (review_data.get('review_text') and len(review_data['review_text']) > 10) or \\\n",
    "                       (review_data.get('review_title') and len(review_data['review_title']) > 5):\n",
    "                        reviews.append(review_data)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    continue  # Ignorer les reviews problÃ©matiques\n",
    "            \n",
    "            return reviews\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Erreur extraction reviews: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def _go_to_next_page(self):\n",
    "        \"\"\"Tente de passer Ã  la page suivante\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # SÃ©lecteurs de bouton \"suivant\"\n",
    "            next_selectors = [\n",
    "                '.a-pagination .a-last a',\n",
    "                'a[aria-label=\"Next page\"]',\n",
    "                '.a-pagination li:last-child a',\n",
    "                'a[href*=\"pageNumber\"]'\n",
    "            ]\n",
    "            \n",
    "            for selector in next_selectors:\n",
    "                try:\n",
    "                    next_button = self.driver.find_element(By.CSS_SELECTOR, selector)\n",
    "                    if next_button.is_enabled():\n",
    "                        next_button.click()\n",
    "                        time.sleep(2)\n",
    "                        return True\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            return False\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Erreur navigation page suivante: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def _clean_review_data(self, df):\n",
    "        \"\"\"Nettoie et optimise les donnÃ©es rÃ©cupÃ©rÃ©es\"\"\"\n",
    "        \n",
    "        try:\n",
    "            print(\"ğŸ§¹ Nettoyage des donnÃ©es...\")\n",
    "            \n",
    "            # Supprimer les doublons\n",
    "            initial_count = len(df)\n",
    "            df = df.drop_duplicates(subset=['review_text', 'product_name'], keep='first')\n",
    "            print(f\"ğŸ“ Doublons supprimÃ©s: {initial_count - len(df)}\")\n",
    "            \n",
    "            # Nettoyer les textes\n",
    "            df['review_text'] = df['review_text'].str.strip()\n",
    "            df['review_title'] = df['review_title'].str.strip()\n",
    "            df['product_name'] = df['product_name'].str.strip()\n",
    "            \n",
    "            # Filtrer les reviews trop courtes\n",
    "            df = df[df['review_text'].str.len() > 15]\n",
    "            \n",
    "            # Ajouter des mÃ©triques\n",
    "            df['review_length'] = df['review_text'].str.len()\n",
    "            df['word_count'] = df['review_text'].str.split().str.len()\n",
    "            \n",
    "            # Nettoyer les ratings\n",
    "            df['user_rating'] = pd.to_numeric(df['user_rating'], errors='coerce')\n",
    "            \n",
    "            print(f\"âœ… {len(df)} reviews nettoyÃ©es\")\n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Erreur nettoyage: {e}\")\n",
    "            return df\n",
    "    \n",
    "    def save_data(self, df, filename=None):\n",
    "        \"\"\"Sauvegarde les donnÃ©es avec horodatage\"\"\"\n",
    "        \n",
    "        try:\n",
    "            if filename is None:\n",
    "                timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                filename = f\"../data/raw/reviews_robustes_{timestamp}.csv\"\n",
    "            \n",
    "            import os\n",
    "            os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "            \n",
    "            df.to_csv(filename, index=False, encoding='utf-8')\n",
    "            print(f\"ğŸ’¾ DonnÃ©es sauvegardÃ©es: {filename}\")\n",
    "            \n",
    "            # Sauvegarder aussi en JSON pour backup\n",
    "            json_filename = filename.replace('.csv', '.json')\n",
    "            df.to_json(json_filename, orient='records', force_ascii=False, indent=2)\n",
    "            \n",
    "            return filename\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Erreur sauvegarde: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Ferme proprement le scraper\"\"\"\n",
    "        if self.driver:\n",
    "            try:\n",
    "                self.driver.quit()\n",
    "                print(\"âœ… Driver scraper fermÃ©\")\n",
    "            except:\n",
    "                pass\n",
    "            self.driver = None\n",
    "\n",
    "print(\"âœ… Scraper robuste crÃ©Ã©!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97beaf6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Workflow robuste prÃªt!\n",
      "ğŸ“– Utilisez robust_reviews_menu() pour commencer\n",
      "ğŸš€ Ou run_example('laptops_gaming') pour un test rapide\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# WORKFLOW PRINCIPAL ROBUSTE\n",
    "# ============================================================================\n",
    "\n",
    "def robust_reviews_workflow(category=\"laptop\", site=\"amazon\", max_products=10, reviews_per_rating=50, headless=False):\n",
    "    \"\"\"\n",
    "    Workflow principal ultra-robuste pour scraper les reviews de produits\n",
    "    \n",
    "    Phase 1: DÃ©tection automatique des balises (Scout)\n",
    "    Phase 2: Scraping des reviews avec balises validÃ©es (Scraper)\n",
    "    \n",
    "    Args:\n",
    "        category: catÃ©gorie de produits (ex: \"laptop\", \"smartphone\")\n",
    "        site: site Ã  scraper (\"amazon\" ou \"ebay\")\n",
    "        max_products: nombre max de produits Ã  analyser\n",
    "        reviews_per_rating: nombre de reviews Ã  rÃ©cupÃ©rer par note\n",
    "        headless: mode sans interface (True) ou visible (False)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*100)\n",
    "    print(\"ğŸš€ WORKFLOW ROBUSTE - SCRAPING REVIEWS DE PRODUITS\")\n",
    "    print(\"=\"*100)\n",
    "    print(f\"ğŸ“¦ CatÃ©gorie: {category}\")\n",
    "    print(f\"ğŸŒ Site: {site}\")\n",
    "    print(f\"ğŸ“Š Produits max: {max_products}\")\n",
    "    print(f\"â­ Reviews par note: {reviews_per_rating}\")\n",
    "    print(f\"ğŸ‘ï¸ Mode: {'Headless' if headless else 'Visible'}\")\n",
    "    print(f\"ğŸ“ˆ Total estimÃ©: {max_products * 5 * reviews_per_rating} reviews max\")\n",
    "    print()\n",
    "    \n",
    "    # Variables pour cleanup\n",
    "    scout = None\n",
    "    scraper = None\n",
    "    \n",
    "    try:\n",
    "        # ====================================================================\n",
    "        # PHASE 1: DÃ‰TECTION DES BALISES (SCOUT)\n",
    "        # ====================================================================\n",
    "        print(\"ğŸ” PHASE 1: DÃ‰TECTION AUTOMATIQUE DES BALISES\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        scout = RobustProductReviewScout()\n",
    "        \n",
    "        # Setup du driver scout\n",
    "        if not scout.setup_robust_driver(headless=True, timeout=30):\n",
    "            print(\"âŒ Impossible d'initialiser le scout\")\n",
    "            return None\n",
    "        \n",
    "        # DÃ©tection des sÃ©lecteurs\n",
    "        site_url = f\"https://www.{site}.com\"\n",
    "        detected_selectors = scout.detect_site_selectors(site_url, category)\n",
    "        \n",
    "        if not detected_selectors or not detected_selectors.get('products'):\n",
    "            print(\"âŒ Ã‰chec de la dÃ©tection des sÃ©lecteurs\")\n",
    "            scout.close()\n",
    "            return None\n",
    "        \n",
    "        print(\"âœ… SÃ©lecteurs dÃ©tectÃ©s avec succÃ¨s!\")\n",
    "        print(f\"ğŸ“¦ Produits: {list(detected_selectors['products'].keys())}\")\n",
    "        print(f\"ğŸ“ Reviews: {list(detected_selectors['reviews'].keys())}\")\n",
    "        \n",
    "        # Fermer le scout\n",
    "        scout.close()\n",
    "        scout = None\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        \n",
    "        # ====================================================================\n",
    "        # PHASE 2: SCRAPING DES REVIEWS (SCRAPER)\n",
    "        # ====================================================================\n",
    "        print(\"ğŸ“Š PHASE 2: SCRAPING DES REVIEWS\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        scraper = RobustProductReviewScraper()\n",
    "        scraper.selectors = detected_selectors  # Utiliser les sÃ©lecteurs dÃ©tectÃ©s\n",
    "        \n",
    "        # Setup du driver scraper\n",
    "        if not scraper.setup_robust_driver(headless=headless, timeout=60):\n",
    "            print(\"âŒ Impossible d'initialiser le scraper\")\n",
    "            return None\n",
    "        \n",
    "        # Avertissement utilisateur\n",
    "        if not headless:\n",
    "            print(\"\\nğŸš¨ AVERTISSEMENT: Scraping sur site rÃ©el en cours!\")\n",
    "            print(\"â° DurÃ©e estimÃ©e: {:.1f} minutes\".format(max_products * 3))\n",
    "            print(\"ğŸ“ Respectez les ToS et les limitations de dÃ©bit\")\n",
    "            \n",
    "            response = input(\"\\nğŸ”„ Continuer? (o/n): \").strip().lower()\n",
    "            if response not in ['o', 'oui', 'y', 'yes', '']:\n",
    "                print(\"â¹ï¸ ArrÃªt demandÃ© par l'utilisateur\")\n",
    "                scraper.close()\n",
    "                return None\n",
    "        \n",
    "        # Lancement du scraping\n",
    "        print(f\"\\nğŸ¯ DÃ©but du scraping pour '{category}' sur {site}...\")\n",
    "        \n",
    "        df_reviews = scraper.scrape_category_reviews(\n",
    "            category=category,\n",
    "            site=site,\n",
    "            max_products=max_products,\n",
    "            reviews_per_rating=reviews_per_rating\n",
    "        )\n",
    "        \n",
    "        # ====================================================================\n",
    "        # PHASE 3: RÃ‰SULTATS ET SAUVEGARDE\n",
    "        # ====================================================================\n",
    "        if df_reviews.empty:\n",
    "            print(\"âŒ Aucune review rÃ©cupÃ©rÃ©e\")\n",
    "            scraper.close()\n",
    "            return None\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"ğŸ“Š ANALYSE DES RÃ‰SULTATS\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        # Statistiques dÃ©taillÃ©es\n",
    "        stats = {\n",
    "            'total_reviews': len(df_reviews),\n",
    "            'unique_products': df_reviews['product_name'].nunique(),\n",
    "            'avg_review_length': df_reviews['review_length'].mean(),\n",
    "            'rating_distribution': df_reviews['user_rating'].value_counts().sort_index(),\n",
    "            'top_products': df_reviews['product_name'].value_counts().head(5)\n",
    "        }\n",
    "        \n",
    "        print(f\"âœ… Total reviews: {stats['total_reviews']}\")\n",
    "        print(f\"ğŸ“¦ Produits uniques: {stats['unique_products']}\")\n",
    "        print(f\"ğŸ“ Longueur moyenne: {stats['avg_review_length']:.0f} caractÃ¨res\")\n",
    "        print(f\"â­ Distribution des notes:\")\n",
    "        for rating, count in stats['rating_distribution'].items():\n",
    "            if pd.notna(rating):\n",
    "                print(f\"   {rating} Ã©toiles: {count} reviews\")\n",
    "        \n",
    "        print(f\"\\nğŸ† Top produits par nombre de reviews:\")\n",
    "        for product, count in stats['top_products'].items():\n",
    "            print(f\"   â€¢ {product[:50]}... ({count} reviews)\")\n",
    "        \n",
    "        # Sauvegarde\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"../data/raw/{site}_{category}_{timestamp}.csv\"\n",
    "        \n",
    "        saved_file = scraper.save_data(df_reviews, filename)\n",
    "        \n",
    "        # AperÃ§u des donnÃ©es\n",
    "        print(f\"\\nğŸ“‹ APERÃ‡U DES DONNÃ‰ES (5 premiÃ¨res reviews):\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        sample_cols = ['product_name', 'user_rating', 'review_text', 'reviewer_name']\n",
    "        available_cols = [col for col in sample_cols if col in df_reviews.columns]\n",
    "        \n",
    "        for i, row in df_reviews[available_cols].head(5).iterrows():\n",
    "            print(f\"\\nReview {i+1}:\")\n",
    "            for col in available_cols:\n",
    "                value = str(row[col])\n",
    "                if col == 'review_text' and len(value) > 100:\n",
    "                    value = value[:100] + \"...\"\n",
    "                elif col == 'product_name' and len(value) > 50:\n",
    "                    value = value[:50] + \"...\"\n",
    "                print(f\"  {col}: {value}\")\n",
    "        \n",
    "        # Fermer le scraper\n",
    "        scraper.close()\n",
    "        scraper = None\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*100)\n",
    "        print(\"ğŸ‰ WORKFLOW TERMINÃ‰ AVEC SUCCÃˆS!\")\n",
    "        print(\"=\"*100)\n",
    "        \n",
    "        return df_reviews\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nâ¹ï¸ ArrÃªt demandÃ© par l'utilisateur\")\n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ Erreur workflow: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "        \n",
    "    finally:\n",
    "        # Cleanup des ressources\n",
    "        if scout:\n",
    "            try:\n",
    "                scout.close()\n",
    "            except:\n",
    "                pass\n",
    "        if scraper:\n",
    "            try:\n",
    "                scraper.close()\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "def quick_scout_test(site=\"amazon\", category=\"laptop\"):\n",
    "    \"\"\"\n",
    "    Test rapide du scout uniquement\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ§ª TEST RAPIDE - Scout pour {category} sur {site}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    scout = RobustProductReviewScout()\n",
    "    \n",
    "    try:\n",
    "        if scout.setup_robust_driver(headless=True):\n",
    "            site_url = f\"https://www.{site}.com\"\n",
    "            selectors = scout.detect_site_selectors(site_url, category)\n",
    "            \n",
    "            if selectors:\n",
    "                print(\"âœ… Test scout rÃ©ussi!\")\n",
    "                print(f\"ğŸ“¦ SÃ©lecteurs produits: {list(selectors['products'].keys())}\")\n",
    "                print(f\"ğŸ“ SÃ©lecteurs reviews: {list(selectors['reviews'].keys())}\")\n",
    "                return selectors\n",
    "            else:\n",
    "                print(\"âŒ Test scout Ã©chouÃ©\")\n",
    "                return None\n",
    "        else:\n",
    "            print(\"âŒ Impossible de crÃ©er le driver scout\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Erreur test scout: {e}\")\n",
    "        return None\n",
    "        \n",
    "    finally:\n",
    "        scout.close()\n",
    "\n",
    "def robust_reviews_menu():\n",
    "    \"\"\"\n",
    "    Menu principal pour le workflow robuste\n",
    "    \"\"\"\n",
    "    print(\"=\"*100)\n",
    "    print(\"ğŸ¯ WORKFLOW ROBUSTE - REVIEWS DE PRODUITS\")\n",
    "    print(\"=\"*100)\n",
    "    print()\n",
    "    print(\"1ï¸âƒ£ Workflow complet (scout + scraper)\")\n",
    "    print(\"2ï¸âƒ£ Test scout uniquement\")\n",
    "    print(\"3ï¸âƒ£ Configuration avancÃ©e\")\n",
    "    print(\"4ï¸âƒ£ Voir fichiers de donnÃ©es\")\n",
    "    print(\"5ï¸âƒ£ Quitter\")\n",
    "    print()\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            choice = input(\"ğŸ‘‰ Votre choix (1-5): \").strip()\n",
    "            \n",
    "            if choice == '1':\n",
    "                print(\"\\nğŸ“‹ CONFIGURATION DU WORKFLOW COMPLET\")\n",
    "                print(\"-\" * 50)\n",
    "                \n",
    "                category = input(\"ğŸ·ï¸ CatÃ©gorie (ex: laptop, smartphone): \").strip() or \"laptop\"\n",
    "                site = input(\"ğŸŒ Site (amazon/ebay): \").strip() or \"amazon\"\n",
    "                \n",
    "                try:\n",
    "                    max_products = int(input(\"ğŸ“¦ Nombre de produits (dÃ©faut: 5): \") or \"5\")\n",
    "                    reviews_per_rating = int(input(\"â­ Reviews par note (dÃ©faut: 20): \") or \"20\")\n",
    "                except ValueError:\n",
    "                    max_products, reviews_per_rating = 5, 20\n",
    "                \n",
    "                headless_choice = input(\"ğŸ‘ï¸ Mode headless? (o/n, dÃ©faut: n): \").strip().lower()\n",
    "                headless = headless_choice in ['o', 'oui', 'y', 'yes']\n",
    "                \n",
    "                print(f\"\\nğŸš€ Lancement du workflow...\")\n",
    "                result = robust_reviews_workflow(category, site, max_products, reviews_per_rating, headless)\n",
    "                \n",
    "                if result is not None:\n",
    "                    print(f\"\\nâœ… Workflow terminÃ© - {len(result)} reviews rÃ©cupÃ©rÃ©es\")\n",
    "                else:\n",
    "                    print(\"\\nâŒ Workflow Ã©chouÃ©\")\n",
    "                \n",
    "                return result\n",
    "                \n",
    "            elif choice == '2':\n",
    "                print(\"\\nğŸ§ª TEST SCOUT\")\n",
    "                print(\"-\" * 30)\n",
    "                \n",
    "                site = input(\"ğŸŒ Site (amazon/ebay): \").strip() or \"amazon\"\n",
    "                category = input(\"ğŸ·ï¸ CatÃ©gorie: \").strip() or \"laptop\"\n",
    "                \n",
    "                result = quick_scout_test(site, category)\n",
    "                if result:\n",
    "                    print(\"âœ… Scout fonctionne correctement!\")\n",
    "                else:\n",
    "                    print(\"âŒ ProblÃ¨me avec le scout\")\n",
    "                \n",
    "            elif choice == '3':\n",
    "                print(\"\\nâš™ï¸ CONFIGURATION AVANCÃ‰E\")\n",
    "                print(\"-\" * 40)\n",
    "                print(\"ğŸ“– ParamÃ¨tres disponibles dans robust_reviews_workflow():\")\n",
    "                print(\"   â€¢ category: catÃ©gorie de produits\")\n",
    "                print(\"   â€¢ site: amazon ou ebay\")\n",
    "                print(\"   â€¢ max_products: nombre de produits max\")\n",
    "                print(\"   â€¢ reviews_per_rating: reviews par note (1-5)\")\n",
    "                print(\"   â€¢ headless: mode sans interface\")\n",
    "                print(\"\\nğŸ’¡ Exemple:\")\n",
    "                print(\"   df = robust_reviews_workflow('gaming laptop', 'amazon', 8, 30, False)\")\n",
    "                \n",
    "            elif choice == '4':\n",
    "                print(\"\\nğŸ“ FICHIERS DE DONNÃ‰ES\")\n",
    "                print(\"-\" * 30)\n",
    "                \n",
    "                import os\n",
    "                data_dir = \"../data/raw\"\n",
    "                \n",
    "                if os.path.exists(data_dir):\n",
    "                    files = [f for f in os.listdir(data_dir) if f.endswith('.csv')]\n",
    "                    if files:\n",
    "                        print(\"ğŸ“„ Fichiers CSV trouvÃ©s:\")\n",
    "                        for f in sorted(files, reverse=True)[:10]:  # 10 plus rÃ©cents\n",
    "                            size = os.path.getsize(os.path.join(data_dir, f)) / 1024  # KB\n",
    "                            print(f\"   â€¢ {f} ({size:.1f} KB)\")\n",
    "                    else:\n",
    "                        print(\"âŒ Aucun fichier CSV trouvÃ©\")\n",
    "                else:\n",
    "                    print(\"âŒ Dossier data/raw non trouvÃ©\")\n",
    "                \n",
    "            elif choice == '5':\n",
    "                print(\"ğŸ‘‹ Ã€ bientÃ´t!\")\n",
    "                break\n",
    "                \n",
    "            else:\n",
    "                print(\"âŒ Choix invalide\")\n",
    "                \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nğŸ‘‹ Ã€ bientÃ´t!\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Erreur: {e}\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Configuration d'exemples prÃªts Ã  l'emploi\n",
    "ROBUST_EXAMPLES = {\n",
    "    'laptops_gaming': {\n",
    "        'category': 'gaming laptop',\n",
    "        'site': 'amazon',\n",
    "        'max_products': 8,\n",
    "        'reviews_per_rating': 25,\n",
    "        'headless': False\n",
    "    },\n",
    "    'smartphones': {\n",
    "        'category': 'smartphone',\n",
    "        'site': 'amazon', \n",
    "        'max_products': 6,\n",
    "        'reviews_per_rating': 30,\n",
    "        'headless': False\n",
    "    },\n",
    "    'headphones': {\n",
    "        'category': 'wireless headphones',\n",
    "        'site': 'amazon',\n",
    "        'max_products': 10,\n",
    "        'reviews_per_rating': 20,\n",
    "        'headless': False\n",
    "    }\n",
    "}\n",
    "\n",
    "def run_example(example_name):\n",
    "    \"\"\"ExÃ©cute un exemple prÃ©dÃ©fini\"\"\"\n",
    "    if example_name in ROBUST_EXAMPLES:\n",
    "        config = ROBUST_EXAMPLES[example_name]\n",
    "        print(f\"ğŸš€ Lancement exemple: {example_name}\")\n",
    "        return robust_reviews_workflow(**config)\n",
    "    else:\n",
    "        print(f\"âŒ Exemple '{example_name}' non trouvÃ©\")\n",
    "        print(f\"ğŸ“‹ Disponibles: {list(ROBUST_EXAMPLES.keys())}\")\n",
    "        return None\n",
    "\n",
    "print(\"âœ… Workflow robuste prÃªt!\")\n",
    "print(\"ğŸ“– Utilisez robust_reviews_menu() pour commencer\")\n",
    "print(\"ğŸš€ Ou run_example('laptops_gaming') pour un test rapide\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8295519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Lancement exemple: laptops_gaming\n",
      "====================================================================================================\n",
      "ğŸš€ WORKFLOW ROBUSTE - SCRAPING REVIEWS DE PRODUITS\n",
      "====================================================================================================\n",
      "ğŸ“¦ CatÃ©gorie: gaming laptop\n",
      "ğŸŒ Site: amazon\n",
      "ğŸ“Š Produits max: 8\n",
      "â­ Reviews par note: 25\n",
      "ğŸ‘ï¸ Mode: Visible\n",
      "ğŸ“ˆ Total estimÃ©: 1000 reviews max\n",
      "\n",
      "ğŸ” PHASE 1: DÃ‰TECTION AUTOMATIQUE DES BALISES\n",
      "----------------------------------------------------------------------\n",
      "ğŸ”§ Tentative 1/3 - Setup driver scout...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-29 15:02:31,911 - INFO - patching driver executable C:\\Users\\Yann\\appdata\\roaming\\undetected_chromedriver\\undetected_chromedriver.exe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Driver scout initialisÃ© avec succÃ¨s!\n",
      "ğŸ” DÃ©tection des sÃ©lecteurs pour amazon...\n",
      "ğŸŒ Navigation vers: https://www.amazon.com/s?k=gaming+laptop\n",
      "âœ… Conteneur: [data-component-type=\"s-search-result\"] (16 Ã©lÃ©ments)\n",
      "âœ… Titre: h2 span\n",
      "âœ… Conteneur: [data-component-type=\"s-search-result\"] (16 Ã©lÃ©ments)\n",
      "âœ… Titre: h2 span\n",
      "âœ… URL: .a-link-normal\n",
      "âœ… URL: .a-link-normal\n",
      "âœ… Rating: .a-icon-alt\n",
      "âœ… SÃ©lecteurs produits dÃ©tectÃ©s: 4\n",
      "ğŸ”— Test reviews sur: https://www.amazon.com/ACEMAGIC-16-1inch-Computer-Windows-Graphics/dp/B0FB99VF5F...\n",
      "âœ… Rating: .a-icon-alt\n",
      "âœ… SÃ©lecteurs produits dÃ©tectÃ©s: 4\n",
      "ğŸ”— Test reviews sur: https://www.amazon.com/ACEMAGIC-16-1inch-Computer-Windows-Graphics/dp/B0FB99VF5F...\n",
      "ğŸ”— Navigation vers page reviews: https://www.amazon.com/ACEMAGIC-16-1inch-Computer-Windows-Graphics/dp/B0FB99VF5F...\n",
      "ğŸ”— Navigation vers page reviews: https://www.amazon.com/ACEMAGIC-16-1inch-Computer-Windows-Graphics/dp/B0FB99VF5F...\n",
      "âœ… Conteneur reviews: [data-hook=\"review\"] (12 reviews)\n",
      "âœ… Texte review: .review-text\n",
      "âœ… Titre review: .review-title\n",
      "âœ… Rating review: [data-hook=\"review-star-rating\"] .a-icon-alt\n",
      "âœ… Auteur review: .a-profile-name\n",
      "âœ… Date review: [data-hook=\"review-date\"]\n",
      "âœ… SÃ©lecteurs reviews dÃ©tectÃ©s: 6\n",
      "âœ… SÃ©lecteurs sauvegardÃ©s: ../config/detected_selectors_amazon.json\n",
      "âœ… SÃ©lecteurs dÃ©tectÃ©s avec succÃ¨s!\n",
      "ğŸ“¦ Produits: ['container', 'title', 'url', 'rating']\n",
      "ğŸ“ Reviews: ['container', 'text', 'title', 'rating', 'author', 'date']\n",
      "âœ… Conteneur reviews: [data-hook=\"review\"] (12 reviews)\n",
      "âœ… Texte review: .review-text\n",
      "âœ… Titre review: .review-title\n",
      "âœ… Rating review: [data-hook=\"review-star-rating\"] .a-icon-alt\n",
      "âœ… Auteur review: .a-profile-name\n",
      "âœ… Date review: [data-hook=\"review-date\"]\n",
      "âœ… SÃ©lecteurs reviews dÃ©tectÃ©s: 6\n",
      "âœ… SÃ©lecteurs sauvegardÃ©s: ../config/detected_selectors_amazon.json\n",
      "âœ… SÃ©lecteurs dÃ©tectÃ©s avec succÃ¨s!\n",
      "ğŸ“¦ Produits: ['container', 'title', 'url', 'rating']\n",
      "ğŸ“ Reviews: ['container', 'text', 'title', 'rating', 'author', 'date']\n",
      "âœ… Driver scout fermÃ©\n",
      "\n",
      "======================================================================\n",
      "ğŸ“Š PHASE 2: SCRAPING DES REVIEWS\n",
      "----------------------------------------------------------------------\n",
      "ğŸš€ Tentative 1/3 - Setup driver scraper...\n",
      "âœ… Driver scout fermÃ©\n",
      "\n",
      "======================================================================\n",
      "ğŸ“Š PHASE 2: SCRAPING DES REVIEWS\n",
      "----------------------------------------------------------------------\n",
      "ğŸš€ Tentative 1/3 - Setup driver scraper...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-29 15:03:15,744 - INFO - patching driver executable C:\\Users\\Yann\\appdata\\roaming\\undetected_chromedriver\\undetected_chromedriver.exe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Driver scraper prÃªt!\n",
      "ğŸ­ User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko)...\n",
      "\n",
      "ğŸš¨ AVERTISSEMENT: Scraping sur site rÃ©el en cours!\n",
      "â° DurÃ©e estimÃ©e: 24.0 minutes\n",
      "ğŸ“ Respectez les ToS et les limitations de dÃ©bit\n",
      "\n",
      "ğŸ¯ DÃ©but du scraping pour 'gaming laptop' sur amazon...\n",
      "================================================================================\n",
      "ğŸ¯ DÃ‰BUT DU SCRAPING ROBUSTE\n",
      "================================================================================\n",
      "ğŸ“¦ CatÃ©gorie: gaming laptop\n",
      "ğŸŒ Site: amazon\n",
      "ğŸ“Š Produits max: 8\n",
      "â­ Reviews par note: 25\n",
      "\n",
      "ğŸ” Recherche: https://www.amazon.com/s?k=gaming+laptop\n",
      "âŒ Erreur rÃ©cupÃ©ration produits: Message: invalid session id: session deleted as the browser has closed the connection\n",
      "from disconnected: not connected to DevTools\n",
      "  (Session info: chrome=138.0.7204.50)\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x0xd04493+62419]\n",
      "\tGetHandleVerifier [0x0xd044d4+62484]\n",
      "\t(No symbol) [0x0xb42133]\n",
      "\t(No symbol) [0x0xb31b40]\n",
      "\t(No symbol) [0x0xb4f912]\n",
      "\t(No symbol) [0x0xbb5d6c]\n",
      "\t(No symbol) [0x0xbd0159]\n",
      "\t(No symbol) [0x0xbaf266]\n",
      "\t(No symbol) [0x0xb7e852]\n",
      "\t(No symbol) [0x0xb7f6f4]\n",
      "\tGetHandleVerifier [0x0xf74773+2619059]\n",
      "\tGetHandleVerifier [0x0xf6fb8a+2599626]\n",
      "\tGetHandleVerifier [0x0xd2b03a+221050]\n",
      "\tGetHandleVerifier [0x0xd1b2b8+156152]\n",
      "\tGetHandleVerifier [0x0xd21c6d+183213]\n",
      "\tGetHandleVerifier [0x0xd0c378+94904]\n",
      "\tGetHandleVerifier [0x0xd0c502+95298]\n",
      "\tGetHandleVerifier [0x0xcf765a+9626]\n",
      "\tBaseThreadInitThunk [0x0x75055d49+25]\n",
      "\tRtlInitializeExceptionChain [0x0x7756d09b+107]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x0x7756d021+561]\n",
      "\n",
      "âŒ Aucun produit trouvÃ©\n",
      "âŒ Aucune review rÃ©cupÃ©rÃ©e\n",
      "âœ… Driver scraper fermÃ©\n",
      "\n",
      "ğŸ¯ DÃ©but du scraping pour 'gaming laptop' sur amazon...\n",
      "================================================================================\n",
      "ğŸ¯ DÃ‰BUT DU SCRAPING ROBUSTE\n",
      "================================================================================\n",
      "ğŸ“¦ CatÃ©gorie: gaming laptop\n",
      "ğŸŒ Site: amazon\n",
      "ğŸ“Š Produits max: 8\n",
      "â­ Reviews par note: 25\n",
      "\n",
      "ğŸ” Recherche: https://www.amazon.com/s?k=gaming+laptop\n",
      "âŒ Erreur rÃ©cupÃ©ration produits: Message: invalid session id: session deleted as the browser has closed the connection\n",
      "from disconnected: not connected to DevTools\n",
      "  (Session info: chrome=138.0.7204.50)\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x0xd04493+62419]\n",
      "\tGetHandleVerifier [0x0xd044d4+62484]\n",
      "\t(No symbol) [0x0xb42133]\n",
      "\t(No symbol) [0x0xb31b40]\n",
      "\t(No symbol) [0x0xb4f912]\n",
      "\t(No symbol) [0x0xbb5d6c]\n",
      "\t(No symbol) [0x0xbd0159]\n",
      "\t(No symbol) [0x0xbaf266]\n",
      "\t(No symbol) [0x0xb7e852]\n",
      "\t(No symbol) [0x0xb7f6f4]\n",
      "\tGetHandleVerifier [0x0xf74773+2619059]\n",
      "\tGetHandleVerifier [0x0xf6fb8a+2599626]\n",
      "\tGetHandleVerifier [0x0xd2b03a+221050]\n",
      "\tGetHandleVerifier [0x0xd1b2b8+156152]\n",
      "\tGetHandleVerifier [0x0xd21c6d+183213]\n",
      "\tGetHandleVerifier [0x0xd0c378+94904]\n",
      "\tGetHandleVerifier [0x0xd0c502+95298]\n",
      "\tGetHandleVerifier [0x0xcf765a+9626]\n",
      "\tBaseThreadInitThunk [0x0x75055d49+25]\n",
      "\tRtlInitializeExceptionChain [0x0x7756d09b+107]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x0x7756d021+561]\n",
      "\n",
      "âŒ Aucun produit trouvÃ©\n",
      "âŒ Aucune review rÃ©cupÃ©rÃ©e\n",
      "âœ… Driver scraper fermÃ©\n"
     ]
    }
   ],
   "source": [
    "run_example('laptops_gaming')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8436523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "ğŸ¯ WORKFLOW ROBUSTE - REVIEWS DE PRODUITS\n",
      "====================================================================================================\n",
      "\n",
      "1ï¸âƒ£ Workflow complet (scout + scraper)\n",
      "2ï¸âƒ£ Test scout uniquement\n",
      "3ï¸âƒ£ Configuration avancÃ©e\n",
      "4ï¸âƒ£ Voir fichiers de donnÃ©es\n",
      "5ï¸âƒ£ Quitter\n",
      "\n",
      "\n",
      "ğŸ“‹ CONFIGURATION DU WORKFLOW COMPLET\n",
      "--------------------------------------------------\n",
      "\n",
      "ğŸ“‹ CONFIGURATION DU WORKFLOW COMPLET\n",
      "--------------------------------------------------\n",
      "\n",
      "ğŸš€ Lancement du workflow...\n",
      "====================================================================================================\n",
      "ğŸš€ WORKFLOW ROBUSTE - SCRAPING REVIEWS DE PRODUITS\n",
      "====================================================================================================\n",
      "ğŸ“¦ CatÃ©gorie: laptop\n",
      "ğŸŒ Site: amazon\n",
      "ğŸ“Š Produits max: 2\n",
      "â­ Reviews par note: 10\n",
      "ğŸ‘ï¸ Mode: Headless\n",
      "ğŸ“ˆ Total estimÃ©: 100 reviews max\n",
      "\n",
      "ğŸ” PHASE 1: DÃ‰TECTION AUTOMATIQUE DES BALISES\n",
      "----------------------------------------------------------------------\n",
      "ğŸ”§ Tentative 1/3 - Setup driver scout...\n",
      "\n",
      "ğŸš€ Lancement du workflow...\n",
      "====================================================================================================\n",
      "ğŸš€ WORKFLOW ROBUSTE - SCRAPING REVIEWS DE PRODUITS\n",
      "====================================================================================================\n",
      "ğŸ“¦ CatÃ©gorie: laptop\n",
      "ğŸŒ Site: amazon\n",
      "ğŸ“Š Produits max: 2\n",
      "â­ Reviews par note: 10\n",
      "ğŸ‘ï¸ Mode: Headless\n",
      "ğŸ“ˆ Total estimÃ©: 100 reviews max\n",
      "\n",
      "ğŸ” PHASE 1: DÃ‰TECTION AUTOMATIQUE DES BALISES\n",
      "----------------------------------------------------------------------\n",
      "ğŸ”§ Tentative 1/3 - Setup driver scout...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-29 15:03:55,356 - INFO - patching driver executable C:\\Users\\Yann\\appdata\\roaming\\undetected_chromedriver\\undetected_chromedriver.exe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Driver scout initialisÃ© avec succÃ¨s!\n",
      "ğŸ” DÃ©tection des sÃ©lecteurs pour amazon...\n",
      "ğŸŒ Navigation vers: https://www.amazon.com/s?k=laptop\n",
      "âœ… Conteneur: [data-component-type=\"s-search-result\"] (16 Ã©lÃ©ments)\n",
      "âœ… Titre: h2 span\n",
      "âœ… Conteneur: [data-component-type=\"s-search-result\"] (16 Ã©lÃ©ments)\n",
      "âœ… Titre: h2 span\n",
      "âœ… URL: .a-link-normal\n",
      "âœ… URL: .a-link-normal\n",
      "âœ… Rating: .a-icon-alt\n",
      "âœ… SÃ©lecteurs produits dÃ©tectÃ©s: 4\n",
      "ğŸ”— Test reviews sur: https://www.amazon.com/HP-Micro-edge-Microsoft-14-dq0040nr-Snowflake/dp/B0947BJ6...\n",
      "âœ… Rating: .a-icon-alt\n",
      "âœ… SÃ©lecteurs produits dÃ©tectÃ©s: 4\n",
      "ğŸ”— Test reviews sur: https://www.amazon.com/HP-Micro-edge-Microsoft-14-dq0040nr-Snowflake/dp/B0947BJ6...\n",
      "ğŸ”— Navigation vers page reviews: https://www.amazon.com/HP-Micro-edge-Microsoft-14-dq0040nr-Snowflake/dp/B0947BJ6...\n",
      "ğŸ”— Navigation vers page reviews: https://www.amazon.com/HP-Micro-edge-Microsoft-14-dq0040nr-Snowflake/dp/B0947BJ6...\n",
      "âœ… Conteneur reviews: [data-hook=\"review\"] (13 reviews)\n",
      "âœ… Texte review: [data-hook=\"review-body\"] span\n",
      "âœ… Titre review: .review-title\n",
      "âœ… Rating review: [data-hook=\"review-star-rating\"] .a-icon-alt\n",
      "âœ… Auteur review: .a-profile-name\n",
      "âœ… Date review: [data-hook=\"review-date\"]\n",
      "âœ… SÃ©lecteurs reviews dÃ©tectÃ©s: 6\n",
      "âœ… SÃ©lecteurs sauvegardÃ©s: ../config/detected_selectors_amazon.json\n",
      "âœ… SÃ©lecteurs dÃ©tectÃ©s avec succÃ¨s!\n",
      "ğŸ“¦ Produits: ['container', 'title', 'url', 'rating']\n",
      "ğŸ“ Reviews: ['container', 'text', 'title', 'rating', 'author', 'date']\n",
      "âœ… Conteneur reviews: [data-hook=\"review\"] (13 reviews)\n",
      "âœ… Texte review: [data-hook=\"review-body\"] span\n",
      "âœ… Titre review: .review-title\n",
      "âœ… Rating review: [data-hook=\"review-star-rating\"] .a-icon-alt\n",
      "âœ… Auteur review: .a-profile-name\n",
      "âœ… Date review: [data-hook=\"review-date\"]\n",
      "âœ… SÃ©lecteurs reviews dÃ©tectÃ©s: 6\n",
      "âœ… SÃ©lecteurs sauvegardÃ©s: ../config/detected_selectors_amazon.json\n",
      "âœ… SÃ©lecteurs dÃ©tectÃ©s avec succÃ¨s!\n",
      "ğŸ“¦ Produits: ['container', 'title', 'url', 'rating']\n",
      "ğŸ“ Reviews: ['container', 'text', 'title', 'rating', 'author', 'date']\n",
      "âœ… Driver scout fermÃ©\n",
      "\n",
      "======================================================================\n",
      "ğŸ“Š PHASE 2: SCRAPING DES REVIEWS\n",
      "----------------------------------------------------------------------\n",
      "ğŸš€ Tentative 1/3 - Setup driver scraper...\n",
      "âœ… Driver scout fermÃ©\n",
      "\n",
      "======================================================================\n",
      "ğŸ“Š PHASE 2: SCRAPING DES REVIEWS\n",
      "----------------------------------------------------------------------\n",
      "ğŸš€ Tentative 1/3 - Setup driver scraper...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-29 15:04:41,538 - INFO - patching driver executable C:\\Users\\Yann\\appdata\\roaming\\undetected_chromedriver\\undetected_chromedriver.exe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Driver scraper prÃªt!\n",
      "ğŸ­ User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0...\n",
      "\n",
      "ğŸ¯ DÃ©but du scraping pour 'laptop' sur amazon...\n",
      "================================================================================\n",
      "ğŸ¯ DÃ‰BUT DU SCRAPING ROBUSTE\n",
      "================================================================================\n",
      "ğŸ“¦ CatÃ©gorie: laptop\n",
      "ğŸŒ Site: amazon\n",
      "ğŸ“Š Produits max: 2\n",
      "â­ Reviews par note: 10\n",
      "\n",
      "ğŸ” Recherche: https://www.amazon.com/s?k=laptop\n",
      "ğŸ“¦ 16 conteneurs trouvÃ©s\n",
      "âœ… Produit 1: HP 14 Laptop, Intel Celeron N4020, 4 GB ...\n",
      "âœ… Produit 2: SGIN Laptop 15.6 Inch Laptops Computer, ...\n",
      "âœ… 2 produits trouvÃ©s\n",
      "\n",
      "ğŸ“¦ PRODUIT 1/2\n",
      "ğŸ·ï¸ HP 14 Laptop, Intel Celeron N4020, 4 GB RAM, 64 GB Storage, ...\n",
      "ğŸ“¦ 16 conteneurs trouvÃ©s\n",
      "âœ… Produit 1: HP 14 Laptop, Intel Celeron N4020, 4 GB ...\n",
      "âœ… Produit 2: SGIN Laptop 15.6 Inch Laptops Computer, ...\n",
      "âœ… 2 produits trouvÃ©s\n",
      "\n",
      "ğŸ“¦ PRODUIT 1/2\n",
      "ğŸ·ï¸ HP 14 Laptop, Intel Celeron N4020, 4 GB RAM, 64 GB Storage, ...\n",
      "ğŸ“ Page 1: 12 reviews\n",
      "ğŸ“ Page 1: 12 reviews\n",
      "ğŸ“„ Plus de pages disponibles\n",
      "âœ… 12 reviews rÃ©cupÃ©rÃ©es\n",
      "â³ DÃ©lai: 6.0s...\n",
      "ğŸ“„ Plus de pages disponibles\n",
      "âœ… 12 reviews rÃ©cupÃ©rÃ©es\n",
      "â³ DÃ©lai: 6.0s...\n",
      "\n",
      "ğŸ“¦ PRODUIT 2/2\n",
      "ğŸ·ï¸ SGIN Laptop 15.6 Inch Laptops Computer, Celeron N4000 Proces...\n",
      "\n",
      "ğŸ“¦ PRODUIT 2/2\n",
      "ğŸ·ï¸ SGIN Laptop 15.6 Inch Laptops Computer, Celeron N4000 Proces...\n",
      "ğŸ“ Page 1: 5 reviews\n",
      "ğŸ“ Page 1: 5 reviews\n",
      "ğŸ“„ Plus de pages disponibles\n",
      "âœ… 5 reviews rÃ©cupÃ©rÃ©es\n",
      "â³ DÃ©lai: 5.2s...\n",
      "ğŸ“„ Plus de pages disponibles\n",
      "âœ… 5 reviews rÃ©cupÃ©rÃ©es\n",
      "â³ DÃ©lai: 5.2s...\n",
      "ğŸ§¹ Nettoyage des donnÃ©es...\n",
      "ğŸ“ Doublons supprimÃ©s: 1\n",
      "âœ… 15 reviews nettoyÃ©es\n",
      "\n",
      "================================================================================\n",
      "ğŸ“Š SCRAPING TERMINÃ‰ - STATISTIQUES\n",
      "================================================================================\n",
      "â±ï¸ DurÃ©e: 4.0 minutes\n",
      "ğŸ“¦ Produits traitÃ©s: 2\n",
      "ğŸ“ Reviews rÃ©cupÃ©rÃ©es: 17\n",
      "âŒ Erreurs: 0\n",
      "ğŸ“ˆ Taux de succÃ¨s: 100.0%\n",
      "\n",
      "======================================================================\n",
      "ğŸ“Š ANALYSE DES RÃ‰SULTATS\n",
      "----------------------------------------------------------------------\n",
      "âœ… Total reviews: 15\n",
      "ğŸ“¦ Produits uniques: 2\n",
      "ğŸ“ Longueur moyenne: 492 caractÃ¨res\n",
      "â­ Distribution des notes:\n",
      "   1.0 Ã©toiles: 1 reviews\n",
      "   3.0 Ã©toiles: 1 reviews\n",
      "   4.0 Ã©toiles: 2 reviews\n",
      "   5.0 Ã©toiles: 9 reviews\n",
      "\n",
      "ğŸ† Top produits par nombre de reviews:\n",
      "   â€¢ HP 14 Laptop, Intel Celeron N4020, 4 GB RAM, 64 GB... (10 reviews)\n",
      "   â€¢ SGIN Laptop 15.6 Inch Laptops Computer, Celeron N4... (5 reviews)\n",
      "ğŸ’¾ DonnÃ©es sauvegardÃ©es: ../data/raw/amazon_laptop_20250629_150840.csv\n",
      "\n",
      "ğŸ“‹ APERÃ‡U DES DONNÃ‰ES (5 premiÃ¨res reviews):\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Review 1:\n",
      "  product_name: HP 14 Laptop, Intel Celeron N4020, 4 GB RAM, 64 GB...\n",
      "  user_rating: 5.0\n",
      "  review_text: Got myself this Laptop. The sound is great. It's not touch screen sadly, but the Speed is decent for...\n",
      "  reviewer_name: Maria Torres\n",
      "\n",
      "Review 2:\n",
      "  product_name: HP 14 Laptop, Intel Celeron N4020, 4 GB RAM, 64 GB...\n",
      "  user_rating: 5.0\n",
      "  review_text: This laptop is incredibly fast with amazing battery life, and it definitely doesn't overheat. The ke...\n",
      "  reviewer_name: Sigi-Ann Miller\n",
      "\n",
      "Review 3:\n",
      "  product_name: HP 14 Laptop, Intel Celeron N4020, 4 GB RAM, 64 GB...\n",
      "  user_rating: 4.0\n",
      "  review_text: This is listed as a budget laptop, but if one is knowledgeable about pc's you can make it into a les...\n",
      "  reviewer_name: sam17704\n",
      "\n",
      "Review 4:\n",
      "  product_name: HP 14 Laptop, Intel Celeron N4020, 4 GB RAM, 64 GB...\n",
      "  user_rating: 5.0\n",
      "  review_text: Great price. But now I know just how much I appreciate my tablet. Sorry. But I wouldnâ€™t buy again\n",
      "  reviewer_name: skyhawk\n",
      "\n",
      "Review 5:\n",
      "  product_name: HP 14 Laptop, Intel Celeron N4020, 4 GB RAM, 64 GB...\n",
      "  user_rating: 5.0\n",
      "  review_text: this item been very helpful in what am doing with my own life. This product is helpful.\n",
      "  reviewer_name: sunny\n",
      "ğŸ§¹ Nettoyage des donnÃ©es...\n",
      "ğŸ“ Doublons supprimÃ©s: 1\n",
      "âœ… 15 reviews nettoyÃ©es\n",
      "\n",
      "================================================================================\n",
      "ğŸ“Š SCRAPING TERMINÃ‰ - STATISTIQUES\n",
      "================================================================================\n",
      "â±ï¸ DurÃ©e: 4.0 minutes\n",
      "ğŸ“¦ Produits traitÃ©s: 2\n",
      "ğŸ“ Reviews rÃ©cupÃ©rÃ©es: 17\n",
      "âŒ Erreurs: 0\n",
      "ğŸ“ˆ Taux de succÃ¨s: 100.0%\n",
      "\n",
      "======================================================================\n",
      "ğŸ“Š ANALYSE DES RÃ‰SULTATS\n",
      "----------------------------------------------------------------------\n",
      "âœ… Total reviews: 15\n",
      "ğŸ“¦ Produits uniques: 2\n",
      "ğŸ“ Longueur moyenne: 492 caractÃ¨res\n",
      "â­ Distribution des notes:\n",
      "   1.0 Ã©toiles: 1 reviews\n",
      "   3.0 Ã©toiles: 1 reviews\n",
      "   4.0 Ã©toiles: 2 reviews\n",
      "   5.0 Ã©toiles: 9 reviews\n",
      "\n",
      "ğŸ† Top produits par nombre de reviews:\n",
      "   â€¢ HP 14 Laptop, Intel Celeron N4020, 4 GB RAM, 64 GB... (10 reviews)\n",
      "   â€¢ SGIN Laptop 15.6 Inch Laptops Computer, Celeron N4... (5 reviews)\n",
      "ğŸ’¾ DonnÃ©es sauvegardÃ©es: ../data/raw/amazon_laptop_20250629_150840.csv\n",
      "\n",
      "ğŸ“‹ APERÃ‡U DES DONNÃ‰ES (5 premiÃ¨res reviews):\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Review 1:\n",
      "  product_name: HP 14 Laptop, Intel Celeron N4020, 4 GB RAM, 64 GB...\n",
      "  user_rating: 5.0\n",
      "  review_text: Got myself this Laptop. The sound is great. It's not touch screen sadly, but the Speed is decent for...\n",
      "  reviewer_name: Maria Torres\n",
      "\n",
      "Review 2:\n",
      "  product_name: HP 14 Laptop, Intel Celeron N4020, 4 GB RAM, 64 GB...\n",
      "  user_rating: 5.0\n",
      "  review_text: This laptop is incredibly fast with amazing battery life, and it definitely doesn't overheat. The ke...\n",
      "  reviewer_name: Sigi-Ann Miller\n",
      "\n",
      "Review 3:\n",
      "  product_name: HP 14 Laptop, Intel Celeron N4020, 4 GB RAM, 64 GB...\n",
      "  user_rating: 4.0\n",
      "  review_text: This is listed as a budget laptop, but if one is knowledgeable about pc's you can make it into a les...\n",
      "  reviewer_name: sam17704\n",
      "\n",
      "Review 4:\n",
      "  product_name: HP 14 Laptop, Intel Celeron N4020, 4 GB RAM, 64 GB...\n",
      "  user_rating: 5.0\n",
      "  review_text: Great price. But now I know just how much I appreciate my tablet. Sorry. But I wouldnâ€™t buy again\n",
      "  reviewer_name: skyhawk\n",
      "\n",
      "Review 5:\n",
      "  product_name: HP 14 Laptop, Intel Celeron N4020, 4 GB RAM, 64 GB...\n",
      "  user_rating: 5.0\n",
      "  review_text: this item been very helpful in what am doing with my own life. This product is helpful.\n",
      "  reviewer_name: sunny\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yann\\AppData\\Local\\Temp\\ipykernel_21532\\361164114.py:587: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['review_text'] = df['review_text'].str.strip()\n",
      "C:\\Users\\Yann\\AppData\\Local\\Temp\\ipykernel_21532\\361164114.py:588: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['review_title'] = df['review_title'].str.strip()\n",
      "C:\\Users\\Yann\\AppData\\Local\\Temp\\ipykernel_21532\\361164114.py:589: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['product_name'] = df['product_name'].str.strip()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Driver scraper fermÃ©\n",
      "\n",
      "====================================================================================================\n",
      "ğŸ‰ WORKFLOW TERMINÃ‰ AVEC SUCCÃˆS!\n",
      "====================================================================================================\n",
      "\n",
      "âœ… Workflow terminÃ© - 15 reviews rÃ©cupÃ©rÃ©es\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_name</th>\n",
       "      <th>product_category</th>\n",
       "      <th>product_url</th>\n",
       "      <th>product_price</th>\n",
       "      <th>scraped_at</th>\n",
       "      <th>review_text</th>\n",
       "      <th>review_title</th>\n",
       "      <th>user_rating</th>\n",
       "      <th>reviewer_name</th>\n",
       "      <th>review_date</th>\n",
       "      <th>review_length</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HP 14 Laptop, Intel Celeron N4020, 4 GB RAM, 6...</td>\n",
       "      <td>laptop</td>\n",
       "      <td>https://www.amazon.com/HP-Micro-edge-Microsoft...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>2025-06-29T15:05:04.594833</td>\n",
       "      <td>Got myself this Laptop. The sound is great. It...</td>\n",
       "      <td>Honest Reviewww</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Maria Torres</td>\n",
       "      <td>Reviewed in the United States on June 20, 2025</td>\n",
       "      <td>224</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HP 14 Laptop, Intel Celeron N4020, 4 GB RAM, 6...</td>\n",
       "      <td>laptop</td>\n",
       "      <td>https://www.amazon.com/HP-Micro-edge-Microsoft...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>2025-06-29T15:05:04.635622</td>\n",
       "      <td>This laptop is incredibly fast with amazing ba...</td>\n",
       "      <td>Good quality</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Sigi-Ann Miller</td>\n",
       "      <td>Reviewed in the United States on June 19, 2025</td>\n",
       "      <td>180</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HP 14 Laptop, Intel Celeron N4020, 4 GB RAM, 6...</td>\n",
       "      <td>laptop</td>\n",
       "      <td>https://www.amazon.com/HP-Micro-edge-Microsoft...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>2025-06-29T15:05:04.666732</td>\n",
       "      <td>This is listed as a budget laptop, but if one ...</td>\n",
       "      <td>It's a steal if you know how to fine tune it.</td>\n",
       "      <td>4.0</td>\n",
       "      <td>sam17704</td>\n",
       "      <td>Reviewed in the United States on May 8, 2025</td>\n",
       "      <td>593</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HP 14 Laptop, Intel Celeron N4020, 4 GB RAM, 6...</td>\n",
       "      <td>laptop</td>\n",
       "      <td>https://www.amazon.com/HP-Micro-edge-Microsoft...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>2025-06-29T15:05:04.707828</td>\n",
       "      <td>Great price. But now I know just how much I ap...</td>\n",
       "      <td>Slow</td>\n",
       "      <td>5.0</td>\n",
       "      <td>skyhawk</td>\n",
       "      <td>Reviewed in the United States on June 24, 2025</td>\n",
       "      <td>97</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HP 14 Laptop, Intel Celeron N4020, 4 GB RAM, 6...</td>\n",
       "      <td>laptop</td>\n",
       "      <td>https://www.amazon.com/HP-Micro-edge-Microsoft...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>2025-06-29T15:05:04.741401</td>\n",
       "      <td>this item been very helpful in what am doing w...</td>\n",
       "      <td>great item</td>\n",
       "      <td>5.0</td>\n",
       "      <td>sunny</td>\n",
       "      <td>Reviewed in the United States on June 19, 2025</td>\n",
       "      <td>87</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HP 14 Laptop, Intel Celeron N4020, 4 GB RAM, 6...</td>\n",
       "      <td>laptop</td>\n",
       "      <td>https://www.amazon.com/HP-Micro-edge-Microsoft...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>2025-06-29T15:05:04.773148</td>\n",
       "      <td>Perfect for the business I'm running</td>\n",
       "      <td>Is laptop very well needed</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Tim J Callantine</td>\n",
       "      <td>Reviewed in the United States on June 24, 2025</td>\n",
       "      <td>36</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HP 14 Laptop, Intel Celeron N4020, 4 GB RAM, 6...</td>\n",
       "      <td>laptop</td>\n",
       "      <td>https://www.amazon.com/HP-Micro-edge-Microsoft...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>2025-06-29T15:05:04.804627</td>\n",
       "      <td>This is a good laptop, not a great one. It is ...</td>\n",
       "      <td>GOOD, NOT GREAT</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Michael</td>\n",
       "      <td>Reviewed in the United States on May 26, 2025</td>\n",
       "      <td>162</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HP 14 Laptop, Intel Celeron N4020, 4 GB RAM, 6...</td>\n",
       "      <td>laptop</td>\n",
       "      <td>https://www.amazon.com/HP-Micro-edge-Microsoft...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>2025-06-29T15:05:04.835407</td>\n",
       "      <td>Works well.\\nWish I had searched for one with ...</td>\n",
       "      <td>Good basic laptop</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Eric Spindler</td>\n",
       "      <td>Reviewed in the United States on June 25, 2025</td>\n",
       "      <td>98</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>HP 14 Laptop, Intel Celeron N4020, 4 GB RAM, 6...</td>\n",
       "      <td>laptop</td>\n",
       "      <td>https://www.amazon.com/HP-Micro-edge-Microsoft...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>2025-06-29T15:05:04.865424</td>\n",
       "      <td>Excelente producto</td>\n",
       "      <td>Laptop HP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sergio Vergara</td>\n",
       "      <td>Reviewed in Mexico on August 27, 2024</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>HP 14 Laptop, Intel Celeron N4020, 4 GB RAM, 6...</td>\n",
       "      <td>laptop</td>\n",
       "      <td>https://www.amazon.com/HP-Micro-edge-Microsoft...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>2025-06-29T15:05:50.040514</td>\n",
       "      <td>Compre este equipo por la marca HP, la verdad ...</td>\n",
       "      <td>Laptop HP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Karla Alejandra Castillo</td>\n",
       "      <td>Reviewed in Mexico on November 30, 2023</td>\n",
       "      <td>649</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>SGIN Laptop 15.6 Inch Laptops Computer, Celero...</td>\n",
       "      <td>laptop</td>\n",
       "      <td>https://www.amazon.com/SGIN-Laptops-Computer-C...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>2025-06-29T15:07:35.170075</td>\n",
       "      <td>Is 2025 a year of Linux laptop? Given how well...</td>\n",
       "      <td>Cheap laptop for Linux/Ubuntu setup</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Max G.</td>\n",
       "      <td>Reviewed in the United States on April 21, 2025</td>\n",
       "      <td>1718</td>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>SGIN Laptop 15.6 Inch Laptops Computer, Celero...</td>\n",
       "      <td>laptop</td>\n",
       "      <td>https://www.amazon.com/SGIN-Laptops-Computer-C...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>2025-06-29T15:07:35.202699</td>\n",
       "      <td>Honestly, I wasnâ€™t expecting much when I order...</td>\n",
       "      <td>Lightweight and Fast</td>\n",
       "      <td>5.0</td>\n",
       "      <td>VIGY</td>\n",
       "      <td>Reviewed in the United States on May 1, 2025</td>\n",
       "      <td>1360</td>\n",
       "      <td>246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>SGIN Laptop 15.6 Inch Laptops Computer, Celero...</td>\n",
       "      <td>laptop</td>\n",
       "      <td>https://www.amazon.com/SGIN-Laptops-Computer-C...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>2025-06-29T15:07:35.232934</td>\n",
       "      <td>This 15.6\" Laptop is the underdog that punches...</td>\n",
       "      <td>ğŸ’» Sleek, Silver, and Surprisingly Powerful! ğŸš€</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Wehash Technology</td>\n",
       "      <td>Reviewed in the United States on April 8, 2025</td>\n",
       "      <td>1212</td>\n",
       "      <td>197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>SGIN Laptop 15.6 Inch Laptops Computer, Celero...</td>\n",
       "      <td>laptop</td>\n",
       "      <td>https://www.amazon.com/SGIN-Laptops-Computer-C...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>2025-06-29T15:07:35.263044</td>\n",
       "      <td>At first, this laptop seemed like a solid choi...</td>\n",
       "      <td>Disappointing Screen Issues After Setup = Unus...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Brittney D. from the Britt Brat Breakdown</td>\n",
       "      <td>Reviewed in the United States on June 24, 2025</td>\n",
       "      <td>616</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>SGIN Laptop 15.6 Inch Laptops Computer, Celero...</td>\n",
       "      <td>laptop</td>\n",
       "      <td>https://www.amazon.com/SGIN-Laptops-Computer-C...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>2025-06-29T15:07:35.293552</td>\n",
       "      <td>This laptop is a great choice. Multitasking, s...</td>\n",
       "      <td>Worth it.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Amazon Customer</td>\n",
       "      <td>Reviewed in the United States on June 3, 2025</td>\n",
       "      <td>330</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         product_name product_category  \\\n",
       "0   HP 14 Laptop, Intel Celeron N4020, 4 GB RAM, 6...           laptop   \n",
       "1   HP 14 Laptop, Intel Celeron N4020, 4 GB RAM, 6...           laptop   \n",
       "2   HP 14 Laptop, Intel Celeron N4020, 4 GB RAM, 6...           laptop   \n",
       "3   HP 14 Laptop, Intel Celeron N4020, 4 GB RAM, 6...           laptop   \n",
       "4   HP 14 Laptop, Intel Celeron N4020, 4 GB RAM, 6...           laptop   \n",
       "5   HP 14 Laptop, Intel Celeron N4020, 4 GB RAM, 6...           laptop   \n",
       "6   HP 14 Laptop, Intel Celeron N4020, 4 GB RAM, 6...           laptop   \n",
       "7   HP 14 Laptop, Intel Celeron N4020, 4 GB RAM, 6...           laptop   \n",
       "8   HP 14 Laptop, Intel Celeron N4020, 4 GB RAM, 6...           laptop   \n",
       "10  HP 14 Laptop, Intel Celeron N4020, 4 GB RAM, 6...           laptop   \n",
       "12  SGIN Laptop 15.6 Inch Laptops Computer, Celero...           laptop   \n",
       "13  SGIN Laptop 15.6 Inch Laptops Computer, Celero...           laptop   \n",
       "14  SGIN Laptop 15.6 Inch Laptops Computer, Celero...           laptop   \n",
       "15  SGIN Laptop 15.6 Inch Laptops Computer, Celero...           laptop   \n",
       "16  SGIN Laptop 15.6 Inch Laptops Computer, Celero...           laptop   \n",
       "\n",
       "                                          product_url product_price  \\\n",
       "0   https://www.amazon.com/HP-Micro-edge-Microsoft...           N/A   \n",
       "1   https://www.amazon.com/HP-Micro-edge-Microsoft...           N/A   \n",
       "2   https://www.amazon.com/HP-Micro-edge-Microsoft...           N/A   \n",
       "3   https://www.amazon.com/HP-Micro-edge-Microsoft...           N/A   \n",
       "4   https://www.amazon.com/HP-Micro-edge-Microsoft...           N/A   \n",
       "5   https://www.amazon.com/HP-Micro-edge-Microsoft...           N/A   \n",
       "6   https://www.amazon.com/HP-Micro-edge-Microsoft...           N/A   \n",
       "7   https://www.amazon.com/HP-Micro-edge-Microsoft...           N/A   \n",
       "8   https://www.amazon.com/HP-Micro-edge-Microsoft...           N/A   \n",
       "10  https://www.amazon.com/HP-Micro-edge-Microsoft...           N/A   \n",
       "12  https://www.amazon.com/SGIN-Laptops-Computer-C...           N/A   \n",
       "13  https://www.amazon.com/SGIN-Laptops-Computer-C...           N/A   \n",
       "14  https://www.amazon.com/SGIN-Laptops-Computer-C...           N/A   \n",
       "15  https://www.amazon.com/SGIN-Laptops-Computer-C...           N/A   \n",
       "16  https://www.amazon.com/SGIN-Laptops-Computer-C...           N/A   \n",
       "\n",
       "                    scraped_at  \\\n",
       "0   2025-06-29T15:05:04.594833   \n",
       "1   2025-06-29T15:05:04.635622   \n",
       "2   2025-06-29T15:05:04.666732   \n",
       "3   2025-06-29T15:05:04.707828   \n",
       "4   2025-06-29T15:05:04.741401   \n",
       "5   2025-06-29T15:05:04.773148   \n",
       "6   2025-06-29T15:05:04.804627   \n",
       "7   2025-06-29T15:05:04.835407   \n",
       "8   2025-06-29T15:05:04.865424   \n",
       "10  2025-06-29T15:05:50.040514   \n",
       "12  2025-06-29T15:07:35.170075   \n",
       "13  2025-06-29T15:07:35.202699   \n",
       "14  2025-06-29T15:07:35.232934   \n",
       "15  2025-06-29T15:07:35.263044   \n",
       "16  2025-06-29T15:07:35.293552   \n",
       "\n",
       "                                          review_text  \\\n",
       "0   Got myself this Laptop. The sound is great. It...   \n",
       "1   This laptop is incredibly fast with amazing ba...   \n",
       "2   This is listed as a budget laptop, but if one ...   \n",
       "3   Great price. But now I know just how much I ap...   \n",
       "4   this item been very helpful in what am doing w...   \n",
       "5                Perfect for the business I'm running   \n",
       "6   This is a good laptop, not a great one. It is ...   \n",
       "7   Works well.\\nWish I had searched for one with ...   \n",
       "8                                  Excelente producto   \n",
       "10  Compre este equipo por la marca HP, la verdad ...   \n",
       "12  Is 2025 a year of Linux laptop? Given how well...   \n",
       "13  Honestly, I wasnâ€™t expecting much when I order...   \n",
       "14  This 15.6\" Laptop is the underdog that punches...   \n",
       "15  At first, this laptop seemed like a solid choi...   \n",
       "16  This laptop is a great choice. Multitasking, s...   \n",
       "\n",
       "                                         review_title  user_rating  \\\n",
       "0                                     Honest Reviewww          5.0   \n",
       "1                                        Good quality          5.0   \n",
       "2       It's a steal if you know how to fine tune it.          4.0   \n",
       "3                                                Slow          5.0   \n",
       "4                                          great item          5.0   \n",
       "5                          Is laptop very well needed          5.0   \n",
       "6                                     GOOD, NOT GREAT          3.0   \n",
       "7                                   Good basic laptop          4.0   \n",
       "8                                           Laptop HP          NaN   \n",
       "10                                          Laptop HP          NaN   \n",
       "12                Cheap laptop for Linux/Ubuntu setup          5.0   \n",
       "13                               Lightweight and Fast          5.0   \n",
       "14      ğŸ’» Sleek, Silver, and Surprisingly Powerful! ğŸš€          5.0   \n",
       "15  Disappointing Screen Issues After Setup = Unus...          1.0   \n",
       "16                                          Worth it.          5.0   \n",
       "\n",
       "                                reviewer_name  \\\n",
       "0                                Maria Torres   \n",
       "1                             Sigi-Ann Miller   \n",
       "2                                    sam17704   \n",
       "3                                     skyhawk   \n",
       "4                                       sunny   \n",
       "5                            Tim J Callantine   \n",
       "6                                     Michael   \n",
       "7                               Eric Spindler   \n",
       "8                              Sergio Vergara   \n",
       "10                   Karla Alejandra Castillo   \n",
       "12                                     Max G.   \n",
       "13                                       VIGY   \n",
       "14                          Wehash Technology   \n",
       "15  Brittney D. from the Britt Brat Breakdown   \n",
       "16                            Amazon Customer   \n",
       "\n",
       "                                        review_date  review_length  word_count  \n",
       "0    Reviewed in the United States on June 20, 2025            224          40  \n",
       "1    Reviewed in the United States on June 19, 2025            180          29  \n",
       "2      Reviewed in the United States on May 8, 2025            593         126  \n",
       "3    Reviewed in the United States on June 24, 2025             97          19  \n",
       "4    Reviewed in the United States on June 19, 2025             87          17  \n",
       "5    Reviewed in the United States on June 24, 2025             36           6  \n",
       "6     Reviewed in the United States on May 26, 2025            162          34  \n",
       "7    Reviewed in the United States on June 25, 2025             98          19  \n",
       "8             Reviewed in Mexico on August 27, 2024             18           2  \n",
       "10          Reviewed in Mexico on November 30, 2023            649         126  \n",
       "12  Reviewed in the United States on April 21, 2025           1718         306  \n",
       "13     Reviewed in the United States on May 1, 2025           1360         246  \n",
       "14   Reviewed in the United States on April 8, 2025           1212         197  \n",
       "15   Reviewed in the United States on June 24, 2025            616         106  \n",
       "16    Reviewed in the United States on June 3, 2025            330          58  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "robust_reviews_menu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "503c90b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 12)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "df = pd.read_csv(\"../data/raw/amazon_laptop_20250629_150840.csv\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "631a9ba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['product_name', 'product_category', 'product_url', 'product_price',\n",
       "       'scraped_at', 'review_text', 'review_title', 'user_rating',\n",
       "       'reviewer_name', 'review_date', 'review_length', 'word_count'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a6baaef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_rating\n",
       "5.0    9\n",
       "4.0    2\n",
       "3.0    1\n",
       "1.0    1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['user_rating'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42d24a8",
   "metadata": {},
   "source": [
    "# More robust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c0342917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scout ultra-sÃ©curisÃ© crÃ©Ã© avec fallbacks multiples et logging!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SCOUT ULTRA-SÃ‰CURISÃ‰ AVEC FALLBACKS MULTIPLES\n",
    "# ============================================================================\n",
    "\n",
    "from datetime import datetime\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException, WebDriverException\n",
    "\n",
    "class UltraSecureProductReviewScout:\n",
    "    \"\"\"\n",
    "    Scout ultra-sÃ©curisÃ© avec fallbacks multiples pour dÃ©tecter les balises\n",
    "    - Fallbacks de driver multiples\n",
    "    - DÃ©tection adaptative des sÃ©lecteurs\n",
    "    - Gestion d'erreurs exhaustive\n",
    "    - Datetime pour tracking des commentaires\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.driver = None\n",
    "        self.wait = None\n",
    "        self.detected_selectors = {}\n",
    "        self.fallback_drivers = []\n",
    "        self.session_log = []\n",
    "        \n",
    "        # Base de sÃ©lecteurs Ã©tendue avec fallbacks\n",
    "        self.base_selectors = {\n",
    "            'amazon': {\n",
    "                'products': {\n",
    "                    'containers': [\n",
    "                        '[data-component-type=\"s-search-result\"]',\n",
    "                        '.s-result-item',\n",
    "                        '.s-widget-container .s-card-container',\n",
    "                        '[data-asin]:not([data-asin=\"\"])',\n",
    "                        '.s-card-container',\n",
    "                        '.sg-col-inner',\n",
    "                        '[cel_widget_id*=\"MAIN-SEARCH_RESULTS\"]'\n",
    "                    ],\n",
    "                    'titles': [\n",
    "                        'h2 span',\n",
    "                        'h2 a span',\n",
    "                        '.s-size-mini span',\n",
    "                        '.a-size-base-plus',\n",
    "                        '[data-cy=\"title-recipe-title\"]',\n",
    "                        '.a-size-medium',\n",
    "                        '.s-link-style a span',\n",
    "                        'h2.a-size-mini span'\n",
    "                    ],\n",
    "                    'urls': [\n",
    "                        'h2 a',\n",
    "                        '.a-link-normal',\n",
    "                        'a[href*=\"/dp/\"]',\n",
    "                        'a[href*=\"/gp/product/\"]',\n",
    "                        '.s-link-style a',\n",
    "                        'a[href*=\"amazon.com/\"]'\n",
    "                    ],\n",
    "                    'prices': [\n",
    "                        '.a-price .a-offscreen',\n",
    "                        '.a-price-whole',\n",
    "                        '.a-price-range .a-offscreen',\n",
    "                        '.a-price-symbol + .a-price-whole',\n",
    "                        '.a-price-fraction',\n",
    "                        '.s-price-range-display',\n",
    "                        '.a-price-display'\n",
    "                    ],\n",
    "                    'ratings': [\n",
    "                        '.a-icon-alt',\n",
    "                        '.a-star-mini .a-icon-alt',\n",
    "                        'span[aria-label*=\"stars\"]',\n",
    "                        '.a-icon-star',\n",
    "                        '.a-icon-star-small'\n",
    "                    ]\n",
    "                },\n",
    "                'reviews': {\n",
    "                    'containers': [\n",
    "                        '[data-hook=\"review\"]',\n",
    "                        '.review',\n",
    "                        '.cr-original-review-content',\n",
    "                        '.reviewText',\n",
    "                        '.a-section.review',\n",
    "                        '[data-hook=\"mobley-review-content\"]'\n",
    "                    ],\n",
    "                    'titles': [\n",
    "                        '[data-hook=\"review-title\"] span',\n",
    "                        '.review-title',\n",
    "                        '.cr-original-review-content .review-title',\n",
    "                        '.a-text-bold span',\n",
    "                        '[data-hook=\"review-title\"]'\n",
    "                    ],\n",
    "                    'texts': [\n",
    "                        '[data-hook=\"review-body\"] span',\n",
    "                        '.review-text',\n",
    "                        '.cr-original-review-content .review-text',\n",
    "                        '.reviewText',\n",
    "                        '[data-hook=\"review-collapsed-text\"]',\n",
    "                        '.a-expander-content span'\n",
    "                    ],\n",
    "                    'ratings': [\n",
    "                        '[data-hook=\"review-star-rating\"] .a-icon-alt',\n",
    "                        '.review-rating .a-icon-alt',\n",
    "                        '.cr-original-review-content .a-icon-alt',\n",
    "                        '.a-icon-star .a-icon-alt',\n",
    "                        'i.a-icon-star'\n",
    "                    ],\n",
    "                    'authors': [\n",
    "                        '.a-profile-name',\n",
    "                        '.review-author',\n",
    "                        '.cr-original-review-content .author',\n",
    "                        '[data-hook=\"genome-widget\"] .a-profile-name',\n",
    "                        '.a-profile-content .a-profile-name'\n",
    "                    ],\n",
    "                    'dates': [\n",
    "                        '[data-hook=\"review-date\"]',\n",
    "                        '.review-date',\n",
    "                        '.cr-original-review-content .review-date',\n",
    "                        '.a-color-secondary.review-date'\n",
    "                    ],\n",
    "                    'helpful_votes': [\n",
    "                        '[data-hook=\"helpful-vote-statement\"]',\n",
    "                        '.cr-vote-buttons',\n",
    "                        '.helpful-vote'\n",
    "                    ]\n",
    "                }\n",
    "            },\n",
    "            'ebay': {\n",
    "                'products': {\n",
    "                    'containers': [\n",
    "                        '.s-item',\n",
    "                        '.srp-results .s-item',\n",
    "                        '.b-listing__wrap',\n",
    "                        '.x-refine__main__list .s-item'\n",
    "                    ],\n",
    "                    'titles': [\n",
    "                        '.s-item__title',\n",
    "                        '.it-ttl',\n",
    "                        '.b-listing__title',\n",
    "                        '.s-item__title-text'\n",
    "                    ],\n",
    "                    'urls': [\n",
    "                        '.s-item__link',\n",
    "                        '.it-ttl a',\n",
    "                        '.b-listing__title a',\n",
    "                        '.s-item__title a'\n",
    "                    ],\n",
    "                    'prices': [\n",
    "                        '.s-item__price',\n",
    "                        '.notranslate',\n",
    "                        '.b-listing__price',\n",
    "                        '.s-item__price-detail'\n",
    "                    ]\n",
    "                },\n",
    "                'reviews': {\n",
    "                    'containers': [\n",
    "                        '.review-item',\n",
    "                        '.ebay-review',\n",
    "                        '.reviews .review',\n",
    "                        '.review-wrapper'\n",
    "                    ],\n",
    "                    'texts': [\n",
    "                        '.review-item-content',\n",
    "                        '.ebay-review-text',\n",
    "                        '.review-content',\n",
    "                        '.review-text'\n",
    "                    ],\n",
    "                    'ratings': [\n",
    "                        '.star-rating',\n",
    "                        '.rating-stars',\n",
    "                        '.ebay-star-rating'\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def log_action(self, action, status=\"SUCCESS\", details=\"\"):\n",
    "        \"\"\"Log des actions avec timestamp\"\"\"\n",
    "        entry = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'action': action,\n",
    "            'status': status,\n",
    "            'details': details\n",
    "        }\n",
    "        self.session_log.append(entry)\n",
    "        print(f\"[{entry['timestamp']}] {status}: {action} - {details}\")\n",
    "    \n",
    "    def setup_ultra_secure_driver(self, headless=True, timeout=30, max_fallback_attempts=5):\n",
    "        \"\"\"Configuration driver avec fallbacks multiples ultra-sÃ©curisÃ©s\"\"\"\n",
    "        \n",
    "        self.log_action(\"DRIVER_SETUP_START\", \"INFO\", f\"Headless: {headless}, Timeout: {timeout}s\")\n",
    "        \n",
    "        # MÃ©thodes de crÃ©ation de driver par ordre de prÃ©fÃ©rence\n",
    "        driver_methods = [\n",
    "            self._create_undetected_chrome_driver,\n",
    "            self._create_selenium_chrome_driver,\n",
    "            self._create_basic_chrome_driver,\n",
    "            self._create_firefox_fallback_driver,\n",
    "            self._create_edge_fallback_driver\n",
    "        ]\n",
    "        \n",
    "        for attempt in range(max_fallback_attempts):\n",
    "            for method_idx, method in enumerate(driver_methods):\n",
    "                try:\n",
    "                    self.log_action(f\"DRIVER_ATTEMPT\", \"INFO\", f\"Tentative {attempt+1}/{max_fallback_attempts}, MÃ©thode {method_idx+1}\")\n",
    "                    \n",
    "                    # Nettoyer les drivers existants\n",
    "                    self._cleanup_existing_drivers()\n",
    "                    \n",
    "                    # Essayer la mÃ©thode\n",
    "                    driver = method(headless, timeout)\n",
    "                    \n",
    "                    if driver and self._test_driver_functionality(driver):\n",
    "                        self.driver = driver\n",
    "                        self.wait = WebDriverWait(driver, timeout)\n",
    "                        self.log_action(\"DRIVER_SETUP_SUCCESS\", \"SUCCESS\", f\"MÃ©thode {method_idx+1} rÃ©ussie\")\n",
    "                        return True\n",
    "                    else:\n",
    "                        if driver:\n",
    "                            try:\n",
    "                                driver.quit()\n",
    "                            except:\n",
    "                                pass\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    self.log_action(f\"DRIVER_METHOD_{method_idx+1}_FAILED\", \"ERROR\", str(e)[:100])\n",
    "                    continue\n",
    "            \n",
    "            # DÃ©lai entre les tentatives\n",
    "            if attempt < max_fallback_attempts - 1:\n",
    "                delay = (attempt + 1) * 3\n",
    "                self.log_action(\"RETRY_DELAY\", \"INFO\", f\"Attente {delay}s avant nouvelle tentative\")\n",
    "                time.sleep(delay)\n",
    "        \n",
    "        self.log_action(\"DRIVER_SETUP_FAILED\", \"ERROR\", \"Toutes les mÃ©thodes ont Ã©chouÃ©\")\n",
    "        return False\n",
    "    \n",
    "    def _create_undetected_chrome_driver(self, headless, timeout):\n",
    "        \"\"\"MÃ©thode 1: Undetected Chrome (prÃ©fÃ©rÃ©e)\"\"\"\n",
    "        try:\n",
    "            import undetected_chromedriver as uc\n",
    "            \n",
    "            options = uc.ChromeOptions()\n",
    "            \n",
    "            # Arguments ultra-sÃ©curisÃ©s\n",
    "            secure_args = [\n",
    "                '--no-sandbox',\n",
    "                '--disable-dev-shm-usage',\n",
    "                '--disable-gpu',\n",
    "                '--disable-web-security',\n",
    "                '--disable-features=VizDisplayCompositor',\n",
    "                '--disable-blink-features=AutomationControlled',\n",
    "                '--disable-extensions',\n",
    "                '--no-first-run',\n",
    "                '--no-default-browser-check',\n",
    "                '--disable-default-apps',\n",
    "                '--window-size=1920,1080',\n",
    "                '--start-maximized',\n",
    "                '--disable-infobars',\n",
    "                '--disable-notifications',\n",
    "                '--disable-popup-blocking'\n",
    "            ]\n",
    "            \n",
    "            for arg in secure_args:\n",
    "                options.add_argument(arg)\n",
    "            \n",
    "            if headless:\n",
    "                options.add_argument('--headless=new')\n",
    "            \n",
    "            # User agent ultra-rÃ©aliste\n",
    "            user_agents = [\n",
    "                'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "                'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',\n",
    "                'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "            ]\n",
    "            \n",
    "            try:\n",
    "                user_agent = random.choice(REALISTIC_USER_AGENTS)\n",
    "            except:\n",
    "                user_agent = random.choice(user_agents)\n",
    "                \n",
    "            options.add_argument(f'--user-agent={user_agent}')\n",
    "            \n",
    "            # PrÃ©fÃ©rences sÃ©curisÃ©es\n",
    "            prefs = {\n",
    "                \"profile.default_content_setting_values.notifications\": 2,\n",
    "                \"profile.default_content_settings.popups\": 0,\n",
    "                \"profile.managed_default_content_settings.images\": 1,\n",
    "                \"profile.default_content_setting_values.geolocation\": 2,\n",
    "                \"profile.default_content_setting_values.media_stream_mic\": 2,\n",
    "                \"profile.default_content_setting_values.media_stream_camera\": 2\n",
    "            }\n",
    "            options.add_experimental_option(\"prefs\", prefs)\n",
    "            \n",
    "            # CrÃ©ation du driver\n",
    "            driver = uc.Chrome(\n",
    "                options=options,\n",
    "                version_main=None,\n",
    "                headless=headless,\n",
    "                use_subprocess=False,\n",
    "                log_level=3\n",
    "            )\n",
    "            \n",
    "            # Configuration des timeouts\n",
    "            driver.set_page_load_timeout(timeout)\n",
    "            driver.implicitly_wait(10)\n",
    "            \n",
    "            return driver\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.log_action(\"UNDETECTED_CHROME_FAILED\", \"ERROR\", str(e))\n",
    "            return None\n",
    "    \n",
    "    def _create_selenium_chrome_driver(self, headless, timeout):\n",
    "        \"\"\"MÃ©thode 2: Selenium Chrome classique\"\"\"\n",
    "        try:\n",
    "            from selenium import webdriver\n",
    "            from selenium.webdriver.chrome.service import Service\n",
    "            from selenium.webdriver.chrome.options import Options\n",
    "            from webdriver_manager.chrome import ChromeDriverManager\n",
    "            \n",
    "            options = Options()\n",
    "            \n",
    "            # Arguments de base\n",
    "            args = [\n",
    "                '--no-sandbox',\n",
    "                '--disable-dev-shm-usage',\n",
    "                '--disable-gpu',\n",
    "                '--window-size=1920,1080'\n",
    "            ]\n",
    "            \n",
    "            if headless:\n",
    "                args.append('--headless')\n",
    "            \n",
    "            for arg in args:\n",
    "                options.add_argument(arg)\n",
    "            \n",
    "            # Service gÃ©rÃ© automatiquement\n",
    "            service = Service(ChromeDriverManager().install())\n",
    "            \n",
    "            driver = webdriver.Chrome(service=service, options=options)\n",
    "            driver.set_page_load_timeout(timeout)\n",
    "            \n",
    "            return driver\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.log_action(\"SELENIUM_CHROME_FAILED\", \"ERROR\", str(e))\n",
    "            return None\n",
    "    \n",
    "    def _create_basic_chrome_driver(self, headless, timeout):\n",
    "        \"\"\"MÃ©thode 3: Chrome basique sans webdriver-manager\"\"\"\n",
    "        try:\n",
    "            from selenium import webdriver\n",
    "            from selenium.webdriver.chrome.options import Options\n",
    "            \n",
    "            options = Options()\n",
    "            options.add_argument('--no-sandbox')\n",
    "            options.add_argument('--disable-dev-shm-usage')\n",
    "            \n",
    "            if headless:\n",
    "                options.add_argument('--headless')\n",
    "            \n",
    "            driver = webdriver.Chrome(options=options)\n",
    "            driver.set_page_load_timeout(timeout)\n",
    "            \n",
    "            return driver\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.log_action(\"BASIC_CHROME_FAILED\", \"ERROR\", str(e))\n",
    "            return None\n",
    "    \n",
    "    def _create_firefox_fallback_driver(self, headless, timeout):\n",
    "        \"\"\"MÃ©thode 4: Firefox fallback\"\"\"\n",
    "        try:\n",
    "            from selenium import webdriver\n",
    "            from selenium.webdriver.firefox.options import Options\n",
    "            \n",
    "            options = Options()\n",
    "            if headless:\n",
    "                options.add_argument('--headless')\n",
    "            \n",
    "            driver = webdriver.Firefox(options=options)\n",
    "            driver.set_page_load_timeout(timeout)\n",
    "            \n",
    "            return driver\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.log_action(\"FIREFOX_FAILED\", \"ERROR\", str(e))\n",
    "            return None\n",
    "    \n",
    "    def _create_edge_fallback_driver(self, headless, timeout):\n",
    "        \"\"\"MÃ©thode 5: Edge fallback\"\"\"\n",
    "        try:\n",
    "            from selenium import webdriver\n",
    "            from selenium.webdriver.edge.options import Options\n",
    "            \n",
    "            options = Options()\n",
    "            options.add_argument('--no-sandbox')\n",
    "            if headless:\n",
    "                options.add_argument('--headless')\n",
    "            \n",
    "            driver = webdriver.Edge(options=options)\n",
    "            driver.set_page_load_timeout(timeout)\n",
    "            \n",
    "            return driver\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.log_action(\"EDGE_FAILED\", \"ERROR\", str(e))\n",
    "            return None\n",
    "    \n",
    "    def _cleanup_existing_drivers(self):\n",
    "        \"\"\"Nettoie les drivers existants\"\"\"\n",
    "        try:\n",
    "            if self.driver:\n",
    "                self.driver.quit()\n",
    "                self.driver = None\n",
    "            \n",
    "            for driver in self.fallback_drivers:\n",
    "                try:\n",
    "                    driver.quit()\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            self.fallback_drivers.clear()\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.log_action(\"CLEANUP_WARNING\", \"WARNING\", str(e))\n",
    "    \n",
    "    def _test_driver_functionality(self, driver):\n",
    "        \"\"\"Test complet de fonctionnalitÃ© du driver\"\"\"\n",
    "        try:\n",
    "            # Test 1: Navigation basique\n",
    "            driver.get(\"data:text/html,<html><body><h1>Test</h1></body></html>\")\n",
    "            \n",
    "            # Test 2: Trouver un Ã©lÃ©ment\n",
    "            element = driver.find_element(By.TAG_NAME, \"h1\")\n",
    "            if element.text != \"Test\":\n",
    "                return False\n",
    "            \n",
    "            # Test 3: ExÃ©cuter JavaScript\n",
    "            result = driver.execute_script(\"return 'test'\")\n",
    "            if result != \"test\":\n",
    "                return False\n",
    "            \n",
    "            # Test 4: Test de navigation rÃ©elle (optionnel et rapide)\n",
    "            try:\n",
    "                driver.set_page_load_timeout(10)\n",
    "                driver.get(\"https://httpbin.org/user-agent\")\n",
    "                time.sleep(2)\n",
    "            except:\n",
    "                pass  # Pas critique si Ã§a Ã©choue\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.log_action(\"DRIVER_TEST_FAILED\", \"ERROR\", str(e))\n",
    "            return False\n",
    "    \n",
    "    def detect_site_selectors_ultra_secure(self, site_url, search_term=\"laptop\", max_retries=3):\n",
    "        \"\"\"\n",
    "        DÃ©tection ultra-sÃ©curisÃ©e avec fallbacks multiples pour les sÃ©lecteurs\n",
    "        \"\"\"\n",
    "        \n",
    "        if not self.driver:\n",
    "            self.log_action(\"DETECT_SELECTORS_NO_DRIVER\", \"ERROR\", \"Driver non initialisÃ©\")\n",
    "            return {}\n",
    "        \n",
    "        site_type = self._detect_site_type(site_url)\n",
    "        if site_type == 'unknown':\n",
    "            self.log_action(\"DETECT_SELECTORS_UNKNOWN_SITE\", \"ERROR\", f\"Site non supportÃ©: {site_url}\")\n",
    "            return {}\n",
    "        \n",
    "        self.log_action(\"DETECT_SELECTORS_START\", \"INFO\", f\"Site: {site_type}, Terme: {search_term}\")\n",
    "        \n",
    "        detected = {\n",
    "            'site': site_type,\n",
    "            'products': {},\n",
    "            'reviews': {},\n",
    "            'detection_timestamp': datetime.now().isoformat(),\n",
    "            'search_term': search_term\n",
    "        }\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                self.log_action(f\"DETECT_ATTEMPT_{attempt+1}\", \"INFO\", f\"Tentative {attempt+1}/{max_retries}\")\n",
    "                \n",
    "                # Phase 1: DÃ©tection produits avec fallbacks\n",
    "                product_selectors = self._detect_product_selectors_with_fallbacks(site_url, search_term, site_type)\n",
    "                \n",
    "                if product_selectors:\n",
    "                    detected['products'] = product_selectors\n",
    "                    self.log_action(\"PRODUCT_SELECTORS_SUCCESS\", \"SUCCESS\", f\"{len(product_selectors)} sÃ©lecteurs trouvÃ©s\")\n",
    "                    \n",
    "                    # Phase 2: DÃ©tection reviews avec fallbacks\n",
    "                    review_selectors = self._detect_review_selectors_with_fallbacks(site_type, product_selectors)\n",
    "                    \n",
    "                    if review_selectors:\n",
    "                        detected['reviews'] = review_selectors\n",
    "                        self.log_action(\"REVIEW_SELECTORS_SUCCESS\", \"SUCCESS\", f\"{len(review_selectors)} sÃ©lecteurs trouvÃ©s\")\n",
    "                    else:\n",
    "                        # Fallback vers sÃ©lecteurs de base\n",
    "                        detected['reviews'] = self.base_selectors[site_type]['reviews']\n",
    "                        self.log_action(\"REVIEW_SELECTORS_FALLBACK\", \"WARNING\", \"Utilisation sÃ©lecteurs par dÃ©faut\")\n",
    "                    \n",
    "                    # Sauvegarde sÃ©curisÃ©e\n",
    "                    self._save_detected_selectors_secure(detected, site_type)\n",
    "                    return detected\n",
    "                else:\n",
    "                    self.log_action(f\"PRODUCT_SELECTORS_ATTEMPT_{attempt+1}_FAILED\", \"ERROR\", \"Aucun sÃ©lecteur produit trouvÃ©\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.log_action(f\"DETECT_ATTEMPT_{attempt+1}_ERROR\", \"ERROR\", str(e))\n",
    "                \n",
    "                if attempt < max_retries - 1:\n",
    "                    delay = (attempt + 1) * 5\n",
    "                    self.log_action(\"DETECT_RETRY_DELAY\", \"INFO\", f\"Attente {delay}s\")\n",
    "                    time.sleep(delay)\n",
    "        \n",
    "        # Fallback final : utiliser les sÃ©lecteurs de base\n",
    "        self.log_action(\"DETECT_FINAL_FALLBACK\", \"WARNING\", \"Utilisation sÃ©lecteurs de base complets\")\n",
    "        detected['products'] = self._get_fallback_product_selectors(site_type)\n",
    "        detected['reviews'] = self.base_selectors[site_type]['reviews']\n",
    "        \n",
    "        return detected\n",
    "    \n",
    "    def _detect_site_type(self, site_url):\n",
    "        \"\"\"DÃ©tection robuste du type de site\"\"\"\n",
    "        site_url_lower = site_url.lower()\n",
    "        \n",
    "        if 'amazon' in site_url_lower:\n",
    "            return 'amazon'\n",
    "        elif 'ebay' in site_url_lower:\n",
    "            return 'ebay'\n",
    "        else:\n",
    "            return 'unknown'\n",
    "    \n",
    "    def _detect_product_selectors_with_fallbacks(self, site_url, search_term, site_type, max_page_attempts=3):\n",
    "        \"\"\"DÃ©tection de sÃ©lecteurs produits avec fallbacks multiples\"\"\"\n",
    "        \n",
    "        # URLs de recherche alternatives\n",
    "        search_urls = self._generate_search_urls(site_type, search_term)\n",
    "        \n",
    "        for url_attempt, search_url in enumerate(search_urls[:max_page_attempts]):\n",
    "            try:\n",
    "                self.log_action(f\"NAVIGATE_SEARCH_URL_{url_attempt+1}\", \"INFO\", search_url[:80])\n",
    "                \n",
    "                # Navigation sÃ©curisÃ©e\n",
    "                if not self._safe_navigate(search_url):\n",
    "                    continue\n",
    "                \n",
    "                # Attendre le chargement avec fallbacks\n",
    "                if not self._wait_for_page_load():\n",
    "                    continue\n",
    "                \n",
    "                # Tester les sÃ©lecteurs avec diffÃ©rentes stratÃ©gies\n",
    "                selectors = self._test_product_selectors_comprehensive(site_type)\n",
    "                \n",
    "                if selectors and len(selectors) >= 2:  # Au moins conteneur + un autre\n",
    "                    self.log_action(\"PRODUCT_SELECTORS_FOUND\", \"SUCCESS\", f\"URL: {url_attempt+1}, SÃ©lecteurs: {len(selectors)}\")\n",
    "                    return selectors\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.log_action(f\"SEARCH_URL_{url_attempt+1}_ERROR\", \"ERROR\", str(e))\n",
    "                continue\n",
    "        \n",
    "        return {}\n",
    "    \n",
    "    def _generate_search_urls(self, site_type, search_term):\n",
    "        \"\"\"GÃ©nÃ¨re plusieurs URLs de recherche alternatives\"\"\"\n",
    "        \n",
    "        search_term_encoded = search_term.replace(' ', '+')\n",
    "        search_term_underscore = search_term.replace(' ', '_')\n",
    "        \n",
    "        if site_type == 'amazon':\n",
    "            return [\n",
    "                f\"https://www.amazon.com/s?k={search_term_encoded}\",\n",
    "                f\"https://www.amazon.com/s?k={search_term_encoded}&ref=sr_pg_1\",\n",
    "                f\"https://www.amazon.com/s?field-keywords={search_term_encoded}\",\n",
    "                f\"https://amazon.com/s?k={search_term_encoded}\"\n",
    "            ]\n",
    "        elif site_type == 'ebay':\n",
    "            return [\n",
    "                f\"https://www.ebay.com/sch/i.html?_nkw={search_term_encoded}\",\n",
    "                f\"https://www.ebay.com/sch/i.html?_nkw={search_term_underscore}\",\n",
    "                f\"https://ebay.com/sch/i.html?_nkw={search_term_encoded}\"\n",
    "            ]\n",
    "        \n",
    "        return []\n",
    "    \n",
    "    def _safe_navigate(self, url, max_retries=3):\n",
    "        \"\"\"Navigation sÃ©curisÃ©e avec retry\"\"\"\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                self.driver.get(url)\n",
    "                return True\n",
    "                \n",
    "            except TimeoutException:\n",
    "                self.log_action(f\"NAVIGATE_TIMEOUT_ATTEMPT_{attempt+1}\", \"WARNING\", f\"Timeout sur {url}\")\n",
    "                if attempt < max_retries - 1:\n",
    "                    time.sleep(3)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                self.log_action(f\"NAVIGATE_ERROR_ATTEMPT_{attempt+1}\", \"ERROR\", str(e))\n",
    "                if attempt < max_retries - 1:\n",
    "                    time.sleep(5)\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def _wait_for_page_load(self, timeout=15):\n",
    "        \"\"\"Attente du chargement de page avec fallbacks\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # StratÃ©gie 1: Attendre que le DOM soit prÃªt\n",
    "            self.wait.until(lambda driver: driver.execute_script(\"return document.readyState\") == \"complete\")\n",
    "            time.sleep(2)\n",
    "            \n",
    "            # StratÃ©gie 2: Attendre des Ã©lÃ©ments spÃ©cifiques\n",
    "            common_selectors = ['body', '[data-component-type]', '.s-result-item', '.s-item']\n",
    "            \n",
    "            for selector in common_selectors:\n",
    "                try:\n",
    "                    WebDriverWait(self.driver, 5).until(\n",
    "                        EC.presence_of_element_located((By.CSS_SELECTOR, selector))\n",
    "                    )\n",
    "                    break\n",
    "                except TimeoutException:\n",
    "                    continue\n",
    "            \n",
    "            # DÃ©lai supplÃ©mentaire pour le JavaScript\n",
    "            time.sleep(3)\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.log_action(\"PAGE_LOAD_WARNING\", \"WARNING\", str(e))\n",
    "            time.sleep(5)  # Fallback basique\n",
    "            return True  # Continue mÃªme en cas d'erreur\n",
    "    \n",
    "    def _test_product_selectors_comprehensive(self, site_type):\n",
    "        \"\"\"Test complet des sÃ©lecteurs produits avec scoring\"\"\"\n",
    "        \n",
    "        base_selectors = self.base_selectors[site_type]['products']\n",
    "        validated_selectors = {}\n",
    "        selector_scores = {}\n",
    "        \n",
    "        # Test conteneurs avec scoring\n",
    "        self.log_action(\"TEST_CONTAINERS_START\", \"INFO\", f\"Test de {len(base_selectors['containers'])} conteneurs\")\n",
    "        \n",
    "        for container_selector in base_selectors['containers']:\n",
    "            try:\n",
    "                elements = self.driver.find_elements(By.CSS_SELECTOR, container_selector)\n",
    "                score = len(elements)\n",
    "                \n",
    "                if score >= 3:  # Minimum 3 produits\n",
    "                    selector_scores[container_selector] = score\n",
    "                    self.log_action(\"CONTAINER_VALID\", \"SUCCESS\", f\"{container_selector}: {score} Ã©lÃ©ments\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.log_action(\"CONTAINER_ERROR\", \"WARNING\", f\"{container_selector}: {str(e)[:50]}\")\n",
    "                continue\n",
    "        \n",
    "        # Choisir le meilleur conteneur\n",
    "        if selector_scores:\n",
    "            best_container = max(selector_scores, key=selector_scores.get)\n",
    "            validated_selectors['container'] = best_container\n",
    "            self.log_action(\"BEST_CONTAINER_SELECTED\", \"SUCCESS\", f\"{best_container} (score: {selector_scores[best_container]})\")\n",
    "        else:\n",
    "            self.log_action(\"NO_CONTAINER_FOUND\", \"ERROR\", \"Aucun conteneur valide trouvÃ©\")\n",
    "            return {}\n",
    "        \n",
    "        # Test autres sÃ©lecteurs dans le contexte du meilleur conteneur\n",
    "        container_elements = self.driver.find_elements(By.CSS_SELECTOR, best_container)\n",
    "        \n",
    "        if container_elements:\n",
    "            first_container = container_elements[0]\n",
    "            \n",
    "            # Test chaque type de sÃ©lecteur\n",
    "            selector_types = ['titles', 'urls', 'prices', 'ratings']\n",
    "            \n",
    "            for selector_type in selector_types:\n",
    "                self.log_action(f\"TEST_{selector_type.upper()}_START\", \"INFO\", f\"Test de {len(base_selectors[selector_type])} {selector_type}\")\n",
    "                \n",
    "                for selector in base_selectors[selector_type]:\n",
    "                    try:\n",
    "                        element = first_container.find_element(By.CSS_SELECTOR, selector)\n",
    "                        \n",
    "                        # Validation spÃ©cifique par type\n",
    "                        if self._validate_selector_content(element, selector_type):\n",
    "                            validated_selectors[selector_type[:-1]] = selector  # Enlever le 's'\n",
    "                            self.log_action(f\"{selector_type.upper()}_VALID\", \"SUCCESS\", f\"{selector}\")\n",
    "                            break\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        continue\n",
    "        \n",
    "        return validated_selectors\n",
    "    \n",
    "    def _validate_selector_content(self, element, selector_type):\n",
    "        \"\"\"Validation du contenu selon le type de sÃ©lecteur\"\"\"\n",
    "        \n",
    "        try:\n",
    "            if selector_type == 'titles':\n",
    "                text = element.text.strip()\n",
    "                return len(text) > 5 and not text.isdigit()\n",
    "            \n",
    "            elif selector_type == 'urls':\n",
    "                href = element.get_attribute('href')\n",
    "                return href and ('amazon.com' in href or 'ebay.com' in href or '/dp/' in href)\n",
    "            \n",
    "            elif selector_type == 'prices':\n",
    "                text = element.text.strip()\n",
    "                return text and ('$' in text or 'â‚¬' in text or 'Â£' in text or any(c.isdigit() for c in text))\n",
    "            \n",
    "            elif selector_type == 'ratings':\n",
    "                text = element.get_attribute('textContent') or element.text\n",
    "                return text and ('star' in text.lower() or 'Ã©toile' in text.lower() or any(c.isdigit() for c in text))\n",
    "            \n",
    "        except Exception:\n",
    "            return False\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def _detect_review_selectors_with_fallbacks(self, site_type, product_selectors):\n",
    "        \"\"\"DÃ©tection sÃ©lecteurs reviews avec navigation sÃ©curisÃ©e\"\"\"\n",
    "        \n",
    "        if not product_selectors.get('url'):\n",
    "            self.log_action(\"REVIEW_DETECT_NO_URL\", \"ERROR\", \"Pas de sÃ©lecteur URL produit\")\n",
    "            return {}\n",
    "        \n",
    "        try:\n",
    "            # RÃ©cupÃ©rer plusieurs liens produits pour plus de robustesse\n",
    "            product_links = self.driver.find_elements(By.CSS_SELECTOR, product_selectors['url'])[:3]\n",
    "            \n",
    "            for link_idx, link in enumerate(product_links):\n",
    "                try:\n",
    "                    product_url = link.get_attribute('href')\n",
    "                    if not product_url:\n",
    "                        continue\n",
    "                    \n",
    "                    self.log_action(f\"TEST_PRODUCT_{link_idx+1}\", \"INFO\", product_url[:80])\n",
    "                    \n",
    "                    # Navigation vers le produit\n",
    "                    if not self._safe_navigate(product_url):\n",
    "                        continue\n",
    "                    \n",
    "                    if not self._wait_for_page_load():\n",
    "                        continue\n",
    "                    \n",
    "                    # Chercher et naviguer vers les reviews\n",
    "                    reviews_url = self._find_reviews_page_comprehensive(product_url)\n",
    "                    \n",
    "                    if reviews_url:\n",
    "                        self.log_action(\"REVIEWS_PAGE_FOUND\", \"SUCCESS\", reviews_url[:80])\n",
    "                        \n",
    "                        if not self._safe_navigate(reviews_url):\n",
    "                            continue\n",
    "                        \n",
    "                        if not self._wait_for_page_load():\n",
    "                            continue\n",
    "                    \n",
    "                    # Tester les sÃ©lecteurs de reviews\n",
    "                    review_selectors = self._test_review_selectors_comprehensive(site_type)\n",
    "                    \n",
    "                    if review_selectors and len(review_selectors) >= 2:\n",
    "                        self.log_action(\"REVIEW_SELECTORS_SUCCESS\", \"SUCCESS\", f\"Produit {link_idx+1}: {len(review_selectors)} sÃ©lecteurs\")\n",
    "                        return review_selectors\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    self.log_action(f\"REVIEW_PRODUCT_{link_idx+1}_ERROR\", \"ERROR\", str(e))\n",
    "                    continue\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.log_action(\"REVIEW_DETECT_GLOBAL_ERROR\", \"ERROR\", str(e))\n",
    "        \n",
    "        return {}\n",
    "    \n",
    "    def _find_reviews_page_comprehensive(self, product_url):\n",
    "        \"\"\"Recherche comprehensive de la page reviews\"\"\"\n",
    "        \n",
    "        # StratÃ©gie 1: Chercher les liens existants\n",
    "        review_link_selectors = [\n",
    "            'a[href*=\"customer-reviews\"]',\n",
    "            'a[href*=\"reviews\"]',\n",
    "            'a[href*=\"review\"]',\n",
    "            '.cr-widget-ACR a',\n",
    "            '[data-hook=\"see-all-reviews-link-foot\"]',\n",
    "            'a[href*=\"product-reviews\"]'\n",
    "        ]\n",
    "        \n",
    "        for selector in review_link_selectors:\n",
    "            try:\n",
    "                links = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                for link in links:\n",
    "                    href = link.get_attribute('href')\n",
    "                    if href and 'review' in href:\n",
    "                        self.log_action(\"REVIEW_LINK_FOUND\", \"SUCCESS\", f\"Via {selector}\")\n",
    "                        return href\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        # StratÃ©gie 2: Construction d'URL pour Amazon\n",
    "        if 'amazon.com' in product_url:\n",
    "            asin_patterns = [\n",
    "                r'/dp/([A-Z0-9]{10})',\n",
    "                r'/gp/product/([A-Z0-9]{10})',\n",
    "                r'asin=([A-Z0-9]{10})'\n",
    "            ]\n",
    "            \n",
    "            for pattern in asin_patterns:\n",
    "                match = re.search(pattern, product_url)\n",
    "                if match:\n",
    "                    asin = match.group(1)\n",
    "                    constructed_url = f\"https://www.amazon.com/product-reviews/{asin}\"\n",
    "                    self.log_action(\"REVIEW_URL_CONSTRUCTED\", \"SUCCESS\", f\"ASIN: {asin}\")\n",
    "                    return constructed_url\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def _test_review_selectors_comprehensive(self, site_type):\n",
    "        \"\"\"Test complet des sÃ©lecteurs de reviews\"\"\"\n",
    "        \n",
    "        base_selectors = self.base_selectors[site_type]['reviews']\n",
    "        validated_selectors = {}\n",
    "        \n",
    "        # Test conteneurs de reviews\n",
    "        self.log_action(\"TEST_REVIEW_CONTAINERS\", \"INFO\", f\"Test de {len(base_selectors['containers'])} conteneurs\")\n",
    "        \n",
    "        best_container = None\n",
    "        max_reviews = 0\n",
    "        \n",
    "        for container_selector in base_selectors['containers']:\n",
    "            try:\n",
    "                review_elements = self.driver.find_elements(By.CSS_SELECTOR, container_selector)\n",
    "                if len(review_elements) > max_reviews:\n",
    "                    max_reviews = len(review_elements)\n",
    "                    best_container = container_selector\n",
    "                    \n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        if best_container and max_reviews >= 2:\n",
    "            validated_selectors['container'] = best_container\n",
    "            self.log_action(\"REVIEW_CONTAINER_SELECTED\", \"SUCCESS\", f\"{best_container}: {max_reviews} reviews\")\n",
    "            \n",
    "            # Test autres sÃ©lecteurs dans le contexte\n",
    "            review_containers = self.driver.find_elements(By.CSS_SELECTOR, best_container)\n",
    "            \n",
    "            if review_containers:\n",
    "                first_review = review_containers[0]\n",
    "                \n",
    "                # Test chaque type de sÃ©lecteur review\n",
    "                review_types = ['texts', 'titles', 'ratings', 'authors', 'dates']\n",
    "                \n",
    "                for review_type in review_types:\n",
    "                    if review_type in base_selectors:\n",
    "                        for selector in base_selectors[review_type]:\n",
    "                            try:\n",
    "                                element = first_review.find_element(By.CSS_SELECTOR, selector)\n",
    "                                \n",
    "                                if self._validate_review_content(element, review_type):\n",
    "                                    validated_selectors[review_type[:-1]] = selector  # Enlever le 's'\n",
    "                                    self.log_action(f\"REVIEW_{review_type.upper()}_VALID\", \"SUCCESS\", selector)\n",
    "                                    break\n",
    "                                    \n",
    "                            except:\n",
    "                                continue\n",
    "        \n",
    "        return validated_selectors\n",
    "    \n",
    "    def _validate_review_content(self, element, content_type):\n",
    "        \"\"\"Validation du contenu des reviews\"\"\"\n",
    "        \n",
    "        try:\n",
    "            if content_type == 'texts':\n",
    "                text = element.text.strip()\n",
    "                return len(text) > 20  # Review doit avoir au moins 20 caractÃ¨res\n",
    "            \n",
    "            elif content_type == 'titles':\n",
    "                text = element.text.strip()\n",
    "                return len(text) > 5 and len(text) < 200\n",
    "            \n",
    "            elif content_type == 'ratings':\n",
    "                text = element.get_attribute('textContent') or element.text\n",
    "                return text and ('star' in text.lower() or any(c.isdigit() for c in text))\n",
    "            \n",
    "            elif content_type == 'authors':\n",
    "                text = element.text.strip()\n",
    "                return len(text) > 1 and len(text) < 100\n",
    "            \n",
    "            elif content_type == 'dates':\n",
    "                text = element.text.strip()\n",
    "                return len(text) > 5 and ('20' in text or 'on ' in text.lower() or 'le ' in text.lower())\n",
    "            \n",
    "        except:\n",
    "            return False\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def _get_fallback_product_selectors(self, site_type):\n",
    "        \"\"\"SÃ©lecteurs de fallback robustes\"\"\"\n",
    "        \n",
    "        if site_type == 'amazon':\n",
    "            return {\n",
    "                'container': '[data-component-type=\"s-search-result\"]',\n",
    "                'title': 'h2 span',\n",
    "                'url': 'h2 a',\n",
    "                'price': '.a-price .a-offscreen',\n",
    "                'rating': '.a-icon-alt'\n",
    "            }\n",
    "        elif site_type == 'ebay':\n",
    "            return {\n",
    "                'container': '.s-item',\n",
    "                'title': '.s-item__title',\n",
    "                'url': '.s-item__link',\n",
    "                'price': '.s-item__price'\n",
    "            }\n",
    "        \n",
    "        return {}\n",
    "    \n",
    "    def _save_detected_selectors_secure(self, selectors, site_type):\n",
    "        \"\"\"Sauvegarde sÃ©curisÃ©e avec backup\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # CrÃ©er le rÃ©pertoire de config\n",
    "            config_dir = \"../config\"\n",
    "            os.makedirs(config_dir, exist_ok=True)\n",
    "            \n",
    "            # Nom de fichier avec timestamp\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            filename = f\"{config_dir}/detected_selectors_{site_type}_{timestamp}.json\"\n",
    "            backup_filename = f\"{config_dir}/detected_selectors_{site_type}_backup.json\"\n",
    "            current_filename = f\"{config_dir}/detected_selectors_{site_type}.json\"\n",
    "            \n",
    "            # Sauvegarder avec timestamp\n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                json.dump(selectors, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "            # Sauvegarder backup\n",
    "            with open(backup_filename, 'w', encoding='utf-8') as f:\n",
    "                json.dump(selectors, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "            # Sauvegarder current (sans timestamp)\n",
    "            with open(current_filename, 'w', encoding='utf-8') as f:\n",
    "                json.dump(selectors, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "            self.log_action(\"SELECTORS_SAVED\", \"SUCCESS\", f\"3 fichiers sauvegardÃ©s dans {config_dir}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.log_action(\"SAVE_ERROR\", \"ERROR\", str(e))\n",
    "            return False\n",
    "    \n",
    "    def get_session_log(self):\n",
    "        \"\"\"Retourne le log de session\"\"\"\n",
    "        return self.session_log\n",
    "    \n",
    "    def save_session_log(self, filename=None):\n",
    "        \"\"\"Sauvegarde le log de session\"\"\"\n",
    "        try:\n",
    "            if filename is None:\n",
    "                timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                filename = f\"../logs/scout_session_{timestamp}.json\"\n",
    "            \n",
    "            os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "            \n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                json.dump(self.session_log, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "            self.log_action(\"SESSION_LOG_SAVED\", \"SUCCESS\", filename)\n",
    "            return filename\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.log_action(\"SESSION_LOG_SAVE_ERROR\", \"ERROR\", str(e))\n",
    "            return None\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Fermeture sÃ©curisÃ©e avec log\"\"\"\n",
    "        self.log_action(\"SCOUT_CLOSING\", \"INFO\", \"Fermeture du scout\")\n",
    "        \n",
    "        try:\n",
    "            if self.driver:\n",
    "                self.driver.quit()\n",
    "                self.driver = None\n",
    "            \n",
    "            for driver in self.fallback_drivers:\n",
    "                try:\n",
    "                    driver.quit()\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            self.fallback_drivers.clear()\n",
    "            \n",
    "            # Sauvegarder le log automatiquement\n",
    "            self.save_session_log()\n",
    "            \n",
    "            self.log_action(\"SCOUT_CLOSED\", \"SUCCESS\", \"Scout fermÃ© proprement\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.log_action(\"SCOUT_CLOSE_ERROR\", \"ERROR\", str(e))\n",
    "\n",
    "print(\"âœ… Scout ultra-sÃ©curisÃ© crÃ©Ã© avec fallbacks multiples et logging!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68dbacc",
   "metadata": {},
   "source": [
    "# ğŸ”’ Version Ultra-SÃ©curisÃ©e du Scout (2025-01-20)\n",
    "\n",
    "Cette version amÃ©liore encore le scout avec :\n",
    "- **Fallbacks multiples** pour la crÃ©ation du driver (undetected_chromedriver, Selenium Chrome, Firefox, Edge)\n",
    "- **DÃ©tection adaptative** des balises avec scoring et fallback sur sÃ©lecteurs de base\n",
    "- **Gestion d'erreurs exhaustive** avec logs datetime\n",
    "- **Sauvegarde automatique** des sÃ©lecteurs dÃ©tectÃ©s avec timestamp et backup\n",
    "- **Navigation ultra-sÃ©curisÃ©e** avec anti-dÃ©tection avancÃ©\n",
    "- **Options Chrome optimisÃ©es** pour Ã©viter la dÃ©tection\n",
    "- **Rotation d'user-agents** rÃ©alistes\n",
    "- **Gestion des proxies** (si configurÃ©s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "961cb153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ CrÃ©ation de la classe UltraSecureProductReviewScout...\n",
      "âœ… Classe UltraSecureProductReviewScout crÃ©Ã©e avec succÃ¨s!\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”’ CLASSE ULTRA-SÃ‰CURISÃ‰E POUR DÃ‰TECTION DES BALISES (2025-01-20)\n",
    "print(\"ğŸ”„ CrÃ©ation de la classe UltraSecureProductReviewScout...\")\n",
    "\n",
    "class UltraSecureProductReviewScout:\n",
    "    \"\"\"\n",
    "    Scout ultra-sÃ©curisÃ© avec fallbacks multiples pour la dÃ©tection des balises\n",
    "    AmÃ©liorations 2025-01-20 :\n",
    "    - Fallbacks multiples pour driver (undetected_chromedriver, Selenium, Firefox, Edge)\n",
    "    - DÃ©tection adaptative avec scoring et fallback\n",
    "    - Options Chrome optimisÃ©es anti-dÃ©tection\n",
    "    - Gestion exhaustive des erreurs avec logs datetime\n",
    "    - Sauvegarde automatique des sÃ©lecteurs et logs\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, headless=True, save_screenshots=True, use_proxy=None):\n",
    "        self.headless = headless\n",
    "        self.save_screenshots = save_screenshots\n",
    "        self.use_proxy = use_proxy\n",
    "        self.driver = None\n",
    "        self.fallback_drivers = []\n",
    "        self.session_log = {\n",
    "            \"session_start\": datetime.now().isoformat(),\n",
    "            \"actions\": [],\n",
    "            \"detected_selectors\": {},\n",
    "            \"fallback_attempts\": [],\n",
    "            \"errors\": []\n",
    "        }\n",
    "        self.detected_selectors = {}\n",
    "        self.current_site = None\n",
    "        \n",
    "        # User agents rÃ©alistes rotatifs\n",
    "        self.user_agents = [\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\",\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
    "            \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\",\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:122.0) Gecko/20100101 Firefox/122.0\",\n",
    "            \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2.1 Safari/605.1.15\"\n",
    "        ]\n",
    "        self.current_user_agent = random.choice(self.user_agents)\n",
    "        \n",
    "    def log_action(self, action_type, status, details=\"\", extra_data=None):\n",
    "        \"\"\"Log avec datetime pour toutes les actions\"\"\"\n",
    "        log_entry = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"action\": action_type,\n",
    "            \"status\": status,\n",
    "            \"details\": details\n",
    "        }\n",
    "        if extra_data:\n",
    "            log_entry[\"extra_data\"] = extra_data\n",
    "            \n",
    "        self.session_log[\"actions\"].append(log_entry)\n",
    "        \n",
    "        # Affichage avec datetime pour les actions importantes\n",
    "        if status in [\"ERROR\", \"SUCCESS\", \"WARNING\"]:\n",
    "            timestamp = datetime.now().strftime(\"%H:%M:%S\")\n",
    "            print(f\"[{timestamp}] {action_type}: {status} - {details}\")\n",
    "    \n",
    "    def get_ultra_secure_chrome_options(self):\n",
    "        \"\"\"Options Chrome ultra-sÃ©curisÃ©es optimisÃ©es\"\"\"\n",
    "        options = Options()\n",
    "        \n",
    "        if self.headless:\n",
    "            options.add_argument(\"--headless=new\")\n",
    "        \n",
    "        # Arguments anti-dÃ©tection optimisÃ©s\n",
    "        anti_detection_args = [\n",
    "            \"--no-sandbox\",\n",
    "            \"--disable-dev-shm-usage\",\n",
    "            \"--disable-gpu\",\n",
    "            \"--disable-software-rasterizer\",\n",
    "            \"--disable-background-timer-throttling\",\n",
    "            \"--disable-backgrounding-occluded-windows\",\n",
    "            \"--disable-renderer-backgrounding\",\n",
    "            \"--disable-infobars\",\n",
    "            \"--disable-extensions\",\n",
    "            \"--disable-plugins\",\n",
    "            \"--disable-images\",  # AccÃ©lÃ¨re le chargement\n",
    "            \"--disable-javascript\",  # Peut Ãªtre retirÃ© si JS nÃ©cessaire\n",
    "            \"--no-first-run\",\n",
    "            \"--no-default-browser-check\",\n",
    "            \"--ignore-certificate-errors\",\n",
    "            \"--ignore-ssl-errors\",\n",
    "            \"--ignore-certificate-errors-spki-list\",\n",
    "            \"--disable-blink-features=AutomationControlled\",\n",
    "            \"--disable-features=VizDisplayCompositor\",\n",
    "            \"--window-size=1920,1080\",\n",
    "            \"--start-maximized\"\n",
    "        ]\n",
    "        \n",
    "        for arg in anti_detection_args:\n",
    "            options.add_argument(arg)\n",
    "        \n",
    "        # User agent rÃ©aliste\n",
    "        options.add_argument(f\"--user-agent={self.current_user_agent}\")\n",
    "        \n",
    "        # PrÃ©fÃ©rences avancÃ©es anti-dÃ©tection\n",
    "        prefs = {\n",
    "            \"profile.default_content_setting_values\": {\n",
    "                \"notifications\": 2,\n",
    "                \"media_stream\": 2,\n",
    "                \"geolocation\": 2\n",
    "            },\n",
    "            \"profile.managed_default_content_settings\": {\n",
    "                \"images\": 2  # Bloquer les images pour accÃ©lÃ©rer\n",
    "            }\n",
    "        }\n",
    "        options.add_experimental_option(\"prefs\", prefs)\n",
    "        \n",
    "        # Proxy si configurÃ©\n",
    "        if self.use_proxy:\n",
    "            options.add_argument(f\"--proxy-server={self.use_proxy}\")\n",
    "        \n",
    "        return options\n",
    "    \n",
    "    def create_fallback_driver(self, driver_type=\"undetected_chrome\"):\n",
    "        \"\"\"CrÃ©ation de driver avec fallbacks multiples\"\"\"\n",
    "        self.log_action(\"DRIVER_CREATION_START\", \"INFO\", f\"Tentative crÃ©ation driver: {driver_type}\")\n",
    "        \n",
    "        try:\n",
    "            if driver_type == \"undetected_chrome\":\n",
    "                import undetected_chromedriver as uc\n",
    "                options = self.get_ultra_secure_chrome_options()\n",
    "                \n",
    "                driver = uc.Chrome(\n",
    "                    options=options,\n",
    "                    version_main=None,  # Auto-dÃ©tection de la version\n",
    "                    driver_executable_path=None,\n",
    "                    browser_executable_path=None,\n",
    "                    use_subprocess=True,\n",
    "                    debug=False\n",
    "                )\n",
    "                \n",
    "                # Scripts anti-dÃ©tection\n",
    "                driver.execute_cdp_cmd('Page.addScriptToEvaluateOnNewDocument', {\n",
    "                    \"source\": \"\"\"\n",
    "                        Object.defineProperty(navigator, 'webdriver', {\n",
    "                            get: () => undefined,\n",
    "                        });\n",
    "                        delete navigator.__proto__.webdriver;\n",
    "                    \"\"\"\n",
    "                })\n",
    "                \n",
    "                self.log_action(\"DRIVER_CREATION_SUCCESS\", \"SUCCESS\", f\"Driver {driver_type} crÃ©Ã©\")\n",
    "                return driver\n",
    "                \n",
    "            elif driver_type == \"selenium_chrome\":\n",
    "                from selenium import webdriver\n",
    "                options = self.get_ultra_secure_chrome_options()\n",
    "                \n",
    "                driver = webdriver.Chrome(options=options)\n",
    "                \n",
    "                # Script anti-dÃ©tection pour Selenium\n",
    "                driver.execute_script(\"\"\"\n",
    "                    Object.defineProperty(navigator, 'webdriver', {\n",
    "                        get: () => undefined,\n",
    "                    });\n",
    "                    delete navigator.__proto__.webdriver;\n",
    "                \"\"\")\n",
    "                \n",
    "                self.log_action(\"DRIVER_CREATION_SUCCESS\", \"SUCCESS\", f\"Driver {driver_type} crÃ©Ã©\")\n",
    "                return driver\n",
    "                \n",
    "            elif driver_type == \"firefox\":\n",
    "                from selenium import webdriver\n",
    "                from selenium.webdriver.firefox.options import Options as FirefoxOptions\n",
    "                \n",
    "                firefox_options = FirefoxOptions()\n",
    "                if self.headless:\n",
    "                    firefox_options.add_argument(\"--headless\")\n",
    "                \n",
    "                firefox_options.add_argument(\"--no-sandbox\")\n",
    "                firefox_options.add_argument(\"--disable-gpu\")\n",
    "                firefox_options.set_preference(\"general.useragent.override\", self.current_user_agent)\n",
    "                \n",
    "                driver = webdriver.Firefox(options=firefox_options)\n",
    "                self.log_action(\"DRIVER_CREATION_SUCCESS\", \"SUCCESS\", f\"Driver {driver_type} crÃ©Ã©\")\n",
    "                return driver\n",
    "                \n",
    "            elif driver_type == \"edge\":\n",
    "                from selenium import webdriver\n",
    "                from selenium.webdriver.edge.options import Options as EdgeOptions\n",
    "                \n",
    "                edge_options = EdgeOptions()\n",
    "                if self.headless:\n",
    "                    edge_options.add_argument(\"--headless\")\n",
    "                \n",
    "                edge_options.add_argument(\"--no-sandbox\")\n",
    "                edge_options.add_argument(\"--disable-gpu\")\n",
    "                edge_options.add_argument(f\"--user-agent={self.current_user_agent}\")\n",
    "                \n",
    "                driver = webdriver.Edge(options=edge_options)\n",
    "                self.log_action(\"DRIVER_CREATION_SUCCESS\", \"SUCCESS\", f\"Driver {driver_type} crÃ©Ã©\")\n",
    "                return driver\n",
    "                \n",
    "            else:\n",
    "                raise ValueError(f\"Type de driver non supportÃ©: {driver_type}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.log_action(\"DRIVER_CREATION_ERROR\", \"ERROR\", f\"Ã‰chec {driver_type}: {str(e)}\")\n",
    "            self.session_log[\"fallback_attempts\"].append({\n",
    "                \"driver_type\": driver_type,\n",
    "                \"error\": str(e),\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            })\n",
    "            return None\n",
    "    \n",
    "    def initialize_driver(self):\n",
    "        \"\"\"Initialisation du driver avec fallbacks multiples\"\"\"\n",
    "        driver_types = [\"undetected_chrome\", \"selenium_chrome\", \"firefox\", \"edge\"]\n",
    "        \n",
    "        self.log_action(\"DRIVER_INITIALIZATION_START\", \"INFO\", \"DÃ©marrage des fallbacks multiples\")\n",
    "        \n",
    "        for driver_type in driver_types:\n",
    "            try:\n",
    "                self.log_action(\"FALLBACK_ATTEMPT\", \"INFO\", f\"Tentative avec {driver_type}\")\n",
    "                \n",
    "                driver = self.create_fallback_driver(driver_type)\n",
    "                if driver:\n",
    "                    self.driver = driver\n",
    "                    self.log_action(\"DRIVER_INITIALIZED\", \"SUCCESS\", f\"Driver {driver_type} initialisÃ©\")\n",
    "                    \n",
    "                    # Test de fonctionnement\n",
    "                    driver.get(\"https://httpbin.org/user-agent\")\n",
    "                    time.sleep(2)\n",
    "                    \n",
    "                    self.log_action(\"DRIVER_TEST_SUCCESS\", \"SUCCESS\", f\"Driver {driver_type} fonctionnel\")\n",
    "                    return True\n",
    "                    \n",
    "            except Exception as e:\n",
    "                self.log_action(\"FALLBACK_ERROR\", \"ERROR\", f\"Ã‰chec {driver_type}: {str(e)}\")\n",
    "                if driver:\n",
    "                    try:\n",
    "                        driver.quit()\n",
    "                    except:\n",
    "                        pass\n",
    "                continue\n",
    "        \n",
    "        self.log_action(\"DRIVER_INITIALIZATION_FAILED\", \"ERROR\", \"Tous les drivers ont Ã©chouÃ©\")\n",
    "        return False\n",
    "    \n",
    "    def detect_product_elements(self, sample_urls, max_products=5):\n",
    "        \"\"\"DÃ©tection adaptative des balises produits avec scoring et fallback\"\"\"\n",
    "        if not self.driver:\n",
    "            if not self.initialize_driver():\n",
    "                self.log_action(\"DETECTION_FAILED\", \"ERROR\", \"Aucun driver disponible\")\n",
    "                return {}\n",
    "        \n",
    "        site_selectors = {\n",
    "            \"product_link\": [],\n",
    "            \"product_title\": [],\n",
    "            \"rating\": [],\n",
    "            \"review_count\": [],\n",
    "            \"price\": []\n",
    "        }\n",
    "        \n",
    "        selector_scores = {}\n",
    "        \n",
    "        self.log_action(\"PRODUCT_DETECTION_START\", \"INFO\", f\"Analyse de {len(sample_urls)} URLs\")\n",
    "        \n",
    "        for i, url in enumerate(sample_urls[:max_products]):\n",
    "            try:\n",
    "                self.log_action(\"PAGE_ANALYSIS_START\", \"INFO\", f\"Analyse page {i+1}: {url[:50]}...\")\n",
    "                \n",
    "                self.driver.get(url)\n",
    "                time.sleep(random.uniform(3, 6))\n",
    "                \n",
    "                # DÃ©tection des liens produits\n",
    "                product_selectors = [\n",
    "                    'a[href*=\"/dp/\"]',  # Amazon\n",
    "                    'a[href*=\"/gp/product/\"]',  # Amazon\n",
    "                    'a[data-testid*=\"product\"]',  # GÃ©nÃ©rique\n",
    "                    'a[class*=\"product\"]',  # GÃ©nÃ©rique\n",
    "                    '.s-result-item a',  # Amazon search\n",
    "                    '[data-component-type=\"s-search-result\"] a'  # Amazon\n",
    "                ]\n",
    "                \n",
    "                for selector in product_selectors:\n",
    "                    try:\n",
    "                        elements = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                        if elements:\n",
    "                            score = len(elements)\n",
    "                            if selector not in selector_scores:\n",
    "                                selector_scores[selector] = 0\n",
    "                            selector_scores[selector] += score\n",
    "                            self.log_action(\"SELECTOR_FOUND\", \"SUCCESS\", f\"{selector}: {score} Ã©lÃ©ments\")\n",
    "                    except:\n",
    "                        continue\n",
    "                \n",
    "                # DÃ©tection des titres\n",
    "                title_selectors = [\n",
    "                    'h1[class*=\"title\"]',\n",
    "                    'h2[class*=\"title\"]', \n",
    "                    '[data-testid*=\"title\"]',\n",
    "                    '.s-size-mini .s-color-base',  # Amazon\n",
    "                    'h3 a span'  # Amazon\n",
    "                ]\n",
    "                \n",
    "                for selector in title_selectors:\n",
    "                    try:\n",
    "                        elements = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                        if elements:\n",
    "                            score = len([e for e in elements if e.text.strip()])\n",
    "                            if selector not in selector_scores:\n",
    "                                selector_scores[selector] = 0\n",
    "                            selector_scores[selector] += score\n",
    "                    except:\n",
    "                        continue\n",
    "                \n",
    "                # DÃ©tection des ratings\n",
    "                rating_selectors = [\n",
    "                    '[class*=\"rating\"]',\n",
    "                    '[data-testid*=\"rating\"]',\n",
    "                    '.a-icon-alt',  # Amazon\n",
    "                    '[aria-label*=\"star\"]',\n",
    "                    '[aria-label*=\"Ã©toile\"]'\n",
    "                ]\n",
    "                \n",
    "                for selector in rating_selectors:\n",
    "                    try:\n",
    "                        elements = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                        if elements:\n",
    "                            score = len(elements)\n",
    "                            if selector not in selector_scores:\n",
    "                                selector_scores[selector] = 0\n",
    "                            selector_scores[selector] += score\n",
    "                    except:\n",
    "                        continue\n",
    "                \n",
    "                self.log_action(\"PAGE_ANALYSIS_SUCCESS\", \"SUCCESS\", f\"Page {i+1} analysÃ©e\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.log_action(\"PAGE_ANALYSIS_ERROR\", \"ERROR\", f\"Erreur page {i+1}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # SÃ©lection des meilleurs sÃ©lecteurs\n",
    "        best_selectors = {}\n",
    "        for element_type in site_selectors.keys():\n",
    "            relevant_selectors = [(sel, score) for sel, score in selector_scores.items() \n",
    "                                if any(keyword in sel.lower() for keyword in [element_type, 'product', 'title', 'rating', 'price'])]\n",
    "            \n",
    "            if relevant_selectors:\n",
    "                best_selector = max(relevant_selectors, key=lambda x: x[1])[0]\n",
    "                best_selectors[element_type] = best_selector\n",
    "                self.log_action(\"BEST_SELECTOR_FOUND\", \"SUCCESS\", f\"{element_type}: {best_selector}\")\n",
    "        \n",
    "        # Fallback sur sÃ©lecteurs de base si dÃ©tection insuffisante\n",
    "        if len(best_selectors) < 2:\n",
    "            self.log_action(\"FALLBACK_TO_BASE_SELECTORS\", \"WARNING\", \"Utilisation des sÃ©lecteurs de base\")\n",
    "            \n",
    "            fallback_selectors = {\n",
    "                \"amazon\": {\n",
    "                    \"product_link\": 'a[href*=\"/dp/\"]',\n",
    "                    \"product_title\": 'h3 a span',\n",
    "                    \"rating\": '.a-icon-alt',\n",
    "                    \"review_count\": 'a[href*=\"#customerReviews\"]',\n",
    "                    \"price\": '.a-price-whole'\n",
    "                },\n",
    "                \"ebay\": {\n",
    "                    \"product_link\": 'a[href*=\"/itm/\"]',\n",
    "                    \"product_title\": '.s-item__title',\n",
    "                    \"rating\": '.ebay-star-rating',\n",
    "                    \"review_count\": '.s-item__reviews',\n",
    "                    \"price\": '.s-item__price'\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            site_name = \"amazon\" if \"amazon\" in sample_urls[0] else \"ebay\"\n",
    "            best_selectors.update(fallback_selectors.get(site_name, {}))\n",
    "        \n",
    "        self.detected_selectors[self.current_site] = best_selectors\n",
    "        self.log_action(\"DETECTION_COMPLETE\", \"SUCCESS\", f\"{len(best_selectors)} sÃ©lecteurs dÃ©tectÃ©s\")\n",
    "        \n",
    "        return best_selectors\n",
    "    \n",
    "    def save_detected_selectors(self, site_name):\n",
    "        \"\"\"Sauvegarde automatique des sÃ©lecteurs avec timestamp et backup\"\"\"\n",
    "        try:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            \n",
    "            # Dossier config\n",
    "            config_dir = \"../config\"\n",
    "            os.makedirs(config_dir, exist_ok=True)\n",
    "            \n",
    "            # Fichier principal\n",
    "            filename = f\"detected_selectors_{site_name}.json\"\n",
    "            filepath = os.path.join(config_dir, filename)\n",
    "            \n",
    "            # Backup de l'ancien fichier s'il existe\n",
    "            if os.path.exists(filepath):\n",
    "                backup_filename = f\"detected_selectors_{site_name}_backup_{timestamp}.json\"\n",
    "                backup_filepath = os.path.join(config_dir, backup_filename)\n",
    "                shutil.copy2(filepath, backup_filepath)\n",
    "                self.log_action(\"BACKUP_CREATED\", \"SUCCESS\", backup_filename)\n",
    "            \n",
    "            # Sauvegarde des nouveaux sÃ©lecteurs\n",
    "            selector_data = {\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"site\": site_name,\n",
    "                \"selectors\": self.detected_selectors.get(site_name, {}),\n",
    "                \"session_info\": {\n",
    "                    \"user_agent\": self.current_user_agent,\n",
    "                    \"detection_method\": \"ultra_secure_scout\",\n",
    "                    \"version\": \"2025-01-20\"\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            with open(filepath, 'w', encoding='utf-8') as f:\n",
    "                json.dump(selector_data, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "            self.log_action(\"SELECTORS_SAVED\", \"SUCCESS\", filename)\n",
    "            return filename\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.log_action(\"SELECTORS_SAVE_ERROR\", \"ERROR\", str(e))\n",
    "            return None\n",
    "    \n",
    "    def save_session_log(self):\n",
    "        \"\"\"Sauvegarde automatique du log de session\"\"\"\n",
    "        try:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            \n",
    "            # Dossier logs\n",
    "            logs_dir = \"../logs\"\n",
    "            os.makedirs(logs_dir, exist_ok=True)\n",
    "            \n",
    "            filename = f\"ultra_scout_session_{timestamp}.json\"\n",
    "            filepath = os.path.join(logs_dir, filename)\n",
    "            \n",
    "            # Finalisation du log\n",
    "            self.session_log[\"session_end\"] = datetime.now().isoformat()\n",
    "            self.session_log[\"total_actions\"] = len(self.session_log[\"actions\"])\n",
    "            self.session_log[\"detected_selectors_count\"] = len(self.detected_selectors)\n",
    "            \n",
    "            with open(filepath, 'w', encoding='utf-8') as f:\n",
    "                json.dump(self.session_log, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "            self.log_action(\"SESSION_LOG_SAVED\", \"SUCCESS\", filename)\n",
    "            return filename\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.log_action(\"SESSION_LOG_SAVE_ERROR\", \"ERROR\", str(e))\n",
    "            return None\n",
    "    \n",
    "    def scout_site(self, site_name, sample_urls, max_products=5):\n",
    "        \"\"\"Scout complet d'un site avec toutes les sÃ©curitÃ©s\"\"\"\n",
    "        self.current_site = site_name\n",
    "        self.log_action(\"SCOUT_START\", \"INFO\", f\"DÃ©but du scout pour {site_name}\")\n",
    "        \n",
    "        try:\n",
    "            # DÃ©tection des Ã©lÃ©ments\n",
    "            selectors = self.detect_product_elements(sample_urls, max_products)\n",
    "            \n",
    "            if selectors:\n",
    "                # Sauvegarde automatique\n",
    "                self.save_detected_selectors(site_name)\n",
    "                self.log_action(\"SCOUT_SUCCESS\", \"SUCCESS\", f\"Scout {site_name} terminÃ© avec succÃ¨s\")\n",
    "                return selectors\n",
    "            else:\n",
    "                self.log_action(\"SCOUT_FAILED\", \"ERROR\", f\"Aucun sÃ©lecteur dÃ©tectÃ© pour {site_name}\")\n",
    "                return {}\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.log_action(\"SCOUT_ERROR\", \"ERROR\", f\"Erreur scout {site_name}: {str(e)}\")\n",
    "            return {}\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Fermeture sÃ©curisÃ©e avec log\"\"\"\n",
    "        self.log_action(\"SCOUT_CLOSING\", \"INFO\", \"Fermeture du scout ultra-sÃ©curisÃ©\")\n",
    "        \n",
    "        try:\n",
    "            if self.driver:\n",
    "                self.driver.quit()\n",
    "                self.driver = None\n",
    "            \n",
    "            for driver in self.fallback_drivers:\n",
    "                try:\n",
    "                    driver.quit()\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            self.fallback_drivers.clear()\n",
    "            \n",
    "            # Sauvegarder le log automatiquement\n",
    "            self.save_session_log()\n",
    "            \n",
    "            self.log_action(\"SCOUT_CLOSED\", \"SUCCESS\", \"Scout ultra-sÃ©curisÃ© fermÃ© proprement\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.log_action(\"SCOUT_CLOSE_ERROR\", \"ERROR\", str(e))\n",
    "\n",
    "print(\"âœ… Classe UltraSecureProductReviewScout crÃ©Ã©e avec succÃ¨s!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c5e117",
   "metadata": {},
   "source": [
    "# ğŸ§ª Exemples de Test du Workflow Ultra-SÃ©curisÃ©\n",
    "\n",
    "Cette section contient des exemples prÃªts Ã  l'emploi pour tester le systÃ¨me complet :\n",
    "- **Test Amazon** : Laptops gaming, reviews dÃ©taillÃ©es\n",
    "- **Test eBay** : Appareils Ã©lectroniques, diffÃ©rentes catÃ©gories  \n",
    "- **Test de robustesse** : Gestion des erreurs, fallbacks\n",
    "- **Validation des sÃ©lecteurs** : VÃ©rification de la dÃ©tection automatique\n",
    "- **Tests de performance** : Temps de rÃ©ponse, pagination\n",
    "\n",
    "âš ï¸ **Important** : Ces tests sont Ã  des fins Ã©ducatives uniquement. Respectez les conditions d'utilisation des sites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "becaaade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ® Exemple de test Amazon - Laptops Gaming\n",
      "==================================================\n",
      "âœ… Fonction de test Amazon crÃ©Ã©e. DÃ©commentez la derniÃ¨re ligne pour l'exÃ©cuter.\n"
     ]
    }
   ],
   "source": [
    "# ğŸ§ª EXEMPLE 1: TEST AMAZON LAPTOP GAMING AVEC SCOUT ULTRA-SÃ‰CURISÃ‰\n",
    "print(\"ğŸ® Exemple de test Amazon - Laptops Gaming\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "def test_amazon_gaming_laptops_ultra_secure():\n",
    "    \"\"\"\n",
    "    Test complet du workflow ultra-sÃ©curisÃ© sur Amazon\n",
    "    CatÃ©gorie: Gaming Laptops\n",
    "    \"\"\"\n",
    "    print(f\"[{datetime.now().strftime('%H:%M:%S')}] ğŸš€ DÃ©marrage du test Amazon Gaming Laptops\")\n",
    "    \n",
    "    # Configuration du test\n",
    "    test_config = {\n",
    "        \"site\": \"amazon\",\n",
    "        \"category\": \"gaming laptop\",\n",
    "        \"max_products\": 8,\n",
    "        \"reviews_per_rating\": 30,\n",
    "        \"headless\": False,  # Mode visible pour debugging\n",
    "        \"use_ultra_secure\": True\n",
    "    }\n",
    "    \n",
    "    print(\"ğŸ“‹ Configuration du test:\")\n",
    "    for key, value in test_config.items():\n",
    "        print(f\"   {key}: {value}\")\n",
    "    \n",
    "    try:\n",
    "        # Phase 1: Test du scout ultra-sÃ©curisÃ©\n",
    "        print(f\"\\n[{datetime.now().strftime('%H:%M:%S')}] ğŸ” Phase 1: Test du Scout Ultra-SÃ©curisÃ©\")\n",
    "        \n",
    "        scout = UltraSecureProductReviewScout(headless=test_config[\"headless\"])\n",
    "        \n",
    "        # URLs d'exemple pour Amazon gaming laptops\n",
    "        sample_urls = [\n",
    "            \"https://www.amazon.com/s?k=gaming+laptop&ref=nb_sb_noss\",\n",
    "            \"https://www.amazon.com/s?k=gaming+laptop+rtx&ref=nb_sb_noss_2\",\n",
    "        ]\n",
    "        \n",
    "        detected_selectors = scout.scout_site(\"amazon\", sample_urls, max_products=5)\n",
    "        \n",
    "        if detected_selectors:\n",
    "            print(f\"âœ… SÃ©lecteurs dÃ©tectÃ©s avec succÃ¨s:\")\n",
    "            for elem_type, selector in detected_selectors.items():\n",
    "                print(f\"   {elem_type}: {selector}\")\n",
    "        else:\n",
    "            print(\"âŒ Ã‰chec de la dÃ©tection des sÃ©lecteurs\")\n",
    "            return None\n",
    "        \n",
    "        scout.close()\n",
    "        \n",
    "        # Phase 2: Test du workflow complet\n",
    "        print(f\"\\n[{datetime.now().strftime('%H:%M:%S')}] ğŸ”„ Phase 2: Test du Workflow Complet\")\n",
    "        \n",
    "        # Utilisation du workflow principal avec les sÃ©lecteurs dÃ©tectÃ©s\n",
    "        result = robust_reviews_workflow(\n",
    "            category=test_config[\"category\"],\n",
    "            site=test_config[\"site\"],\n",
    "            max_products=test_config[\"max_products\"],\n",
    "            reviews_per_rating=test_config[\"reviews_per_rating\"],\n",
    "            headless=test_config[\"headless\"]\n",
    "        )\n",
    "        \n",
    "        if result is not None and len(result) > 0:\n",
    "            print(f\"âœ… Test terminÃ© avec succÃ¨s!\")\n",
    "            print(f\"ğŸ“Š RÃ©sultats: {len(result)} reviews rÃ©cupÃ©rÃ©es\")\n",
    "            print(f\"ğŸ“ˆ Colonnes: {list(result.columns)}\")\n",
    "            \n",
    "            # Analyse rapide des rÃ©sultats\n",
    "            if 'rating' in result.columns:\n",
    "                print(f\"â­ Rating moyen: {result['rating'].mean():.2f}\")\n",
    "            if 'review_text' in result.columns:\n",
    "                print(f\"ğŸ“ Longueur moyenne des reviews: {result['review_text'].str.len().mean():.0f} caractÃ¨res\")\n",
    "            \n",
    "            return result\n",
    "        else:\n",
    "            print(\"âŒ Aucune donnÃ©e rÃ©cupÃ©rÃ©e\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Erreur durant le test: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# âš ï¸ DÃ‰COMMENTEZ POUR EXÃ‰CUTER LE TEST\n",
    "# df_test_amazon = test_amazon_gaming_laptops_ultra_secure()\n",
    "\n",
    "print(\"âœ… Fonction de test Amazon crÃ©Ã©e. DÃ©commentez la derniÃ¨re ligne pour l'exÃ©cuter.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d72be8f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”Œ Exemple de test eBay - Ã‰lectronique\n",
      "==================================================\n",
      "âœ… Fonction de test eBay crÃ©Ã©e. DÃ©commentez la derniÃ¨re ligne pour l'exÃ©cuter.\n"
     ]
    }
   ],
   "source": [
    "# ğŸ§ª EXEMPLE 2: TEST EBAY Ã‰LECTRONIQUE AVEC ROBUSTESSE\n",
    "print(\"ğŸ”Œ Exemple de test eBay - Ã‰lectronique\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "def test_ebay_electronics_robust():\n",
    "    \"\"\"\n",
    "    Test complet du workflow sur eBay\n",
    "    CatÃ©gorie: Smartphones et Ã©lectronique\n",
    "    \"\"\"\n",
    "    print(f\"[{datetime.now().strftime('%H:%M:%S')}] ğŸš€ DÃ©marrage du test eBay Ã‰lectronique\")\n",
    "    \n",
    "    # Configuration du test\n",
    "    test_config = {\n",
    "        \"site\": \"ebay\",\n",
    "        \"category\": \"smartphone iphone\",\n",
    "        \"max_products\": 6,\n",
    "        \"reviews_per_rating\": 20,\n",
    "        \"headless\": True,  # Mode headless pour eBay\n",
    "        \"use_ultra_secure\": True\n",
    "    }\n",
    "    \n",
    "    print(\"ğŸ“‹ Configuration du test:\")\n",
    "    for key, value in test_config.items():\n",
    "        print(f\"   {key}: {value}\")\n",
    "    \n",
    "    try:\n",
    "        # Test du workflow avec gestion d'erreurs renforcÃ©e\n",
    "        print(f\"\\n[{datetime.now().strftime('%H:%M:%S')}] ğŸ”„ Lancement du workflow eBay\")\n",
    "        \n",
    "        result = robust_reviews_workflow(\n",
    "            category=test_config[\"category\"],\n",
    "            site=test_config[\"site\"],\n",
    "            max_products=test_config[\"max_products\"],\n",
    "            reviews_per_rating=test_config[\"reviews_per_rating\"],\n",
    "            headless=test_config[\"headless\"]\n",
    "        )\n",
    "        \n",
    "        if result is not None and len(result) > 0:\n",
    "            print(f\"âœ… Test eBay terminÃ© avec succÃ¨s!\")\n",
    "            print(f\"ğŸ“Š RÃ©sultats: {len(result)} reviews rÃ©cupÃ©rÃ©es\")\n",
    "            \n",
    "            # Analyse des donnÃ©es eBay\n",
    "            print(f\"\\nğŸ“ˆ Analyse des rÃ©sultats eBay:\")\n",
    "            if 'rating' in result.columns:\n",
    "                print(f\"â­ Rating moyen: {result['rating'].mean():.2f}\")\n",
    "                print(f\"ğŸ“Š Distribution des ratings:\")\n",
    "                rating_dist = result['rating'].value_counts().sort_index()\n",
    "                for rating, count in rating_dist.items():\n",
    "                    print(f\"   {rating} Ã©toiles: {count} reviews\")\n",
    "            \n",
    "            if 'review_text' in result.columns:\n",
    "                avg_length = result['review_text'].str.len().mean()\n",
    "                print(f\"ğŸ“ Longueur moyenne des reviews: {avg_length:.0f} caractÃ¨res\")\n",
    "            \n",
    "            return result\n",
    "        else:\n",
    "            print(\"âŒ Aucune donnÃ©e rÃ©cupÃ©rÃ©e sur eBay\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Erreur durant le test eBay: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# âš ï¸ DÃ‰COMMENTEZ POUR EXÃ‰CUTER LE TEST\n",
    "# df_test_ebay = test_ebay_electronics_robust()\n",
    "\n",
    "print(\"âœ… Fonction de test eBay crÃ©Ã©e. DÃ©commentez la derniÃ¨re ligne pour l'exÃ©cuter.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e448e39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ›¡ï¸ Test de robustesse du systÃ¨me\n",
      "==================================================\n",
      "[15:17:16] ğŸ›¡ï¸ DÃ©marrage du test de robustesse\n",
      "\n",
      "[15:17:16] ğŸ§ª Test 1: Amazon - CatÃ©gorie obscure\n",
      "   ğŸ” Test du scout...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-29 15:17:19,421 - INFO - patching driver executable C:\\Users\\Yann\\appdata\\roaming\\undetected_chromedriver\\undetected_chromedriver.exe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:17:20] DRIVER_CREATION_SUCCESS: SUCCESS - Driver undetected_chrome crÃ©Ã©\n",
      "[15:17:20] DRIVER_INITIALIZED: SUCCESS - Driver undetected_chrome initialisÃ©\n",
      "[15:17:24] DRIVER_TEST_SUCCESS: SUCCESS - Driver undetected_chrome fonctionnel\n",
      "[15:17:30] SELECTOR_FOUND: SUCCESS - a[href*=\"/dp/\"]: 197 Ã©lÃ©ments\n",
      "[15:17:30] SELECTOR_FOUND: SUCCESS - .s-result-item a: 311 Ã©lÃ©ments\n",
      "[15:17:30] SELECTOR_FOUND: SUCCESS - [data-component-type=\"s-search-result\"] a: 283 Ã©lÃ©ments\n",
      "[15:17:30] PAGE_ANALYSIS_SUCCESS: SUCCESS - Page 1 analysÃ©e\n",
      "[15:17:30] FALLBACK_TO_BASE_SELECTORS: WARNING - Utilisation des sÃ©lecteurs de base\n",
      "[15:17:30] DETECTION_COMPLETE: SUCCESS - 5 sÃ©lecteurs dÃ©tectÃ©s\n",
      "[15:17:30] SELECTORS_SAVE_ERROR: ERROR - name 'shutil' is not defined\n",
      "[15:17:30] SCOUT_SUCCESS: SUCCESS - Scout amazon terminÃ© avec succÃ¨s\n",
      "   âœ… Scout: SÃ©lecteurs dÃ©tectÃ©s\n",
      "[15:17:30] SESSION_LOG_SAVED: SUCCESS - ultra_scout_session_20250629_151730.json\n",
      "[15:17:30] SCOUT_CLOSED: SUCCESS - Scout ultra-sÃ©curisÃ© fermÃ© proprement\n",
      "\n",
      "[15:17:32] ğŸ§ª Test 2: eBay - Recherche vide\n",
      "   ğŸ” Test du scout...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-29 15:17:35,186 - INFO - patching driver executable C:\\Users\\Yann\\appdata\\roaming\\undetected_chromedriver\\undetected_chromedriver.exe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:17:35] DRIVER_CREATION_SUCCESS: SUCCESS - Driver undetected_chrome crÃ©Ã©\n",
      "[15:17:35] DRIVER_INITIALIZED: SUCCESS - Driver undetected_chrome initialisÃ©\n",
      "[15:17:39] DRIVER_TEST_SUCCESS: SUCCESS - Driver undetected_chrome fonctionnel\n",
      "[15:17:47] PAGE_ANALYSIS_SUCCESS: SUCCESS - Page 1 analysÃ©e\n",
      "[15:17:47] BEST_SELECTOR_FOUND: SUCCESS - product_link: h1[class*=\"title\"]\n",
      "[15:17:47] BEST_SELECTOR_FOUND: SUCCESS - product_title: h1[class*=\"title\"]\n",
      "[15:17:47] BEST_SELECTOR_FOUND: SUCCESS - rating: h1[class*=\"title\"]\n",
      "[15:17:47] BEST_SELECTOR_FOUND: SUCCESS - review_count: h1[class*=\"title\"]\n",
      "[15:17:47] BEST_SELECTOR_FOUND: SUCCESS - price: h1[class*=\"title\"]\n",
      "[15:17:47] DETECTION_COMPLETE: SUCCESS - 5 sÃ©lecteurs dÃ©tectÃ©s\n",
      "[15:17:47] SELECTORS_SAVED: SUCCESS - detected_selectors_ebay.json\n",
      "[15:17:47] SCOUT_SUCCESS: SUCCESS - Scout ebay terminÃ© avec succÃ¨s\n",
      "   âœ… Scout: SÃ©lecteurs dÃ©tectÃ©s\n",
      "[15:17:47] SESSION_LOG_SAVED: SUCCESS - ultra_scout_session_20250629_151747.json\n",
      "[15:17:47] SCOUT_CLOSED: SUCCESS - Scout ultra-sÃ©curisÃ© fermÃ© proprement\n",
      "\n",
      "[15:17:49] ğŸ“Š RÃ‰SUMÃ‰ DES TESTS DE ROBUSTESSE:\n",
      "============================================================\n",
      "âœ… Amazon - CatÃ©gorie obscure: scout_success\n",
      "âœ… eBay - Recherche vide: scout_success\n",
      "\n",
      "[15:17:49] ğŸ”§ Test des fallbacks de drivers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-29 15:17:51,252 - INFO - patching driver executable C:\\Users\\Yann\\appdata\\roaming\\undetected_chromedriver\\undetected_chromedriver.exe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:17:52] DRIVER_CREATION_SUCCESS: SUCCESS - Driver undetected_chrome crÃ©Ã©\n",
      "[15:17:52] DRIVER_INITIALIZED: SUCCESS - Driver undetected_chrome initialisÃ©\n",
      "[15:17:56] DRIVER_TEST_SUCCESS: SUCCESS - Driver undetected_chrome fonctionnel\n",
      "âœ… Au moins un driver fonctionne\n",
      "   Driver utilisÃ©: Chrome\n",
      "[15:17:56] SESSION_LOG_SAVED: SUCCESS - ultra_scout_session_20250629_151756.json\n",
      "[15:17:56] SCOUT_CLOSED: SUCCESS - Scout ultra-sÃ©curisÃ© fermÃ© proprement\n",
      "âœ… Fonction de test de robustesse crÃ©Ã©e. DÃ©commentez la derniÃ¨re ligne pour l'exÃ©cuter.\n"
     ]
    }
   ],
   "source": [
    "# ğŸ§ª EXEMPLE 3: TEST DE ROBUSTESSE ET GESTION D'ERREURS\n",
    "print(\"ğŸ›¡ï¸ Test de robustesse du systÃ¨me\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "def test_system_robustness():\n",
    "    \"\"\"\n",
    "    Test de robustesse du systÃ¨me complet\n",
    "    Validation des fallbacks et de la gestion d'erreurs\n",
    "    \"\"\"\n",
    "    print(f\"[{datetime.now().strftime('%H:%M:%S')}] ğŸ›¡ï¸ DÃ©marrage du test de robustesse\")\n",
    "    \n",
    "    # Tests avec diffÃ©rents scÃ©narios difficiles\n",
    "    test_scenarios = [\n",
    "        {\n",
    "            \"name\": \"Amazon - CatÃ©gorie obscure\",\n",
    "            \"site\": \"amazon\",\n",
    "            \"category\": \"xyz rare product\",\n",
    "            \"max_products\": 3,\n",
    "            \"reviews_per_rating\": 10,\n",
    "            \"expected_result\": \"fallback_selectors\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"eBay - Recherche vide\",\n",
    "            \"site\": \"ebay\",\n",
    "            \"category\": \"nonexistent product xyz\",\n",
    "            \"max_products\": 2,\n",
    "            \"reviews_per_rating\": 5,\n",
    "            \"expected_result\": \"no_products\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for i, scenario in enumerate(test_scenarios):\n",
    "        print(f\"\\n[{datetime.now().strftime('%H:%M:%S')}] ğŸ§ª Test {i+1}: {scenario['name']}\")\n",
    "        \n",
    "        try:\n",
    "            # Test du scout ultra-sÃ©curisÃ©\n",
    "            print(\"   ğŸ” Test du scout...\")\n",
    "            scout = UltraSecureProductReviewScout(headless=True)\n",
    "            \n",
    "            # URLs de test (peuvent Ã©chouer volontairement)\n",
    "            sample_urls = [\n",
    "                f\"https://www.{scenario['site']}.com/s?k={scenario['category'].replace(' ', '+')}\"\n",
    "            ]\n",
    "            \n",
    "            detected_selectors = scout.scout_site(scenario['site'], sample_urls, max_products=2)\n",
    "            \n",
    "            if detected_selectors:\n",
    "                print(\"   âœ… Scout: SÃ©lecteurs dÃ©tectÃ©s\")\n",
    "                results[scenario['name']] = \"scout_success\"\n",
    "            else:\n",
    "                print(\"   âš ï¸ Scout: Fallback utilisÃ©\")\n",
    "                results[scenario['name']] = \"scout_fallback\"\n",
    "            \n",
    "            scout.close()\n",
    "            \n",
    "            # Petit dÃ©lai entre les tests\n",
    "            time.sleep(2)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ Scout: Erreur - {str(e)[:100]}\")\n",
    "            results[scenario['name']] = f\"scout_error: {str(e)[:50]}\"\n",
    "    \n",
    "    # RÃ©sumÃ© des tests de robustesse\n",
    "    print(f\"\\n[{datetime.now().strftime('%H:%M:%S')}] ğŸ“Š RÃ‰SUMÃ‰ DES TESTS DE ROBUSTESSE:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for scenario_name, result in results.items():\n",
    "        status = \"âœ…\" if \"success\" in result or \"fallback\" in result else \"âŒ\"\n",
    "        print(f\"{status} {scenario_name}: {result}\")\n",
    "    \n",
    "    # Test des drivers en fallback\n",
    "    print(f\"\\n[{datetime.now().strftime('%H:%M:%S')}] ğŸ”§ Test des fallbacks de drivers:\")\n",
    "    \n",
    "    scout = UltraSecureProductReviewScout(headless=True)\n",
    "    driver_success = scout.initialize_driver()\n",
    "    \n",
    "    if driver_success:\n",
    "        print(\"âœ… Au moins un driver fonctionne\")\n",
    "        print(f\"   Driver utilisÃ©: {type(scout.driver).__name__}\")\n",
    "    else:\n",
    "        print(\"âŒ Aucun driver disponible\")\n",
    "    \n",
    "    scout.close()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# âš ï¸ DÃ‰COMMENTEZ POUR EXÃ‰CUTER LE TEST DE ROBUSTESSE\n",
    "test_results = test_system_robustness()\n",
    "\n",
    "print(\"âœ… Fonction de test de robustesse crÃ©Ã©e. DÃ©commentez la derniÃ¨re ligne pour l'exÃ©cuter.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a04779",
   "metadata": {},
   "source": [
    "# run on extra robust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "661717ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ Menu Interactif du SystÃ¨me Ultra-SÃ©curisÃ©\n",
      "============================================================\n",
      "\n",
      "[15:18:11] ğŸ® MENU PRINCIPAL - SYSTÃˆME ULTRA-SÃ‰CURISÃ‰\n",
      "============================================================\n",
      "1ï¸âƒ£ Workflow complet (scout + scraper)\n",
      "2ï¸âƒ£ Test scout ultra-sÃ©curisÃ© uniquement\n",
      "3ï¸âƒ£ Exemples de test prÃªts Ã  l'emploi\n",
      "4ï¸âƒ£ Test de robustesse du systÃ¨me\n",
      "5ï¸âƒ£ Configuration avancÃ©e\n",
      "6ï¸âƒ£ Voir les logs et sauvegardes\n",
      "7ï¸âƒ£ Documentation et aide\n",
      "0ï¸âƒ£ Quitter\n",
      "------------------------------------------------------------\n",
      "\n",
      "ğŸ“‹ CONFIGURATION DU WORKFLOW COMPLET\n",
      "----------------------------------------\n",
      "\n",
      "ğŸš€ Lancement du workflow...\n",
      "   Site: amazon\n",
      "   CatÃ©gorie: laptop\n",
      "   Max produits: 3\n",
      "   Reviews par rating: 15\n",
      "   Mode headless: True\n",
      "====================================================================================================\n",
      "ğŸš€ WORKFLOW ROBUSTE - SCRAPING REVIEWS DE PRODUITS\n",
      "====================================================================================================\n",
      "ğŸ“¦ CatÃ©gorie: laptop\n",
      "ğŸŒ Site: amazon\n",
      "ğŸ“Š Produits max: 3\n",
      "â­ Reviews par note: 15\n",
      "ğŸ‘ï¸ Mode: Headless\n",
      "ğŸ“ˆ Total estimÃ©: 225 reviews max\n",
      "\n",
      "ğŸ” PHASE 1: DÃ‰TECTION AUTOMATIQUE DES BALISES\n",
      "----------------------------------------------------------------------\n",
      "ğŸ”§ Tentative 1/3 - Setup driver scout...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-29 15:18:36,070 - INFO - patching driver executable C:\\Users\\Yann\\appdata\\roaming\\undetected_chromedriver\\undetected_chromedriver.exe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Driver scout initialisÃ© avec succÃ¨s!\n",
      "ğŸ” DÃ©tection des sÃ©lecteurs pour amazon...\n",
      "ğŸŒ Navigation vers: https://www.amazon.com/s?k=laptop\n",
      "âœ… Conteneur: [data-component-type=\"s-search-result\"] (21 Ã©lÃ©ments)\n",
      "âœ… Titre: h2 span\n",
      "âœ… URL: .a-link-normal\n",
      "âœ… Rating: .a-icon-alt\n",
      "âœ… SÃ©lecteurs produits dÃ©tectÃ©s: 4\n",
      "ğŸ”— Test reviews sur: https://www.amazon.com/HP-Micro-edge-Microsoft-14-dq0040nr-Snowflake/dp/B0947BJ6...\n",
      "ğŸ”— Navigation vers page reviews: https://www.amazon.com/HP-Micro-edge-Microsoft-14-dq0040nr-Snowflake/dp/B0947BJ6...\n",
      "âœ… Conteneur reviews: [data-hook=\"review\"] (13 reviews)\n",
      "âœ… Texte review: [data-hook=\"review-body\"] span\n",
      "âœ… Titre review: .review-title\n",
      "âœ… Rating review: [data-hook=\"review-star-rating\"] .a-icon-alt\n",
      "âœ… Auteur review: .a-profile-name\n",
      "âœ… Date review: [data-hook=\"review-date\"]\n",
      "âœ… SÃ©lecteurs reviews dÃ©tectÃ©s: 6\n",
      "âœ… SÃ©lecteurs sauvegardÃ©s: ../config/detected_selectors_amazon.json\n",
      "âœ… SÃ©lecteurs dÃ©tectÃ©s avec succÃ¨s!\n",
      "ğŸ“¦ Produits: ['container', 'title', 'url', 'rating']\n",
      "ğŸ“ Reviews: ['container', 'text', 'title', 'rating', 'author', 'date']\n",
      "âœ… Driver scout fermÃ©\n",
      "\n",
      "======================================================================\n",
      "ğŸ“Š PHASE 2: SCRAPING DES REVIEWS\n",
      "----------------------------------------------------------------------\n",
      "ğŸš€ Tentative 1/3 - Setup driver scraper...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-29 15:19:20,825 - INFO - patching driver executable C:\\Users\\Yann\\appdata\\roaming\\undetected_chromedriver\\undetected_chromedriver.exe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Driver scraper prÃªt!\n",
      "ğŸ­ User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, lik...\n",
      "\n",
      "ğŸ¯ DÃ©but du scraping pour 'laptop' sur amazon...\n",
      "================================================================================\n",
      "ğŸ¯ DÃ‰BUT DU SCRAPING ROBUSTE\n",
      "================================================================================\n",
      "ğŸ“¦ CatÃ©gorie: laptop\n",
      "ğŸŒ Site: amazon\n",
      "ğŸ“Š Produits max: 3\n",
      "â­ Reviews par note: 15\n",
      "\n",
      "ğŸ” Recherche: https://www.amazon.com/s?k=laptop\n",
      "ğŸ“¦ 0 conteneurs trouvÃ©s\n",
      "âŒ Aucun produit trouvÃ©\n",
      "âŒ Aucune review rÃ©cupÃ©rÃ©e\n",
      "âœ… Driver scraper fermÃ©\n",
      "âŒ Aucune donnÃ©e rÃ©cupÃ©rÃ©e\n",
      "\n",
      "[15:19:43] ğŸ® MENU PRINCIPAL - SYSTÃˆME ULTRA-SÃ‰CURISÃ‰\n",
      "============================================================\n",
      "1ï¸âƒ£ Workflow complet (scout + scraper)\n",
      "2ï¸âƒ£ Test scout ultra-sÃ©curisÃ© uniquement\n",
      "3ï¸âƒ£ Exemples de test prÃªts Ã  l'emploi\n",
      "4ï¸âƒ£ Test de robustesse du systÃ¨me\n",
      "5ï¸âƒ£ Configuration avancÃ©e\n",
      "6ï¸âƒ£ Voir les logs et sauvegardes\n",
      "7ï¸âƒ£ Documentation et aide\n",
      "0ï¸âƒ£ Quitter\n",
      "------------------------------------------------------------\n",
      "ğŸ“¦ 0 conteneurs trouvÃ©s\n",
      "âŒ Aucun produit trouvÃ©\n",
      "âŒ Aucune review rÃ©cupÃ©rÃ©e\n",
      "âœ… Driver scraper fermÃ©\n",
      "âŒ Aucune donnÃ©e rÃ©cupÃ©rÃ©e\n",
      "\n",
      "[15:19:43] ğŸ® MENU PRINCIPAL - SYSTÃˆME ULTRA-SÃ‰CURISÃ‰\n",
      "============================================================\n",
      "1ï¸âƒ£ Workflow complet (scout + scraper)\n",
      "2ï¸âƒ£ Test scout ultra-sÃ©curisÃ© uniquement\n",
      "3ï¸âƒ£ Exemples de test prÃªts Ã  l'emploi\n",
      "4ï¸âƒ£ Test de robustesse du systÃ¨me\n",
      "5ï¸âƒ£ Configuration avancÃ©e\n",
      "6ï¸âƒ£ Voir les logs et sauvegardes\n",
      "7ï¸âƒ£ Documentation et aide\n",
      "0ï¸âƒ£ Quitter\n",
      "------------------------------------------------------------\n",
      "âŒ Choix invalide. Veuillez choisir entre 0 et 7.\n",
      "\n",
      "[15:19:51] ğŸ® MENU PRINCIPAL - SYSTÃˆME ULTRA-SÃ‰CURISÃ‰\n",
      "============================================================\n",
      "1ï¸âƒ£ Workflow complet (scout + scraper)\n",
      "2ï¸âƒ£ Test scout ultra-sÃ©curisÃ© uniquement\n",
      "3ï¸âƒ£ Exemples de test prÃªts Ã  l'emploi\n",
      "4ï¸âƒ£ Test de robustesse du systÃ¨me\n",
      "5ï¸âƒ£ Configuration avancÃ©e\n",
      "6ï¸âƒ£ Voir les logs et sauvegardes\n",
      "7ï¸âƒ£ Documentation et aide\n",
      "0ï¸âƒ£ Quitter\n",
      "------------------------------------------------------------\n",
      "âŒ Choix invalide. Veuillez choisir entre 0 et 7.\n",
      "\n",
      "[15:19:51] ğŸ® MENU PRINCIPAL - SYSTÃˆME ULTRA-SÃ‰CURISÃ‰\n",
      "============================================================\n",
      "1ï¸âƒ£ Workflow complet (scout + scraper)\n",
      "2ï¸âƒ£ Test scout ultra-sÃ©curisÃ© uniquement\n",
      "3ï¸âƒ£ Exemples de test prÃªts Ã  l'emploi\n",
      "4ï¸âƒ£ Test de robustesse du systÃ¨me\n",
      "5ï¸âƒ£ Configuration avancÃ©e\n",
      "6ï¸âƒ£ Voir les logs et sauvegardes\n",
      "7ï¸âƒ£ Documentation et aide\n",
      "0ï¸âƒ£ Quitter\n",
      "------------------------------------------------------------\n",
      "âŒ Choix invalide. Veuillez choisir entre 0 et 7.\n",
      "\n",
      "[15:19:53] ğŸ® MENU PRINCIPAL - SYSTÃˆME ULTRA-SÃ‰CURISÃ‰\n",
      "============================================================\n",
      "1ï¸âƒ£ Workflow complet (scout + scraper)\n",
      "2ï¸âƒ£ Test scout ultra-sÃ©curisÃ© uniquement\n",
      "3ï¸âƒ£ Exemples de test prÃªts Ã  l'emploi\n",
      "4ï¸âƒ£ Test de robustesse du systÃ¨me\n",
      "5ï¸âƒ£ Configuration avancÃ©e\n",
      "6ï¸âƒ£ Voir les logs et sauvegardes\n",
      "7ï¸âƒ£ Documentation et aide\n",
      "0ï¸âƒ£ Quitter\n",
      "------------------------------------------------------------\n",
      "âŒ Choix invalide. Veuillez choisir entre 0 et 7.\n",
      "\n",
      "[15:19:53] ğŸ® MENU PRINCIPAL - SYSTÃˆME ULTRA-SÃ‰CURISÃ‰\n",
      "============================================================\n",
      "1ï¸âƒ£ Workflow complet (scout + scraper)\n",
      "2ï¸âƒ£ Test scout ultra-sÃ©curisÃ© uniquement\n",
      "3ï¸âƒ£ Exemples de test prÃªts Ã  l'emploi\n",
      "4ï¸âƒ£ Test de robustesse du systÃ¨me\n",
      "5ï¸âƒ£ Configuration avancÃ©e\n",
      "6ï¸âƒ£ Voir les logs et sauvegardes\n",
      "7ï¸âƒ£ Documentation et aide\n",
      "0ï¸âƒ£ Quitter\n",
      "------------------------------------------------------------\n",
      "âŒ Choix invalide. Veuillez choisir entre 0 et 7.\n",
      "\n",
      "[15:19:56] ğŸ® MENU PRINCIPAL - SYSTÃˆME ULTRA-SÃ‰CURISÃ‰\n",
      "============================================================\n",
      "1ï¸âƒ£ Workflow complet (scout + scraper)\n",
      "2ï¸âƒ£ Test scout ultra-sÃ©curisÃ© uniquement\n",
      "3ï¸âƒ£ Exemples de test prÃªts Ã  l'emploi\n",
      "4ï¸âƒ£ Test de robustesse du systÃ¨me\n",
      "5ï¸âƒ£ Configuration avancÃ©e\n",
      "6ï¸âƒ£ Voir les logs et sauvegardes\n",
      "7ï¸âƒ£ Documentation et aide\n",
      "0ï¸âƒ£ Quitter\n",
      "------------------------------------------------------------\n",
      "ğŸ‘‹ Au revoir!\n",
      "âœ… Menu interactif amÃ©liorÃ© crÃ©Ã©. DÃ©commentez la derniÃ¨re ligne pour le lancer.\n",
      "âŒ Choix invalide. Veuillez choisir entre 0 et 7.\n",
      "\n",
      "[15:19:56] ğŸ® MENU PRINCIPAL - SYSTÃˆME ULTRA-SÃ‰CURISÃ‰\n",
      "============================================================\n",
      "1ï¸âƒ£ Workflow complet (scout + scraper)\n",
      "2ï¸âƒ£ Test scout ultra-sÃ©curisÃ© uniquement\n",
      "3ï¸âƒ£ Exemples de test prÃªts Ã  l'emploi\n",
      "4ï¸âƒ£ Test de robustesse du systÃ¨me\n",
      "5ï¸âƒ£ Configuration avancÃ©e\n",
      "6ï¸âƒ£ Voir les logs et sauvegardes\n",
      "7ï¸âƒ£ Documentation et aide\n",
      "0ï¸âƒ£ Quitter\n",
      "------------------------------------------------------------\n",
      "ğŸ‘‹ Au revoir!\n",
      "âœ… Menu interactif amÃ©liorÃ© crÃ©Ã©. DÃ©commentez la derniÃ¨re ligne pour le lancer.\n"
     ]
    }
   ],
   "source": [
    "# ğŸ¯ MENU INTERACTIF AMÃ‰LIORÃ‰ - VERSION ULTRA-SÃ‰CURISÃ‰E\n",
    "print(\"ğŸ¯ Menu Interactif du SystÃ¨me Ultra-SÃ©curisÃ©\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def enhanced_interactive_menu():\n",
    "    \"\"\"\n",
    "    Menu interactif amÃ©liorÃ© avec tous les outils disponibles\n",
    "    Version 2025-01-20 avec scout ultra-sÃ©curisÃ©\n",
    "    \"\"\"\n",
    "    \n",
    "    while True:\n",
    "        print(f\"\\n[{datetime.now().strftime('%H:%M:%S')}] ğŸ® MENU PRINCIPAL - SYSTÃˆME ULTRA-SÃ‰CURISÃ‰\")\n",
    "        print(\"=\" * 60)\n",
    "        print(\"1ï¸âƒ£ Workflow complet (scout + scraper)\")\n",
    "        print(\"2ï¸âƒ£ Test scout ultra-sÃ©curisÃ© uniquement\")\n",
    "        print(\"3ï¸âƒ£ Exemples de test prÃªts Ã  l'emploi\")\n",
    "        print(\"4ï¸âƒ£ Test de robustesse du systÃ¨me\")\n",
    "        print(\"5ï¸âƒ£ Configuration avancÃ©e\")\n",
    "        print(\"6ï¸âƒ£ Voir les logs et sauvegardes\")\n",
    "        print(\"7ï¸âƒ£ Documentation et aide\")\n",
    "        print(\"0ï¸âƒ£ Quitter\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        try:\n",
    "            choice = input(\"ğŸ”¹ Votre choix (0-7): \").strip()\n",
    "            \n",
    "            if choice == \"0\":\n",
    "                print(\"ğŸ‘‹ Au revoir!\")\n",
    "                break\n",
    "                \n",
    "            elif choice == \"1\":\n",
    "                print(\"\\nğŸ“‹ CONFIGURATION DU WORKFLOW COMPLET\")\n",
    "                print(\"-\" * 40)\n",
    "                \n",
    "                # Configuration interactive\n",
    "                site = input(\"ğŸŒ Site (amazon/ebay) [amazon]: \").strip() or \"amazon\"\n",
    "                category = input(\"ğŸ“¦ CatÃ©gorie/recherche [laptop]: \").strip() or \"laptop\"\n",
    "                max_products = int(input(\"ğŸ”¢ Max produits [5]: \").strip() or \"5\")\n",
    "                reviews_per_rating = int(input(\"â­ Reviews par rating [20]: \").strip() or \"20\")\n",
    "                headless = input(\"ğŸ‘ï¸ Mode headless? (y/n) [y]: \").strip().lower() != \"n\"\n",
    "                \n",
    "                print(f\"\\nğŸš€ Lancement du workflow...\")\n",
    "                print(f\"   Site: {site}\")\n",
    "                print(f\"   CatÃ©gorie: {category}\")\n",
    "                print(f\"   Max produits: {max_products}\")\n",
    "                print(f\"   Reviews par rating: {reviews_per_rating}\")\n",
    "                print(f\"   Mode headless: {headless}\")\n",
    "                \n",
    "                result = robust_reviews_workflow(category, site, max_products, reviews_per_rating, headless)\n",
    "                \n",
    "                if result is not None and len(result) > 0:\n",
    "                    print(f\"âœ… Workflow terminÃ©! {len(result)} reviews rÃ©cupÃ©rÃ©es\")\n",
    "                    \n",
    "                    # Option de sauvegarde\n",
    "                    save_choice = input(\"\\nğŸ’¾ Sauvegarder dans une variable? (y/n): \").strip().lower()\n",
    "                    if save_choice == \"y\":\n",
    "                        var_name = input(\"ğŸ“ Nom de la variable [df_results]: \").strip() or \"df_results\"\n",
    "                        globals()[var_name] = result\n",
    "                        print(f\"âœ… DonnÃ©es sauvegardÃ©es dans '{var_name}'\")\n",
    "                else:\n",
    "                    print(\"âŒ Aucune donnÃ©e rÃ©cupÃ©rÃ©e\")\n",
    "                    \n",
    "            elif choice == \"2\":\n",
    "                print(\"\\nğŸ” TEST SCOUT ULTRA-SÃ‰CURISÃ‰\")\n",
    "                print(\"-\" * 40)\n",
    "                \n",
    "                site = input(\"ğŸŒ Site Ã  analyser (amazon/ebay) [amazon]: \").strip() or \"amazon\"\n",
    "                search_term = input(\"ğŸ” Terme de recherche [laptop]: \").strip() or \"laptop\"\n",
    "                headless = input(\"ğŸ‘ï¸ Mode headless? (y/n) [y]: \").strip().lower() != \"n\"\n",
    "                \n",
    "                scout = UltraSecureProductReviewScout(headless=headless)\n",
    "                \n",
    "                sample_urls = [\n",
    "                    f\"https://www.{site}.com/s?k={search_term.replace(' ', '+')}\"\n",
    "                ]\n",
    "                \n",
    "                print(f\"ğŸ” Analyse de {site} pour '{search_term}'...\")\n",
    "                selectors = scout.scout_site(site, sample_urls)\n",
    "                \n",
    "                if selectors:\n",
    "                    print(\"âœ… SÃ©lecteurs dÃ©tectÃ©s:\")\n",
    "                    for elem_type, selector in selectors.items():\n",
    "                        print(f\"   {elem_type}: {selector}\")\n",
    "                else:\n",
    "                    print(\"âŒ Aucun sÃ©lecteur dÃ©tectÃ©\")\n",
    "                \n",
    "                scout.close()\n",
    "                \n",
    "            elif choice == \"3\":\n",
    "                print(\"\\nğŸ§ª EXEMPLES DE TEST PRÃŠTS Ã€ L'EMPLOI\")\n",
    "                print(\"-\" * 40)\n",
    "                print(\"1. Test Amazon Gaming Laptops\")\n",
    "                print(\"2. Test eBay Ã‰lectronique\")\n",
    "                print(\"3. Retour au menu principal\")\n",
    "                \n",
    "                test_choice = input(\"ğŸ”¹ Votre choix (1-3): \").strip()\n",
    "                \n",
    "                if test_choice == \"1\":\n",
    "                    print(\"ğŸ® Lancement du test Amazon Gaming Laptops...\")\n",
    "                    result = test_amazon_gaming_laptops_ultra_secure()\n",
    "                    if result is not None:\n",
    "                        globals()['df_test_amazon'] = result\n",
    "                        print(\"âœ… RÃ©sultats sauvegardÃ©s dans 'df_test_amazon'\")\n",
    "                        \n",
    "                elif test_choice == \"2\":\n",
    "                    print(\"ğŸ”Œ Lancement du test eBay Ã‰lectronique...\")\n",
    "                    result = test_ebay_electronics_robust()\n",
    "                    if result is not None:\n",
    "                        globals()['df_test_ebay'] = result\n",
    "                        print(\"âœ… RÃ©sultats sauvegardÃ©s dans 'df_test_ebay'\")\n",
    "                        \n",
    "            elif choice == \"4\":\n",
    "                print(\"\\nğŸ›¡ï¸ TEST DE ROBUSTESSE DU SYSTÃˆME\")\n",
    "                print(\"-\" * 40)\n",
    "                print(\"Ce test valide les fallbacks et la gestion d'erreurs...\")\n",
    "                \n",
    "                confirm = input(\"ğŸ”¹ Continuer? (y/n): \").strip().lower()\n",
    "                if confirm == \"y\":\n",
    "                    test_results = test_system_robustness()\n",
    "                    globals()['robustness_results'] = test_results\n",
    "                    print(\"âœ… RÃ©sultats sauvegardÃ©s dans 'robustness_results'\")\n",
    "                    \n",
    "            elif choice == \"5\":\n",
    "                print(\"\\nâš™ï¸ CONFIGURATION AVANCÃ‰E\")\n",
    "                print(\"-\" * 40)\n",
    "                print(\"ğŸ“‹ Configurations disponibles:\")\n",
    "                print(f\"   Proxies configurÃ©s: {len(PROXY_LIST)}\")\n",
    "                print(f\"   User agents disponibles: {len(REALISTIC_USER_AGENTS)}\")\n",
    "                print(\"   Exemples prÃ©dÃ©finis: Amazon, eBay, Trustpilot\")\n",
    "                print(\"\\nğŸ”§ Pour modifier les configurations, Ã©ditez les variables:\")\n",
    "                print(\"   - PROXY_LIST (pour les proxies)\")\n",
    "                print(\"   - REALISTIC_USER_AGENTS (pour les user agents)\")\n",
    "                print(\"   - ROBUST_EXAMPLES (pour les exemples)\")\n",
    "                \n",
    "            elif choice == \"6\":\n",
    "                print(\"\\nğŸ“ LOGS ET SAUVEGARDES\")\n",
    "                print(\"-\" * 40)\n",
    "                \n",
    "                # VÃ©rifier les fichiers de sauvegarde\n",
    "                config_dir = \"../config\"\n",
    "                logs_dir = \"../logs\"\n",
    "                data_dir = \"../data/raw\"\n",
    "                \n",
    "                for directory, desc in [(config_dir, \"SÃ©lecteurs dÃ©tectÃ©s\"), \n",
    "                                      (logs_dir, \"Logs de session\"), \n",
    "                                      (data_dir, \"DonnÃ©es scrapÃ©es\")]:\n",
    "                    if os.path.exists(directory):\n",
    "                        files = [f for f in os.listdir(directory) if f.endswith(('.json', '.csv'))]\n",
    "                        print(f\"\\nğŸ“‚ {desc} ({directory}):\")\n",
    "                        if files:\n",
    "                            for file in sorted(files)[-5:]:  # 5 derniers fichiers\n",
    "                                print(f\"   - {file}\")\n",
    "                            if len(files) > 5:\n",
    "                                print(f\"   ... et {len(files)-5} autres fichiers\")\n",
    "                        else:\n",
    "                            print(\"   (aucun fichier)\")\n",
    "                    else:\n",
    "                        print(f\"\\nğŸ“‚ {desc}: Dossier non crÃ©Ã©\")\n",
    "                        \n",
    "            elif choice == \"7\":\n",
    "                print(\"\\nğŸ“– DOCUMENTATION ET AIDE\")\n",
    "                print(\"-\" * 40)\n",
    "                print(\"ğŸ”— Fonctions principales disponibles:\")\n",
    "                print(\"   â€¢ robust_reviews_workflow() - Workflow complet\")\n",
    "                print(\"   â€¢ UltraSecureProductReviewScout - Scout ultra-sÃ©curisÃ©\")\n",
    "                print(\"   â€¢ RobustProductReviewScraper - Scraper robuste\")\n",
    "                print(\"\\nğŸ“‹ Variables globales importantes:\")\n",
    "                print(\"   â€¢ ROBUST_EXAMPLES - Exemples prÃ©dÃ©finis\")\n",
    "                print(\"   â€¢ PROXY_LIST - Liste des proxies\")\n",
    "                print(\"   â€¢ REALISTIC_USER_AGENTS - User agents\")\n",
    "                print(\"\\nğŸ§ª Fonctions de test:\")\n",
    "                print(\"   â€¢ test_amazon_gaming_laptops_ultra_secure()\")\n",
    "                print(\"   â€¢ test_ebay_electronics_robust()\")\n",
    "                print(\"   â€¢ test_system_robustness()\")\n",
    "                print(\"\\nğŸ’¡ Conseils:\")\n",
    "                print(\"   - Utilisez le mode non-headless pour debugging\")\n",
    "                print(\"   - Les sÃ©lecteurs sont sauvegardÃ©s automatiquement\")\n",
    "                print(\"   - Les logs sont horodatÃ©s pour le suivi\")\n",
    "                \n",
    "            else:\n",
    "                print(\"âŒ Choix invalide. Veuillez choisir entre 0 et 7.\")\n",
    "                \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n\\nğŸ›‘ Interruption utilisateur. Au revoir!\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Erreur: {str(e)}\")\n",
    "            print(\"ğŸ”„ Retour au menu principal...\")\n",
    "\n",
    "# âš ï¸ DÃ‰COMMENTEZ POUR LANCER LE MENU INTERACTIF\n",
    "enhanced_interactive_menu()\n",
    "\n",
    "print(\"âœ… Menu interactif amÃ©liorÃ© crÃ©Ã©. DÃ©commentez la derniÃ¨re ligne pour le lancer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61eece5b",
   "metadata": {},
   "source": [
    "# ğŸ¯ RÃ©sumÃ© Final - SystÃ¨me Ultra-SÃ©curisÃ© PrÃªt (2025-01-20)\n",
    "\n",
    "## âœ… **SYSTÃˆME COMPLET IMPLÃ‰MENTÃ‰**\n",
    "\n",
    "### ğŸ”’ **Scout Ultra-SÃ©curisÃ©**\n",
    "- **UltraSecureProductReviewScout** : DÃ©tection automatique avec fallbacks multiples\n",
    "- **Drivers** : undetected_chromedriver â†’ Selenium Chrome â†’ Firefox â†’ Edge\n",
    "- **Anti-dÃ©tection** : Options Chrome optimisÃ©es, rotation user-agents, scripts anti-dÃ©tection\n",
    "- **Logs** : Horodatage complet, sauvegarde automatique des sessions\n",
    "- **SÃ©lecteurs** : DÃ©tection adaptative avec scoring + fallback sur sÃ©lecteurs de base\n",
    "\n",
    "### ğŸ¤– **Scraper Robuste**\n",
    "- **RobustProductReviewScraper** : Navigation sÃ©curisÃ©e, pagination, extraction complÃ¨te\n",
    "- **DonnÃ©es** : Texte, note, date, auteur, titre produit, prix, etc.\n",
    "- **Gestion d'erreurs** : Retry automatique, fallbacks, validation des donnÃ©es\n",
    "- **Sauvegarde** : CSV + JSON automatique avec timestamp\n",
    "\n",
    "### ğŸ® **Workflow Principal**\n",
    "- **robust_reviews_workflow()** : Pipeline complet scout â†’ scraper â†’ analyse\n",
    "- **Menu interactif** : Interface utilisateur avec toutes les options\n",
    "- **Exemples prÃªts** : Amazon gaming laptops, eBay Ã©lectronique\n",
    "- **Tests robustesse** : Validation fallbacks et gestion d'erreurs\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸš€ **COMMENT UTILISER LE SYSTÃˆME**\n",
    "\n",
    "### **Option 1 : Menu Interactif** (RecommandÃ©)\n",
    "```python\n",
    "# DÃ©commentez dans la cellule prÃ©cÃ©dente :\n",
    "enhanced_interactive_menu()\n",
    "```\n",
    "\n",
    "### **Option 2 : Workflow Direct**\n",
    "```python\n",
    "# Exemple rapide Amazon\n",
    "df = robust_reviews_workflow('gaming laptop', 'amazon', 5, 30, False)\n",
    "```\n",
    "\n",
    "### **Option 3 : Tests SpÃ©cifiques**\n",
    "```python\n",
    "# Test Amazon gaming laptops\n",
    "df_amazon = test_amazon_gaming_laptops_ultra_secure()\n",
    "\n",
    "# Test eBay Ã©lectronique  \n",
    "df_ebay = test_ebay_electronics_robust()\n",
    "\n",
    "# Test de robustesse\n",
    "results = test_system_robustness()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“ **FICHIERS GÃ‰NÃ‰RÃ‰S AUTOMATIQUEMENT**\n",
    "\n",
    "- **`../config/detected_selectors_{site}.json`** : SÃ©lecteurs dÃ©tectÃ©s\n",
    "- **`../config/detected_selectors_{site}_backup_{timestamp}.json`** : Backups\n",
    "- **`../logs/ultra_scout_session_{timestamp}.json`** : Logs de session\n",
    "- **`../data/raw/{site}_{category}_{timestamp}.csv`** : DonnÃ©es scrapÃ©es\n",
    "\n",
    "---\n",
    "\n",
    "## âš ï¸ **IMPORTANT - UTILISATION RESPONSABLE**\n",
    "\n",
    "- **Ã‰ducatif uniquement** : Ce systÃ¨me est Ã  des fins d'apprentissage\n",
    "- **Respect des ToS** : VÃ©rifiez les conditions d'utilisation des sites\n",
    "- **Rate limiting** : DÃ©lais automatiques pour Ã©viter la surcharge\n",
    "- **Proxies** : Configurez PROXY_LIST si nÃ©cessaire pour la production\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”„ **PROCHAINES Ã‰TAPES POSSIBLES**\n",
    "\n",
    "1. **Tester sur cas rÃ©els** : Lancer les exemples Amazon/eBay\n",
    "2. **Valider robustesse** : ExÃ©cuter les tests de fallback\n",
    "3. **Personnaliser** : Adapter les sÃ©lecteurs pour d'autres sites\n",
    "4. **AmÃ©liorer** : Ajouter Playwright si besoin, proxies rotation\n",
    "5. **Automatiser** : Scheduler des collectes rÃ©guliÃ¨res\n",
    "\n",
    "Le systÃ¨me est maintenant **ultra-robuste** et **prÃªt Ã  l'emploi** ! ğŸ‰"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scraper_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
