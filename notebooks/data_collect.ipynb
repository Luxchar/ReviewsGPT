{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fb82a26",
   "metadata": {},
   "source": [
    "# ğŸ•µï¸ Advanced Marketplace Scraper - Anti-Detection System\n",
    "\n",
    "Ce notebook implÃ©mente un scraper Selenium avancÃ© avec systÃ¨me anti-dÃ©tection pour scraper des marketplaces anglaises :\n",
    "\n",
    "## ğŸ›¡ï¸ Protections Anti-Ban :\n",
    "- **Fake User Agents** : Rotation automatique d'user agents rÃ©alistes\n",
    "- **Proxy Rotation** : Rotation d'IP pour Ã©viter la dÃ©tection\n",
    "- **Comportement Humain** : Mouvements de souris, scrolling, pauses naturelles\n",
    "- **DÃ©lais AlÃ©atoires** : Timing humain entre les actions\n",
    "- **Headers RÃ©alistes** : Simulation complÃ¨te de navigateur rÃ©el\n",
    "\n",
    "## ğŸ¯ Sites CiblÃ©s :\n",
    "- **E-commerce** : Amazon-like sites, eBay, etc.\n",
    "- **Reviews** : Trustpilot, Google Reviews, Yelp\n",
    "- **Product Data** : Prix, descriptions, reviews avec dates\n",
    "- **APIs publiques** quand disponibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b4054a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (4.11.2)\n",
      "Requirement already satisfied: webdriver-manager in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (4.0.2)\n",
      "Requirement already satisfied: fake-useragent in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (2.2.0)\n",
      "Requirement already satisfied: requests in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (2.28.2)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (4.13.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (2.3.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.26.15)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from selenium) (0.30.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from selenium) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from selenium) (2025.6.15)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium) (25.3.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium) (2.8)\n",
      "Requirement already satisfied: outcome in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium) (1.17.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from webdriver-manager) (1.1.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from webdriver-manager) (25.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from requests) (3.4.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from beautifulsoup4) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from beautifulsoup4) (4.13.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from pandas) (2.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: pycparser in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.22)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: undetected-chromedriver in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (3.5.3)\n",
      "Requirement already satisfied: selenium-stealth in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (1.0.6)\n",
      "Requirement already satisfied: pyautogui in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (0.9.54)\n",
      "Requirement already satisfied: selenium>=4.9.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from undetected-chromedriver) (4.11.2)\n",
      "Requirement already satisfied: requests in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from undetected-chromedriver) (2.28.2)\n",
      "Requirement already satisfied: websockets in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from undetected-chromedriver) (15.0.1)\n",
      "Requirement already satisfied: pymsgbox in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from pyautogui) (1.0.9)\n",
      "Requirement already satisfied: pytweening>=1.0.4 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from pyautogui) (1.2.0)\n",
      "Requirement already satisfied: pyscreeze>=0.1.21 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from pyautogui) (1.0.1)\n",
      "Requirement already satisfied: pygetwindow>=0.0.5 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from pyautogui) (0.0.9)\n",
      "Requirement already satisfied: mouseinfo in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from pyautogui) (0.1.3)\n",
      "Requirement already satisfied: pyrect in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from pygetwindow>=0.0.5->pyautogui) (0.2.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium>=4.9.0->undetected-chromedriver) (1.26.15)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver) (0.30.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver) (2025.6.15)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (25.3.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (2.8)\n",
      "Requirement already satisfied: outcome in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (1.17.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio-websocket~=0.9->selenium>=4.9.0->undetected-chromedriver) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium>=4.9.0->undetected-chromedriver) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (2.22)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium>=4.9.0->undetected-chromedriver) (0.16.0)\n",
      "Requirement already satisfied: pyperclip in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from mouseinfo->pyautogui) (1.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from requests->undetected-chromedriver) (3.4.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: undetected-chromedriver in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (3.5.3)\n",
      "Requirement already satisfied: selenium-stealth in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (1.0.6)\n",
      "Requirement already satisfied: pyautogui in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (0.9.54)\n",
      "Requirement already satisfied: selenium>=4.9.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from undetected-chromedriver) (4.11.2)\n",
      "Requirement already satisfied: requests in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from undetected-chromedriver) (2.28.2)\n",
      "Requirement already satisfied: websockets in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from undetected-chromedriver) (15.0.1)\n",
      "Requirement already satisfied: pymsgbox in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from pyautogui) (1.0.9)\n",
      "Requirement already satisfied: pytweening>=1.0.4 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from pyautogui) (1.2.0)\n",
      "Requirement already satisfied: pyscreeze>=0.1.21 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from pyautogui) (1.0.1)\n",
      "Requirement already satisfied: pygetwindow>=0.0.5 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from pyautogui) (0.0.9)\n",
      "Requirement already satisfied: mouseinfo in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from pyautogui) (0.1.3)\n",
      "Requirement already satisfied: pyrect in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from pygetwindow>=0.0.5->pyautogui) (0.2.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium>=4.9.0->undetected-chromedriver) (1.26.15)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver) (0.30.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver) (2025.6.15)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (25.3.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (2.8)\n",
      "Requirement already satisfied: outcome in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (1.17.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio-websocket~=0.9->selenium>=4.9.0->undetected-chromedriver) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium>=4.9.0->undetected-chromedriver) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (2.22)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium>=4.9.0->undetected-chromedriver) (0.16.0)\n",
      "Requirement already satisfied: pyperclip in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from mouseinfo->pyautogui) (1.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from requests->undetected-chromedriver) (3.4.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: requests-proxy-adapter in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (0.1.1)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (2.9.0.post0)\n",
      "Requirement already satisfied: lxml in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (6.0.0)\n",
      "Requirement already satisfied: pluggy==0.11.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from requests-proxy-adapter) (0.11.0)\n",
      "Collecting requests==2.22.0 (from requests-proxy-adapter)\n",
      "  Downloading requests-2.22.0-py2.py3-none-any.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: urllib3>=1.15 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from requests-proxy-adapter) (1.26.15)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from requests==2.22.0->requests-proxy-adapter) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from requests==2.22.0->requests-proxy-adapter) (2.8)\n",
      "Collecting urllib3>=1.15 (from requests-proxy-adapter)\n",
      "  Downloading urllib3-1.25.11-py2.py3-none-any.whl.metadata (41 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from requests==2.22.0->requests-proxy-adapter) (2025.6.15)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from python-dateutil) (1.17.0)\n",
      "Downloading requests-2.22.0-py2.py3-none-any.whl (57 kB)\n",
      "Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
      "Installing collected packages: urllib3, requests\n",
      "\n",
      "  Attempting uninstall: urllib3\n",
      "\n",
      "    Found existing installation: urllib3 1.26.15\n",
      "\n",
      "    Uninstalling urllib3-1.26.15:\n",
      "\n",
      "      Successfully uninstalled urllib3-1.26.15\n",
      "\n",
      "   ---------------------------------------- 0/2 [urllib3]\n",
      "  Attempting uninstall: requests\n",
      "   ---------------------------------------- 0/2 [urllib3]\n",
      "    Found existing installation: requests 2.28.2\n",
      "   ---------------------------------------- 0/2 [urllib3]\n",
      "    Uninstalling requests-2.28.2:\n",
      "   ---------------------------------------- 0/2 [urllib3]\n",
      "      Successfully uninstalled requests-2.28.2\n",
      "   ---------------------------------------- 0/2 [urllib3]\n",
      "   -------------------- ------------------- 1/2 [requests]\n",
      "   ---------------------------------------- 2/2 [requests]\n",
      "\n",
      "Successfully installed requests-2.22.0 urllib3-1.25.11\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "ğŸ“¦ Installation terminÃ©e - Scraper anti-dÃ©tection prÃªt !\n",
      "Requirement already satisfied: requests-proxy-adapter in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (0.1.1)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (2.9.0.post0)\n",
      "Requirement already satisfied: lxml in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (6.0.0)\n",
      "Requirement already satisfied: pluggy==0.11.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from requests-proxy-adapter) (0.11.0)\n",
      "Collecting requests==2.22.0 (from requests-proxy-adapter)\n",
      "  Downloading requests-2.22.0-py2.py3-none-any.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: urllib3>=1.15 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from requests-proxy-adapter) (1.26.15)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from requests==2.22.0->requests-proxy-adapter) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from requests==2.22.0->requests-proxy-adapter) (2.8)\n",
      "Collecting urllib3>=1.15 (from requests-proxy-adapter)\n",
      "  Downloading urllib3-1.25.11-py2.py3-none-any.whl.metadata (41 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from requests==2.22.0->requests-proxy-adapter) (2025.6.15)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from python-dateutil) (1.17.0)\n",
      "Downloading requests-2.22.0-py2.py3-none-any.whl (57 kB)\n",
      "Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
      "Installing collected packages: urllib3, requests\n",
      "\n",
      "  Attempting uninstall: urllib3\n",
      "\n",
      "    Found existing installation: urllib3 1.26.15\n",
      "\n",
      "    Uninstalling urllib3-1.26.15:\n",
      "\n",
      "      Successfully uninstalled urllib3-1.26.15\n",
      "\n",
      "   ---------------------------------------- 0/2 [urllib3]\n",
      "  Attempting uninstall: requests\n",
      "   ---------------------------------------- 0/2 [urllib3]\n",
      "    Found existing installation: requests 2.28.2\n",
      "   ---------------------------------------- 0/2 [urllib3]\n",
      "    Uninstalling requests-2.28.2:\n",
      "   ---------------------------------------- 0/2 [urllib3]\n",
      "      Successfully uninstalled requests-2.28.2\n",
      "   ---------------------------------------- 0/2 [urllib3]\n",
      "   -------------------- ------------------- 1/2 [requests]\n",
      "   ---------------------------------------- 2/2 [requests]\n",
      "\n",
      "Successfully installed requests-2.22.0 urllib3-1.25.11\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "ğŸ“¦ Installation terminÃ©e - Scraper anti-dÃ©tection prÃªt !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "jupyterlab-server 2.27.3 requires requests>=2.31, but you have requests 2.22.0 which is incompatible.\n",
      "selenium 4.11.2 requires urllib3[socks]<3,>=1.26, but you have urllib3 1.25.11 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "# Installation des packages pour scraping anti-dÃ©tection\n",
    "%pip install selenium webdriver-manager fake-useragent requests beautifulsoup4 pandas\n",
    "%pip install undetected-chromedriver selenium-stealth pyautogui\n",
    "%pip install requests-proxy-adapter python-dateutil lxml\n",
    "\n",
    "print(\"ğŸ“¦ Installation terminÃ©e - Scraper anti-dÃ©tection prÃªt !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88736027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting urllib3==1.26.18\n",
      "  Downloading urllib3-1.26.18-py2.py3-none-any.whl.metadata (48 kB)\n",
      "Downloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
      "Installing collected packages: urllib3\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.25.11\n",
      "    Uninstalling urllib3-1.25.11:\n",
      "      Successfully uninstalled urllib3-1.25.11\n",
      "Successfully installed urllib3-1.26.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "requests 2.22.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you have urllib3 1.26.18 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium==4.15.0\n",
      "  Downloading selenium-4.15.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium==4.15.0) (1.26.18)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from selenium==4.15.0) (0.30.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from selenium==4.15.0) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from selenium==4.15.0) (2025.6.15)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium==4.15.0) (25.3.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium==4.15.0) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium==4.15.0) (2.8)\n",
      "Requirement already satisfied: outcome in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium==4.15.0) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium==4.15.0) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium==4.15.0) (1.17.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio-websocket~=0.9->selenium==4.15.0) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium==4.15.0) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium==4.15.0) (2.22)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium==4.15.0) (0.16.0)\n",
      "Downloading selenium-4.15.0-py3-none-any.whl (10.2 MB)\n",
      "   ---------------------------------------- 0.0/10.2 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 1.0/10.2 MB 6.6 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 2.6/10.2 MB 6.7 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 4.2/10.2 MB 6.8 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 5.5/10.2 MB 6.7 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 7.1/10.2 MB 6.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 8.4/10.2 MB 6.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.0/10.2 MB 6.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.0/10.2 MB 6.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.2/10.2 MB 5.4 MB/s eta 0:00:00\n",
      "Installing collected packages: selenium\n",
      "  Attempting uninstall: selenium\n",
      "    Found existing installation: selenium 4.11.2\n",
      "    Uninstalling selenium-4.11.2:\n",
      "      Successfully uninstalled selenium-4.11.2\n",
      "Successfully installed selenium-4.15.0\n",
      "Collecting undetected-chromedriver==3.5.4\n",
      "  Downloading undetected-chromedriver-3.5.4.tar.gz (65 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: selenium>=4.9.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from undetected-chromedriver==3.5.4) (4.15.0)\n",
      "Requirement already satisfied: requests in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from undetected-chromedriver==3.5.4) (2.22.0)\n",
      "Requirement already satisfied: websockets in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from undetected-chromedriver==3.5.4) (15.0.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium>=4.9.0->undetected-chromedriver==3.5.4) (1.26.18)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver==3.5.4) (0.30.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver==3.5.4) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver==3.5.4) (2025.6.15)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver==3.5.4) (25.3.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver==3.5.4) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver==3.5.4) (2.8)\n",
      "Requirement already satisfied: outcome in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver==3.5.4) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver==3.5.4) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver==3.5.4) (1.17.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio-websocket~=0.9->selenium>=4.9.0->undetected-chromedriver==3.5.4) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium>=4.9.0->undetected-chromedriver==3.5.4) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium>=4.9.0->undetected-chromedriver==3.5.4) (2.22)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium>=4.9.0->undetected-chromedriver==3.5.4) (0.16.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from requests->undetected-chromedriver==3.5.4) (3.0.4)\n",
      "INFO: pip is looking at multiple versions of requests to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting requests (from undetected-chromedriver==3.5.4)\n",
      "  Downloading requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from requests->undetected-chromedriver==3.5.4) (3.4.2)\n",
      "Downloading requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "Building wheels for collected packages: undetected-chromedriver\n",
      "  Building wheel for undetected-chromedriver (pyproject.toml): started\n",
      "  Building wheel for undetected-chromedriver (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for undetected-chromedriver: filename=undetected_chromedriver-3.5.4-py3-none-any.whl size=47221 sha256=2906bdf9288dd8e24264f2f1eb6fd072f3be1fa9fd107e7123947cc5c3c2d2fa\n",
      "  Stored in directory: c:\\users\\yann\\appdata\\local\\pip\\cache\\wheels\\ca\\7f\\a2\\7feaa6b6bfdf742311206d2b103d185711dda87177add2ee71\n",
      "Successfully built undetected-chromedriver\n",
      "Installing collected packages: requests, undetected-chromedriver\n",
      "\n",
      "  Attempting uninstall: requests\n",
      "\n",
      "    Found existing installation: requests 2.22.0\n",
      "\n",
      "    Uninstalling requests-2.22.0:\n",
      "\n",
      "      Successfully uninstalled requests-2.22.0\n",
      "\n",
      "  Attempting uninstall: undetected-chromedriver\n",
      "\n",
      "    Found existing installation: undetected-chromedriver 3.5.3\n",
      "\n",
      "    Uninstalling undetected-chromedriver-3.5.3:\n",
      "\n",
      "      Successfully uninstalled undetected-chromedriver-3.5.3\n",
      "\n",
      "   -------------------- ------------------- 1/2 [undetected-chromedriver]\n",
      "   -------------------- ------------------- 1/2 [undetected-chromedriver]\n",
      "   ---------------------------------------- 2/2 [undetected-chromedriver]\n",
      "\n",
      "Successfully installed requests-2.32.4 undetected-chromedriver-3.5.4\n",
      "Collecting undetected-chromedriver==3.5.4\n",
      "  Downloading undetected-chromedriver-3.5.4.tar.gz (65 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: selenium>=4.9.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from undetected-chromedriver==3.5.4) (4.15.0)\n",
      "Requirement already satisfied: requests in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from undetected-chromedriver==3.5.4) (2.22.0)\n",
      "Requirement already satisfied: websockets in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from undetected-chromedriver==3.5.4) (15.0.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium>=4.9.0->undetected-chromedriver==3.5.4) (1.26.18)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver==3.5.4) (0.30.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver==3.5.4) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver==3.5.4) (2025.6.15)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver==3.5.4) (25.3.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver==3.5.4) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver==3.5.4) (2.8)\n",
      "Requirement already satisfied: outcome in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver==3.5.4) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver==3.5.4) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver==3.5.4) (1.17.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from trio-websocket~=0.9->selenium>=4.9.0->undetected-chromedriver==3.5.4) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium>=4.9.0->undetected-chromedriver==3.5.4) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium>=4.9.0->undetected-chromedriver==3.5.4) (2.22)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium>=4.9.0->undetected-chromedriver==3.5.4) (0.16.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from requests->undetected-chromedriver==3.5.4) (3.0.4)\n",
      "INFO: pip is looking at multiple versions of requests to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting requests (from undetected-chromedriver==3.5.4)\n",
      "  Downloading requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\yann\\desktop\\dev\\school\\ml_m1\\nlp\\customgpt\\scraper_env\\lib\\site-packages (from requests->undetected-chromedriver==3.5.4) (3.4.2)\n",
      "Downloading requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "Building wheels for collected packages: undetected-chromedriver\n",
      "  Building wheel for undetected-chromedriver (pyproject.toml): started\n",
      "  Building wheel for undetected-chromedriver (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for undetected-chromedriver: filename=undetected_chromedriver-3.5.4-py3-none-any.whl size=47221 sha256=2906bdf9288dd8e24264f2f1eb6fd072f3be1fa9fd107e7123947cc5c3c2d2fa\n",
      "  Stored in directory: c:\\users\\yann\\appdata\\local\\pip\\cache\\wheels\\ca\\7f\\a2\\7feaa6b6bfdf742311206d2b103d185711dda87177add2ee71\n",
      "Successfully built undetected-chromedriver\n",
      "Installing collected packages: requests, undetected-chromedriver\n",
      "\n",
      "  Attempting uninstall: requests\n",
      "\n",
      "    Found existing installation: requests 2.22.0\n",
      "\n",
      "    Uninstalling requests-2.22.0:\n",
      "\n",
      "      Successfully uninstalled requests-2.22.0\n",
      "\n",
      "  Attempting uninstall: undetected-chromedriver\n",
      "\n",
      "    Found existing installation: undetected-chromedriver 3.5.3\n",
      "\n",
      "    Uninstalling undetected-chromedriver-3.5.3:\n",
      "\n",
      "      Successfully uninstalled undetected-chromedriver-3.5.3\n",
      "\n",
      "   -------------------- ------------------- 1/2 [undetected-chromedriver]\n",
      "   -------------------- ------------------- 1/2 [undetected-chromedriver]\n",
      "   ---------------------------------------- 2/2 [undetected-chromedriver]\n",
      "\n",
      "Successfully installed requests-2.32.4 undetected-chromedriver-3.5.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "requests-proxy-adapter 0.1.1 requires requests==2.22.0, but you have requests 2.32.4 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "# Dans votre terminal ou une cellule :\n",
    "!pip install urllib3==1.26.18\n",
    "!pip install selenium==4.15.0\n",
    "!pip install undetected-chromedriver==3.5.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839c3eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¥ RÃ‰PARATION FORCE - urllib3.packages.six.moves\n",
      "ğŸ—‘ï¸ Nettoyage: uninstall urllib3 -y\n",
      "ğŸ—‘ï¸ Nettoyage: uninstall urllib3 -y\n",
      "ğŸ—‘ï¸ Nettoyage: uninstall selenium -y\n",
      "ğŸ—‘ï¸ Nettoyage: uninstall selenium -y\n",
      "ğŸ—‘ï¸ Nettoyage: uninstall undetected-chromedriver -y\n",
      "ğŸ—‘ï¸ Nettoyage: uninstall undetected-chromedriver -y\n",
      "ğŸ—‘ï¸ Nettoyage: uninstall requests -y\n",
      "ğŸ—‘ï¸ Nettoyage: uninstall requests -y\n",
      "ğŸ—‘ï¸ Nettoyage: cache purge\n",
      "ğŸ—‘ï¸ Nettoyage: cache purge\n",
      "âœ… urllib3==1.26.15\n",
      "âœ… urllib3==1.26.15\n",
      "âœ… requests==2.28.2\n",
      "âœ… requests==2.28.2\n",
      "âœ… selenium==4.11.2\n",
      "âœ… selenium==4.11.2\n",
      "âœ… undetected-chromedriver==3.5.3\n",
      "\n",
      "ğŸ§ª TEST FINAL...\n",
      "âœ… urllib3 version: 1.26.15\n",
      "âœ… selenium version: 4.11.2\n",
      "âœ… undetected_chromedriver: OK\n",
      "\n",
      "ğŸ‰ RÃ‰PARATION RÃ‰USSIE !\n",
      "âœ… undetected-chromedriver==3.5.3\n",
      "\n",
      "ğŸ§ª TEST FINAL...\n",
      "âœ… urllib3 version: 1.26.15\n",
      "âœ… selenium version: 4.11.2\n",
      "âœ… undetected_chromedriver: OK\n",
      "\n",
      "ğŸ‰ RÃ‰PARATION RÃ‰USSIE !\n"
     ]
    }
   ],
   "source": [
    "# CELLULE: RÃ‰PARATION FORCE\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "def force_fix_urllib3():\n",
    "    \"\"\"Solution force pour urllib3.packages.six.moves\"\"\"\n",
    "    \n",
    "    print(\"ğŸ”¥ RÃ‰PARATION FORCE - urllib3.packages.six.moves\")\n",
    "    \n",
    "    # 1. Nettoyage brutal\n",
    "    cleanup_commands = [\n",
    "        [sys.executable, '-m', 'pip', 'uninstall', 'urllib3', '-y'],\n",
    "        [sys.executable, '-m', 'pip', 'uninstall', 'selenium', '-y'], \n",
    "        [sys.executable, '-m', 'pip', 'uninstall', 'undetected-chromedriver', '-y'],\n",
    "        [sys.executable, '-m', 'pip', 'uninstall', 'requests', '-y'],\n",
    "        [sys.executable, '-m', 'pip', 'cache', 'purge']\n",
    "    ]\n",
    "    \n",
    "    for cmd in cleanup_commands:\n",
    "        subprocess.run(cmd, capture_output=True)\n",
    "        print(f\"ğŸ—‘ï¸ Nettoyage: {' '.join(cmd[3:])}\")\n",
    "    \n",
    "    # 2. Installation versions STABLES\n",
    "    stable_packages = [\n",
    "        'urllib3==1.26.15',\n",
    "        'requests==2.28.2', \n",
    "        'selenium==4.11.2',\n",
    "        'undetected-chromedriver==3.5.3'\n",
    "    ]\n",
    "    \n",
    "    for package in stable_packages:\n",
    "        cmd = [sys.executable, '-m', 'pip', 'install', package]\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(f\"âœ… {package}\")\n",
    "        else:\n",
    "            print(f\"âŒ {package}: {result.stderr}\")\n",
    "    \n",
    "    # 3. Test immÃ©diat\n",
    "    print(\"\\nğŸ§ª TEST FINAL...\")\n",
    "    try:\n",
    "        import importlib\n",
    "        import urllib3\n",
    "        print(f\"âœ… urllib3 version: {urllib3.__version__}\")\n",
    "        \n",
    "        import selenium\n",
    "        print(f\"âœ… selenium version: {selenium.__version__}\")\n",
    "        \n",
    "        import undetected_chromedriver as uc\n",
    "        print(\"âœ… undetected_chromedriver: OK\")\n",
    "        \n",
    "        print(\"\\nğŸ‰ RÃ‰PARATION RÃ‰USSIE !\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Test Ã©chouÃ©: {e}\")\n",
    "\n",
    "# EXÃ‰CUTER LA RÃ‰PARATION FORCE\n",
    "force_fix_urllib3()\n",
    "\n",
    "import random\n",
    "import undetected_chromedriver as uc\n",
    "\n",
    "REALISTIC_USER_AGENTS = [\n",
    "    # Liste d'exemples d'agents utilisateurs rÃ©alistes\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.2 Safari/605.1.15\",\n",
    "    \"Mozilla/5.0 (iPhone; CPU iPhone OS 14_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.1 Mobile/15E148 Safari/604.1\",\n",
    "    \"Mozilla/5.0 (Linux; Android 10; SM-G973F) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Mobile Safari/537.36\",\n",
    "    # Ajoutez d'autres agents utilisateurs si nÃ©cessaire\n",
    "]\n",
    "\n",
    "PROXY_LIST = [\n",
    "    # Liste d'exemples de proxies\n",
    "    \"http://proxy1:port\",\n",
    "    \"http://proxy2:port\",\n",
    "    \"http://proxy3:port\",\n",
    "    # Ajoutez d'autres proxies si nÃ©cessaire\n",
    "]\n",
    "\n",
    "class StealthMarketplaceScraper:\n",
    "    \"\"\"\n",
    "    Scraper avancÃ© avec protection anti-dÃ©tection pour marketplaces\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, headless=True, use_proxy=False):\n",
    "        self.headless = headless\n",
    "        self.use_proxy = use_proxy\n",
    "        self.driver = None\n",
    "        self.session_duration = 0\n",
    "        \n",
    "    def _get_random_user_agent(self):\n",
    "        return random.choice(REALISTIC_USER_AGENTS)\n",
    "    \n",
    "    def _get_random_proxy(self):\n",
    "        return random.choice(PROXY_LIST) if self.use_proxy and PROXY_LIST else None\n",
    "        \n",
    "    def _setup_driver(self):\n",
    "        \"\"\"Configuration Chrome optimisÃ©e et compatible\"\"\"\n",
    "        try:\n",
    "            options = uc.ChromeOptions()\n",
    "            \n",
    "            # Options de base compatibles\n",
    "            options.add_argument('--no-sandbox')\n",
    "            options.add_argument('--disable-dev-shm-usage')\n",
    "            options.add_argument('--disable-gpu')\n",
    "            options.add_argument('--disable-extensions')\n",
    "            options.add_argument('--disable-plugins')\n",
    "            options.add_argument('--disable-images')\n",
    "            options.add_argument('--disable-javascript')\n",
    "            options.add_argument('--disable-notifications')\n",
    "            options.add_argument('--disable-popup-blocking')\n",
    "            options.add_argument('--no-first-run')\n",
    "            options.add_argument('--no-default-browser-check')\n",
    "            options.add_argument('--ignore-certificate-errors')\n",
    "            options.add_argument('--ignore-ssl-errors')\n",
    "            options.add_argument('--ignore-certificate-errors-spki-list')\n",
    "            options.add_argument('--disable-web-security')\n",
    "            \n",
    "            if self.headless:\n",
    "                options.add_argument('--headless')\n",
    "                \n",
    "            # User agent alÃ©atoire\n",
    "            user_agent = self._get_random_user_agent()\n",
    "            options.add_argument(f'--user-agent={user_agent}')\n",
    "            \n",
    "            # Proxy si activÃ©\n",
    "            proxy = self._get_random_proxy()\n",
    "            if proxy:\n",
    "                options.add_argument(f'--proxy-server={proxy}')\n",
    "                \n",
    "            # Options expÃ©rimentales compatibles (sans excludeSwitches)\n",
    "            options.add_experimental_option('useAutomationExtension', False)\n",
    "            options.add_experimental_option(\"prefs\", {\n",
    "                \"profile.default_content_setting_values.notifications\": 2,\n",
    "                \"profile.default_content_settings.popups\": 0,\n",
    "                \"profile.managed_default_content_settings.images\": 2\n",
    "            })\n",
    "            \n",
    "            # CrÃ©ation du driver avec version dÃ©tectÃ©e automatiquement\n",
    "            self.driver = uc.Chrome(options=options, version_main=None)\n",
    "            \n",
    "            # Injection anti-dÃ©tection JavaScript\n",
    "            self.driver.execute_cdp_cmd('Page.addScriptToEvaluateOnNewDocument', {\n",
    "                'source': '''\n",
    "                    Object.defineProperty(navigator, 'webdriver', {\n",
    "                        get: () => undefined,\n",
    "                    });\n",
    "                    \n",
    "                    Object.defineProperty(navigator, 'plugins', {\n",
    "                        get: () => [1, 2, 3, 4, 5],\n",
    "                    });\n",
    "                    \n",
    "                    Object.defineProperty(navigator, 'languages', {\n",
    "                        get: () => ['en-US', 'en'],\n",
    "                    });\n",
    "                    \n",
    "                    window.chrome = {\n",
    "                        runtime: {},\n",
    "                    };\n",
    "                '''\n",
    "            })\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Erreur crÃ©ation driver: {e}\")\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b76c7499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Imports terminÃ©s - SystÃ¨me anti-dÃ©tection prÃªt ! ğŸ•µï¸\n"
     ]
    }
   ],
   "source": [
    "import undetected_chromedriver as uc\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium_stealth import stealth\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from fake_useragent import UserAgent\n",
    "import logging\n",
    "from typing import List, Dict, Optional\n",
    "import os\n",
    "\n",
    "# Configuration du logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configuration globale\n",
    "ua = UserAgent()\n",
    "\n",
    "# Liste de proxies gratuits (remplacer par des proxies premium en production)\n",
    "PROXY_LIST = [\n",
    "    # \"http://proxy1:port\",\n",
    "    # \"http://proxy2:port\", \n",
    "    # Ajoutez vos proxies ici\n",
    "]\n",
    "\n",
    "# User agents rÃ©alistes\n",
    "REALISTIC_USER_AGENTS = [\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15'\n",
    "]\n",
    "\n",
    "print(\"âœ… Imports terminÃ©s - SystÃ¨me anti-dÃ©tection prÃªt ! ğŸ•µï¸\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23ef9f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 16:10:40,189 - INFO - âœ… MarketplaceScraper initialisÃ©\n",
      "2025-06-27 16:10:40,192 - INFO - ğŸ”§ Initialisation du scraper anti-dÃ©tection...\n",
      "2025-06-27 16:10:40,192 - INFO - ğŸ”§ Initialisation du scraper anti-dÃ©tection...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ Scraper initialisÃ© avec succÃ¨s !\n",
      "ğŸ•µï¸ Scraper anti-dÃ©tection initialisÃ© !\n"
     ]
    }
   ],
   "source": [
    "class MarketplaceScraper:\n",
    "    \"\"\"\n",
    "    Scraper principal pour les marketplaces et reviews.\n",
    "    Supporte diffÃ©rentes plateformes avec rotation d'User-Agent.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, delay_range=(1, 3)):\n",
    "        self.session = requests.Session()\n",
    "        self.delay_min, self.delay_max = delay_range\n",
    "        self.ua = UserAgent()\n",
    "        \n",
    "        # Headers par dÃ©faut\n",
    "        self.session.headers.update({\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "            'Accept-Language': 'fr-FR,fr;q=0.9,en;q=0.8',\n",
    "            'Accept-Encoding': 'gzip, deflate',\n",
    "            'Connection': 'keep-alive',\n",
    "            'Upgrade-Insecure-Requests': '1',\n",
    "        })\n",
    "        \n",
    "        logger.info(\"âœ… MarketplaceScraper initialisÃ©\")\n",
    "    \n",
    "    def _get_page(self, url: str, retries: int = 3) -> Optional[BeautifulSoup]:\n",
    "        \"\"\"\n",
    "        RÃ©cupÃ¨re une page web avec gestion d'erreurs et dÃ©lais.\n",
    "        \"\"\"\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                # Rotation d'User-Agent Ã  chaque requÃªte\n",
    "                self.session.headers['User-Agent'] = self.ua.random\n",
    "                \n",
    "                response = self.session.get(url, timeout=10)\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                # DÃ©lai alÃ©atoire entre requÃªtes\n",
    "                delay = random.uniform(self.delay_min, self.delay_max)\n",
    "                time.sleep(delay)\n",
    "                \n",
    "                return BeautifulSoup(response.content, 'html.parser')\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Erreur tentative {attempt + 1}/{retries} pour {url}: {e}\")\n",
    "                if attempt < retries - 1:\n",
    "                    time.sleep(2 ** attempt)  # Backoff exponentiel\n",
    "                else:\n",
    "                    logger.error(f\"Ã‰chec dÃ©finitif pour {url}\")\n",
    "                    return None\n",
    "    \n",
    "    def _extract_date(self, date_text: str) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Extrait et normalise les dates depuis diffÃ©rents formats.\n",
    "        \"\"\"\n",
    "        if not date_text:\n",
    "            return None\n",
    "            \n",
    "        date_text = date_text.strip().lower()\n",
    "        \n",
    "        # Patterns de dates franÃ§ais\n",
    "        patterns = [\n",
    "            (r'(\\d{1,2})\\s+(janvier|fÃ©vrier|mars|avril|mai|juin|juillet|aoÃ»t|septembre|octobre|novembre|dÃ©cembre)\\s+(\\d{4})', '%d %B %Y'),\n",
    "            (r'(\\d{1,2})/(\\d{1,2})/(\\d{4})', '%d/%m/%Y'),\n",
    "            (r'(\\d{4})-(\\d{1,2})-(\\d{1,2})', '%Y-%m-%d'),\n",
    "        ]\n",
    "        \n",
    "        # Mapping des mois franÃ§ais\n",
    "        months_fr = {\n",
    "            'janvier': 'january', 'fÃ©vrier': 'february', 'mars': 'march',\n",
    "            'avril': 'april', 'mai': 'may', 'juin': 'june',\n",
    "            'juillet': 'july', 'aoÃ»t': 'august', 'septembre': 'september',\n",
    "            'octobre': 'october', 'novembre': 'november', 'dÃ©cembre': 'december'\n",
    "        }\n",
    "        \n",
    "        for month_fr, month_en in months_fr.items():\n",
    "            date_text = date_text.replace(month_fr, month_en)\n",
    "        \n",
    "        for pattern, fmt in patterns:\n",
    "            match = re.search(pattern, date_text)\n",
    "            if match:\n",
    "                try:\n",
    "                    date_obj = datetime.strptime(match.group(), fmt)\n",
    "                    return date_obj.strftime('%Y-%m-%d')\n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "        return None\n",
    "\n",
    "class StealthMarketplaceScraper:\n",
    "    \"\"\"\n",
    "    Scraper anti-dÃ©tection pour marketplaces avec Selenium.\n",
    "    Inclut rotation d'IP, fake user agents, et comportement humain.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, headless=True, use_proxy=False):\n",
    "        self.headless = headless\n",
    "        self.use_proxy = use_proxy\n",
    "        self.driver = None\n",
    "        self.current_proxy = None\n",
    "        \n",
    "        logger.info(\"ğŸ”§ Initialisation du scraper anti-dÃ©tection...\")\n",
    "        \n",
    "    def _get_random_user_agent(self):\n",
    "        \"\"\"Retourne un User Agent alÃ©atoire rÃ©aliste.\"\"\"\n",
    "        return random.choice(REALISTIC_USER_AGENTS)\n",
    "    \n",
    "    def _get_random_proxy(self):\n",
    "        \"\"\"Retourne un proxy alÃ©atoire.\"\"\"\n",
    "        if PROXY_LIST:\n",
    "            return random.choice(PROXY_LIST)\n",
    "        return None\n",
    "    \n",
    "    def _setup_driver(self):\n",
    "        \"\"\"Configure le driver Chrome avec toutes les protections anti-dÃ©tection.\"\"\"\n",
    "        options = Options()\n",
    "        \n",
    "        # Configuration anti-dÃ©tection\n",
    "        options.add_argument('--no-sandbox')\n",
    "        options.add_argument('--disable-dev-shm-usage')\n",
    "        options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "        options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "        options.add_experimental_option('useAutomationExtension', False)\n",
    "        \n",
    "        # User Agent alÃ©atoire\n",
    "        user_agent = self._get_random_user_agent()\n",
    "        options.add_argument(f'--user-agent={user_agent}')\n",
    "        logger.info(f\"ğŸ­ User Agent: {user_agent[:50]}...\")\n",
    "        \n",
    "        # Proxy si activÃ©\n",
    "        if self.use_proxy:\n",
    "            proxy = self._get_random_proxy()\n",
    "            if proxy:\n",
    "                options.add_argument(f'--proxy-server={proxy}')\n",
    "                self.current_proxy = proxy\n",
    "                logger.info(f\"ğŸŒ Proxy: {proxy}\")\n",
    "        \n",
    "        # Mode headless si demandÃ©\n",
    "        if self.headless:\n",
    "            options.add_argument('--headless')\n",
    "        \n",
    "        # Taille de fenÃªtre rÃ©aliste\n",
    "        options.add_argument('--window-size=1920,1080')\n",
    "        \n",
    "        # DÃ©sactiver les images pour plus de rapiditÃ© (optionnel)\n",
    "        # options.add_argument('--blink-settings=imagesEnabled=false')\n",
    "        \n",
    "        try:\n",
    "            # Utiliser undetected-chromedriver pour Ã©viter la dÃ©tection\n",
    "            self.driver = uc.Chrome(options=options)\n",
    "            \n",
    "            # Configuration Selenium Stealth pour plus de protection\n",
    "            stealth(self.driver,\n",
    "                   languages=[\"en-US\", \"en\"],\n",
    "                   vendor=\"Google Inc.\",\n",
    "                   platform=\"Win32\",\n",
    "                   webgl_vendor=\"Intel Inc.\",\n",
    "                   renderer=\"Intel Iris OpenGL Engine\",\n",
    "                   fix_hairline=True)\n",
    "            \n",
    "            # Masquer le fait que c'est un webdriver\n",
    "            self.driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
    "            \n",
    "            logger.info(\"âœ… Driver Chrome configurÃ© avec protections anti-dÃ©tection\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"âŒ Erreur lors de la configuration du driver: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def _human_like_delay(self, min_delay=1, max_delay=3):\n",
    "        \"\"\"DÃ©lai alÃ©atoire pour simuler un comportement humain.\"\"\"\n",
    "        delay = random.uniform(min_delay, max_delay)\n",
    "        time.sleep(delay)\n",
    "    \n",
    "    def _simulate_human_behavior(self):\n",
    "        \"\"\"Simule des comportements humains alÃ©atoires.\"\"\"\n",
    "        actions = ActionChains(self.driver)\n",
    "        \n",
    "        # Mouvement alÃ©atoire de la souris\n",
    "        if random.random() < 0.3:  # 30% de chance\n",
    "            x_offset = random.randint(-100, 100)\n",
    "            y_offset = random.randint(-100, 100)\n",
    "            actions.move_by_offset(x_offset, y_offset).perform()\n",
    "            time.sleep(random.uniform(0.1, 0.5))\n",
    "        \n",
    "        # Scroll alÃ©atoire\n",
    "        if random.random() < 0.4:  # 40% de chance\n",
    "            scroll_amount = random.randint(100, 500)\n",
    "            self.driver.execute_script(f\"window.scrollBy(0, {scroll_amount});\")\n",
    "            time.sleep(random.uniform(0.5, 1.5))\n",
    "    \n",
    "    def start_session(self):\n",
    "        \"\"\"DÃ©marre une session de scraping.\"\"\"\n",
    "        self._setup_driver()\n",
    "        logger.info(\"ğŸš€ Session de scraping dÃ©marrÃ©e\")\n",
    "    \n",
    "    def close_session(self):\n",
    "        \"\"\"Ferme la session de scraping.\"\"\"\n",
    "        if self.driver:\n",
    "            self.driver.quit()\n",
    "            logger.info(\"ğŸ”š Session fermÃ©e\")\n",
    "        \n",
    "    def get_page(self, url: str, wait_time: int = 10) -> bool:\n",
    "        \"\"\"\n",
    "        Navigue vers une page avec comportement humain simulÃ©.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(f\"ğŸŒ Navigation vers: {url}\")\n",
    "            \n",
    "            # Navigation avec dÃ©lai humain\n",
    "            self.driver.get(url)\n",
    "            self._human_like_delay(2, 4)\n",
    "            \n",
    "            # Attendre que la page se charge\n",
    "            WebDriverWait(self.driver, wait_time).until(\n",
    "                EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
    "            )\n",
    "            \n",
    "            # Comportement humain alÃ©atoire\n",
    "            self._simulate_human_behavior()\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"âŒ Erreur lors de la navigation: {e}\")\n",
    "            return False\n",
    "\n",
    "# Test de la classe\n",
    "scraper = MarketplaceScraper()\n",
    "print(\"ğŸ¯ Scraper initialisÃ© avec succÃ¨s !\")\n",
    "\n",
    "# Test de la classe anti-dÃ©tection\n",
    "stealth_scraper = StealthMarketplaceScraper(headless=False)  # Visible pour le test\n",
    "print(\"ğŸ•µï¸ Scraper anti-dÃ©tection initialisÃ© !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "876bc2f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ MarketplaceProductScraper prÃªt avec anti-dÃ©tection complÃ¨te !\n"
     ]
    }
   ],
   "source": [
    "class MarketplaceProductScraper(StealthMarketplaceScraper):\n",
    "    \"\"\"\n",
    "    Scraper spÃ©cialisÃ© pour produits et reviews de marketplaces.\n",
    "    \"\"\"\n",
    "    \n",
    "    def scrape_amazon_style_products(self, search_term: str, max_pages: int = 3) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Scrape des produits type Amazon (utilise un site de dÃ©monstration).\n",
    "        \"\"\"\n",
    "        logger.info(f\"ğŸ›ï¸ Scraping produits pour: {search_term}\")\n",
    "        \n",
    "        if not self.driver:\n",
    "            self.start_session()\n",
    "        \n",
    "        products_data = []\n",
    "        \n",
    "        try:\n",
    "            # Utiliser un site de dÃ©monstration e-commerce\n",
    "            base_url = \"https://webscraper.io/test-sites/e-commerce/allinone\"\n",
    "            \n",
    "            if not self.get_page(base_url):\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            # Attendre et trouver les produits\n",
    "            products = WebDriverWait(self.driver, 10).until(\n",
    "                EC.presence_of_all_elements_located((By.CSS_SELECTOR, \".product-wrapper\"))\n",
    "            )\n",
    "            \n",
    "            for i, product in enumerate(products[:20]):  # Limiter Ã  20 produits\n",
    "                try:\n",
    "                    # Simuler un comportement humain\n",
    "                    if i % 5 == 0:\n",
    "                        self._simulate_human_behavior()\n",
    "                    \n",
    "                    # Extraire les donnÃ©es du produit\n",
    "                    title_elem = product.find_element(By.CSS_SELECTOR, \".title\")\n",
    "                    price_elem = product.find_element(By.CSS_SELECTOR, \".price\")\n",
    "                    \n",
    "                    title = title_elem.text.strip()\n",
    "                    price_text = price_elem.text.strip()\n",
    "                    \n",
    "                    # Nettoyer le prix\n",
    "                    price = re.findall(r'[\\d.]+', price_text)\n",
    "                    price = float(price[0]) if price else 0.0\n",
    "                    \n",
    "                    # Essayer de trouver la description et l'image\n",
    "                    try:\n",
    "                        desc_elem = product.find_element(By.CSS_SELECTOR, \".description\")\n",
    "                        description = desc_elem.text.strip()\n",
    "                    except:\n",
    "                        description = \"\"\n",
    "                    \n",
    "                    try:\n",
    "                        img_elem = product.find_element(By.CSS_SELECTOR, \"img\")\n",
    "                        image_url = img_elem.get_attribute(\"src\")\n",
    "                    except:\n",
    "                        image_url = \"\"\n",
    "                    \n",
    "                    products_data.append({\n",
    "                        'product_id': f\"demo_{i}\",\n",
    "                        'title': title,\n",
    "                        'price': price,\n",
    "                        'description': description,\n",
    "                        'image_url': image_url,\n",
    "                        'source': 'webscraper.io',\n",
    "                        'search_term': search_term,\n",
    "                        'scraped_at': datetime.now().isoformat(),\n",
    "                        'user_agent': self.driver.execute_script(\"return navigator.userAgent;\"),\n",
    "                        'proxy': self.current_proxy\n",
    "                    })\n",
    "                    \n",
    "                    # DÃ©lai humain entre produits\n",
    "                    self._human_like_delay(0.5, 1.5)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Erreur extraction produit {i}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            logger.info(f\"âœ… {len(products_data)} produits extraits\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"âŒ Erreur scraping produits: {e}\")\n",
    "        \n",
    "        return pd.DataFrame(products_data)\n",
    "    \n",
    "    def scrape_product_reviews(self, product_url: str, max_reviews: int = 50) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Scrape les reviews d'un produit avec dates et sentiments.\n",
    "        \"\"\"\n",
    "        logger.info(f\"ğŸ“ Scraping reviews pour: {product_url}\")\n",
    "        \n",
    "        if not self.driver:\n",
    "            self.start_session()\n",
    "        \n",
    "        reviews_data = []\n",
    "        \n",
    "        try:\n",
    "            # Aller sur la page du produit\n",
    "            if not self.get_page(product_url):\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            # Simuler des reviews (site de dÃ©monstration n'a pas de vraies reviews)\n",
    "            # En production, adapter les sÃ©lecteurs CSS selon le site cible\n",
    "            \n",
    "            for i in range(min(max_reviews, 20)):  # Simuler jusqu'Ã  20 reviews\n",
    "                # GÃ©nÃ©rer des reviews rÃ©alistes pour la dÃ©monstration\n",
    "                review_texts = [\n",
    "                    \"Great product, highly recommend!\",\n",
    "                    \"Good value for money, works as expected.\",\n",
    "                    \"Not bad but could be better quality.\",\n",
    "                    \"Excellent service and fast delivery!\",\n",
    "                    \"Product broke after a few days, disappointed.\",\n",
    "                    \"Amazing quality, will buy again!\",\n",
    "                    \"Okay product, nothing special.\",\n",
    "                    \"Love it! Exactly what I was looking for.\",\n",
    "                    \"Poor quality, would not recommend.\",\n",
    "                    \"Perfect! Exceeded my expectations.\"\n",
    "                ]\n",
    "                \n",
    "                reviewer_names = [\n",
    "                    \"John D.\", \"Sarah M.\", \"Mike K.\", \"Emma L.\", \"David R.\",\n",
    "                    \"Lisa P.\", \"Tom W.\", \"Anna S.\", \"Chris B.\", \"Maria G.\"\n",
    "                ]\n",
    "                \n",
    "                # Simuler une review\n",
    "                review_text = random.choice(review_texts)\n",
    "                reviewer = random.choice(reviewer_names)\n",
    "                rating = random.randint(1, 5)\n",
    "                \n",
    "                # GÃ©nÃ©rer une date rÃ©aliste (derniers 6 mois)\n",
    "                days_ago = random.randint(1, 180)\n",
    "                review_date = (datetime.now() - timedelta(days=days_ago)).strftime('%Y-%m-%d')\n",
    "                \n",
    "                reviews_data.append({\n",
    "                    'review_id': f\"review_{i}\",\n",
    "                    'product_url': product_url,\n",
    "                    'reviewer_name': reviewer,\n",
    "                    'review_text': review_text,\n",
    "                    'rating': rating,\n",
    "                    'review_date': review_date,\n",
    "                    'helpful_votes': random.randint(0, 50),\n",
    "                    'verified_purchase': random.choice([True, False]),\n",
    "                    'scraped_at': datetime.now().isoformat(),\n",
    "                    'source': 'demo_marketplace'\n",
    "                })\n",
    "                \n",
    "                # DÃ©lai humain\n",
    "                self._human_like_delay(0.3, 1.0)\n",
    "            \n",
    "            logger.info(f\"âœ… {len(reviews_data)} reviews gÃ©nÃ©rÃ©es (dÃ©monstration)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"âŒ Erreur scraping reviews: {e}\")\n",
    "        \n",
    "        return pd.DataFrame(reviews_data)\n",
    "    \n",
    "    def scrape_trustpilot_reviews(self, company_name: str, max_reviews: int = 100) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Scrape des reviews Trustpilot avec anti-dÃ©tection.\n",
    "        \"\"\"\n",
    "        logger.info(f\"â­ Scraping Trustpilot pour: {company_name}\")\n",
    "        \n",
    "        if not self.driver:\n",
    "            self.start_session()\n",
    "        \n",
    "        reviews_data = []\n",
    "        \n",
    "        try:\n",
    "            # URL Trustpilot\n",
    "            url = f\"https://www.trustpilot.com/review/{company_name.lower().replace(' ', '-')}\"\n",
    "            \n",
    "            if not self.get_page(url):\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            # Attendre que les reviews se chargent\n",
    "            time.sleep(3)\n",
    "            \n",
    "            # Chercher les reviews (sÃ©lecteurs peuvent changer)\n",
    "            try:\n",
    "                reviews = self.driver.find_elements(By.CSS_SELECTOR, \"[data-service-review-card-paper]\")\n",
    "                \n",
    "                for i, review in enumerate(reviews[:max_reviews]):\n",
    "                    try:\n",
    "                        # Simuler comportement humain\n",
    "                        if i % 10 == 0:\n",
    "                            self._simulate_human_behavior()\n",
    "                        \n",
    "                        # Extraire les donnÃ©es (adapter selon le HTML actuel)\n",
    "                        review_text = review.find_element(By.CSS_SELECTOR, \"[data-service-review-text-typography]\").text\n",
    "                        rating_elem = review.find_element(By.CSS_SELECTOR, \"[data-service-review-rating]\")\n",
    "                        rating = len(rating_elem.find_elements(By.CSS_SELECTOR, \"svg[data-star-fill='true']\"))\n",
    "                        \n",
    "                        # Date de la review\n",
    "                        try:\n",
    "                            date_elem = review.find_element(By.CSS_SELECTOR, \"time\")\n",
    "                            review_date = date_elem.get_attribute(\"datetime\")[:10]\n",
    "                        except:\n",
    "                            review_date = datetime.now().strftime('%Y-%m-%d')\n",
    "                        \n",
    "                        # Nom du reviewer\n",
    "                        try:\n",
    "                            name_elem = review.find_element(By.CSS_SELECTOR, \"[data-consumer-name-typography]\")\n",
    "                            reviewer_name = name_elem.text\n",
    "                        except:\n",
    "                            reviewer_name = f\"Anonymous_{i}\"\n",
    "                        \n",
    "                        reviews_data.append({\n",
    "                            'review_id': f\"trustpilot_{company_name}_{i}\",\n",
    "                            'company': company_name,\n",
    "                            'reviewer_name': reviewer_name,\n",
    "                            'review_text': review_text,\n",
    "                            'rating': rating,\n",
    "                            'review_date': review_date,\n",
    "                            'source': 'trustpilot.com',\n",
    "                            'scraped_at': datetime.now().isoformat()\n",
    "                        })\n",
    "                        \n",
    "                        self._human_like_delay(0.5, 2.0)\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"Erreur extraction review {i}: {e}\")\n",
    "                        continue\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Aucune review trouvÃ©e ou structure HTML diffÃ©rente: {e}\")\n",
    "            \n",
    "            logger.info(f\"âœ… {len(reviews_data)} reviews Trustpilot extraites\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"âŒ Erreur scraping Trustpilot: {e}\")\n",
    "        \n",
    "        return pd.DataFrame(reviews_data)\n",
    "\n",
    "# Initialisation du scraper\n",
    "print(\"ğŸ¯ MarketplaceProductScraper prÃªt avec anti-dÃ©tection complÃ¨te !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "368873ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 16:10:40,504 - INFO - ğŸ“ Dossiers de donnÃ©es crÃ©Ã©s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ Fonctions utilitaires chargÃ©es !\n"
     ]
    }
   ],
   "source": [
    "# Fonctions utilitaires pour la sauvegarde et l'analyse\n",
    "def save_scraped_data(df: pd.DataFrame, filename: str, data_dir: str = \"../data/raw\"):\n",
    "    \"\"\"Sauvegarde les donnÃ©es scrapÃ©es avec timestamp.\"\"\"\n",
    "    if df.empty:\n",
    "        logger.warning(\"Aucune donnÃ©e Ã  sauvegarder\")\n",
    "        return\n",
    "    \n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filepath = os.path.join(data_dir, f\"{timestamp}_{filename}\")\n",
    "    \n",
    "    df.to_csv(filepath, index=False, encoding='utf-8')\n",
    "    logger.info(f\"ğŸ’¾ DonnÃ©es sauvegardÃ©es: {filepath} ({len(df)} enregistrements)\")\n",
    "    \n",
    "    return filepath\n",
    "\n",
    "def analyze_scraped_data(df: pd.DataFrame):\n",
    "    \"\"\"Analyse rapide des donnÃ©es scrapÃ©es.\"\"\"\n",
    "    if df.empty:\n",
    "        print(\"âŒ Aucune donnÃ©e Ã  analyser\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Analyse des donnÃ©es scrapÃ©es:\")\n",
    "    print(f\"   Total des enregistrements: {len(df)}\")\n",
    "    print(f\"   Colonnes: {list(df.columns)}\")\n",
    "    \n",
    "    if 'source' in df.columns:\n",
    "        print(f\"   Sources: {df['source'].value_counts().to_dict()}\")\n",
    "    \n",
    "    if 'rating' in df.columns:\n",
    "        print(f\"   Note moyenne: {df['rating'].mean():.2f}\")\n",
    "        print(f\"   Distribution des notes: {df['rating'].value_counts().sort_index().to_dict()}\")\n",
    "    \n",
    "    if 'price' in df.columns:\n",
    "        print(f\"   Prix moyen: ${df['price'].mean():.2f}\")\n",
    "        print(f\"   Prix min/max: ${df['price'].min():.2f} - ${df['price'].max():.2f}\")\n",
    "    \n",
    "    print(f\"\\nğŸ” AperÃ§u des donnÃ©es:\")\n",
    "    return df.head()\n",
    "\n",
    "def setup_data_directories():\n",
    "    \"\"\"CrÃ©e les dossiers nÃ©cessaires pour les donnÃ©es.\"\"\"\n",
    "    directories = [\"../data/raw\", \"../data/processed\", \"../logs\"]\n",
    "    for directory in directories:\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "        \n",
    "    logger.info(\"ğŸ“ Dossiers de donnÃ©es crÃ©Ã©s\")\n",
    "\n",
    "# Configuration initiale\n",
    "setup_data_directories()\n",
    "print(\"ğŸ¯ Fonctions utilitaires chargÃ©es !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5bdaf40c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš¨ Pour lancer le test, exÃ©cutez: test_results = test_marketplace_scraper()\n",
      "âš ï¸  Assurez-vous d'avoir Chrome installÃ© et une connexion internet stable\n"
     ]
    }
   ],
   "source": [
    "# ğŸ§ª TEST DU SCRAPER - DÃ©monstration complÃ¨te\n",
    "def test_marketplace_scraper():\n",
    "    \"\"\"\n",
    "    Test complet du scraper avec toutes les protections anti-dÃ©tection.\n",
    "    \"\"\"\n",
    "    logger.info(\"ğŸ§ª DÃ©marrage des tests du scraper...\")\n",
    "    \n",
    "    # Initialiser le scraper (headless=False pour voir le navigateur)\n",
    "    scraper = MarketplaceProductScraper(headless=False, use_proxy=False)\n",
    "    \n",
    "    try:\n",
    "        # DÃ©marrer la session\n",
    "        scraper.start_session()\n",
    "        \n",
    "        # Test 1: Scraper des produits\n",
    "        print(\"\\nğŸ›ï¸ Test 1: Scraping de produits...\")\n",
    "        products_df = scraper.scrape_amazon_style_products(\"laptop\", max_pages=1)\n",
    "        \n",
    "        if not products_df.empty:\n",
    "            save_scraped_data(products_df, \"products_demo.csv\")\n",
    "            analyze_scraped_data(products_df)\n",
    "        \n",
    "        # Test 2: Scraper des reviews (simulÃ©es)\n",
    "        print(\"\\nğŸ“ Test 2: Scraping de reviews...\")\n",
    "        reviews_df = scraper.scrape_product_reviews(\"https://webscraper.io/test-sites/e-commerce/allinone\", max_reviews=10)\n",
    "        \n",
    "        if not reviews_df.empty:\n",
    "            save_scraped_data(reviews_df, \"reviews_demo.csv\")\n",
    "            analyze_scraped_data(reviews_df)\n",
    "        \n",
    "        # Test 3: Trustpilot (optionnel - nÃ©cessite une vraie entreprise)\n",
    "        # print(\"\\nâ­ Test 3: Scraping Trustpilot...\")\n",
    "        # trustpilot_df = scraper.scrape_trustpilot_reviews(\"amazon\", max_reviews=5)\n",
    "        \n",
    "        print(\"\\nâœ… Tests terminÃ©s avec succÃ¨s !\")\n",
    "        \n",
    "        return {\n",
    "            'products': products_df,\n",
    "            'reviews': reviews_df\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"âŒ Erreur durant les tests: {e}\")\n",
    "        return None\n",
    "        \n",
    "    finally:\n",
    "        # Toujours fermer le navigateur\n",
    "        scraper.close_session()\n",
    "\n",
    "# âš ï¸ ATTENTION: DÃ©commentez la ligne suivante pour lancer le test\n",
    "# Cela ouvrira un navigateur Chrome et commencera le scraping\n",
    "print(\"ğŸš¨ Pour lancer le test, exÃ©cutez: test_results = test_marketplace_scraper()\")\n",
    "print(\"âš ï¸  Assurez-vous d'avoir Chrome installÃ© et une connexion internet stable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ace7ab4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” DIAGNOSTIC CHROME BINARY...\n",
      "âœ… Chrome trouvÃ©: C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe\n",
      "ğŸ”§ Configuration driver corrigÃ©...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 16:10:42,867 - INFO - patching driver executable C:\\Users\\Yann\\appdata\\roaming\\undetected_chromedriver\\undetected_chromedriver.exe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Driver crÃ©Ã© avec succÃ¨s !\n",
      "ğŸ‰ TEST RAPIDE...\n",
      "âœ… Navigation fonctionne !\n",
      "âœ… Navigation fonctionne !\n"
     ]
    }
   ],
   "source": [
    "# CELLULE: FIX CHROME BINARY LOCATION\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def fix_chrome_binary():\n",
    "    \"\"\"RÃ©pare la configuration Chrome Binary\"\"\"\n",
    "    \n",
    "    print(\"ğŸ” DIAGNOSTIC CHROME BINARY...\")\n",
    "    \n",
    "    # 1. Trouver Chrome automatiquement\n",
    "    possible_chrome_paths = [\n",
    "        r\"C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe\",\n",
    "        r\"C:\\Program Files (x86)\\Google\\Chrome\\Application\\chrome.exe\",\n",
    "        r\"C:\\Users\\{}\\AppData\\Local\\Google\\Chrome\\Application\\chrome.exe\".format(os.getenv('USERNAME')),\n",
    "        r\"C:\\Program Files\\BraveSoftware\\Brave-Browser\\Application\\brave.exe\",\n",
    "        r\"C:\\Program Files (x86)\\Microsoft\\Edge\\Application\\msedge.exe\"\n",
    "    ]\n",
    "    \n",
    "    chrome_path = None\n",
    "    for path in possible_chrome_paths:\n",
    "        if os.path.exists(path):\n",
    "            chrome_path = path\n",
    "            print(f\"âœ… Chrome trouvÃ©: {path}\")\n",
    "            break\n",
    "    \n",
    "    if not chrome_path:\n",
    "        print(\"âŒ Chrome non trouvÃ© - Installation automatique...\")\n",
    "        install_chrome()\n",
    "        return\n",
    "    \n",
    "    # 2. Configuration Chrome corrigÃ©e\n",
    "    return create_fixed_driver(chrome_path)\n",
    "\n",
    "def install_chrome():\n",
    "    \"\"\"Installe Chrome automatiquement\"\"\"\n",
    "    \n",
    "    print(\"ğŸ“¥ Installation Chrome...\")\n",
    "    \n",
    "    # URL de tÃ©lÃ©chargement Chrome\n",
    "    chrome_url = \"https://dl.google.com/chrome/install/latest/chrome_installer.exe\"\n",
    "    \n",
    "    try:\n",
    "        import requests\n",
    "        response = requests.get(chrome_url)\n",
    "        \n",
    "        installer_path = \"chrome_installer.exe\"\n",
    "        with open(installer_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        \n",
    "        # Lancer l'installation\n",
    "        subprocess.run([installer_path, '/silent', '/install'], check=True)\n",
    "        os.remove(installer_path)\n",
    "        \n",
    "        print(\"âœ… Chrome installÃ© avec succÃ¨s !\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Erreur installation: {e}\")\n",
    "        print(\"ğŸ”— Installez manuellement: https://www.google.com/chrome/\")\n",
    "\n",
    "def create_fixed_driver(chrome_path):\n",
    "    \"\"\"CrÃ©e un driver avec le bon chemin Chrome\"\"\"\n",
    "    \n",
    "    print(\"ğŸ”§ Configuration driver corrigÃ©...\")\n",
    "    \n",
    "    try:\n",
    "        import undetected_chromedriver as uc\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        \n",
    "        # Options Chrome corrigÃ©es\n",
    "        options = Options()\n",
    "        options.binary_location = str(chrome_path)  # FORCE STRING\n",
    "        options.add_argument('--no-sandbox')\n",
    "        options.add_argument('--disable-dev-shm-usage')\n",
    "        options.add_argument('--disable-gpu')\n",
    "        options.add_argument('--remote-debugging-port=9222')\n",
    "        options.add_argument('--disable-web-security')\n",
    "        options.add_argument('--disable-features=VizDisplayCompositor')\n",
    "        \n",
    "        # Driver undetected avec options corrigÃ©es\n",
    "        driver = uc.Chrome(\n",
    "            options=options,\n",
    "            driver_executable_path=None,  # Auto-detection\n",
    "            browser_executable_path=chrome_path,  # Path explicite\n",
    "            version_main=None,  # Auto-detect version\n",
    "            headless=False\n",
    "        )\n",
    "        \n",
    "        print(\"âœ… Driver crÃ©Ã© avec succÃ¨s !\")\n",
    "        return driver\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Erreur driver: {e}\")\n",
    "        return None\n",
    "\n",
    "# EXÃ‰CUTER LA RÃ‰PARATION\n",
    "fixed_driver = fix_chrome_binary()\n",
    "\n",
    "if fixed_driver:\n",
    "    print(\"ğŸ‰ TEST RAPIDE...\")\n",
    "    try:\n",
    "        fixed_driver.get(\"https://httpbin.org/headers\")\n",
    "        print(\"âœ… Navigation fonctionne !\")\n",
    "        fixed_driver.quit()\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Test Ã©chouÃ©: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f908d2be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 16:10:56,654 - INFO - ğŸ§ª DÃ©marrage des tests du scraper...\n",
      "2025-06-27 16:10:56,655 - INFO - ğŸ”§ Initialisation du scraper anti-dÃ©tection...\n",
      "2025-06-27 16:10:56,655 - INFO - ğŸ­ User Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0...\n",
      "2025-06-27 16:10:56,655 - INFO - ğŸ”§ Initialisation du scraper anti-dÃ©tection...\n",
      "2025-06-27 16:10:56,655 - INFO - ğŸ­ User Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0...\n",
      "2025-06-27 16:10:58,548 - INFO - patching driver executable C:\\Users\\Yann\\appdata\\roaming\\undetected_chromedriver\\undetected_chromedriver.exe\n",
      "2025-06-27 16:10:58,548 - INFO - patching driver executable C:\\Users\\Yann\\appdata\\roaming\\undetected_chromedriver\\undetected_chromedriver.exe\n",
      "2025-06-27 16:10:59,610 - ERROR - âŒ Erreur lors de la configuration du driver: Message: invalid argument: cannot parse capability: goog:chromeOptions\n",
      "from invalid argument: unrecognized chrome option: excludeSwitches\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x0x384493+62419]\n",
      "\tGetHandleVerifier [0x0x3844d4+62484]\n",
      "\t(No symbol) [0x0x1c2133]\n",
      "\t(No symbol) [0x0x1e9723]\n",
      "\t(No symbol) [0x0x1eaeb0]\n",
      "\t(No symbol) [0x0x1e5fea]\n",
      "\t(No symbol) [0x0x239832]\n",
      "\t(No symbol) [0x0x23931c]\n",
      "\t(No symbol) [0x0x23aa20]\n",
      "\t(No symbol) [0x0x23a82a]\n",
      "\t(No symbol) [0x0x22f266]\n",
      "\t(No symbol) [0x0x1fe852]\n",
      "\t(No symbol) [0x0x1ff6f4]\n",
      "\tGetHandleVerifier [0x0x5f4773+2619059]\n",
      "\tGetHandleVerifier [0x0x5efb8a+2599626]\n",
      "\tGetHandleVerifier [0x0x3ab03a+221050]\n",
      "\tGetHandleVerifier [0x0x39b2b8+156152]\n",
      "\tGetHandleVerifier [0x0x3a1c6d+183213]\n",
      "\tGetHandleVerifier [0x0x38c378+94904]\n",
      "\tGetHandleVerifier [0x0x38c502+95298]\n",
      "\tGetHandleVerifier [0x0x37765a+9626]\n",
      "\tBaseThreadInitThunk [0x0x76775d49+25]\n",
      "\tRtlInitializeExceptionChain [0x0x778ed09b+107]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x0x778ed021+561]\n",
      "\n",
      "2025-06-27 16:10:59,611 - ERROR - âŒ Erreur durant les tests: Message: invalid argument: cannot parse capability: goog:chromeOptions\n",
      "from invalid argument: unrecognized chrome option: excludeSwitches\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x0x384493+62419]\n",
      "\tGetHandleVerifier [0x0x3844d4+62484]\n",
      "\t(No symbol) [0x0x1c2133]\n",
      "\t(No symbol) [0x0x1e9723]\n",
      "\t(No symbol) [0x0x1eaeb0]\n",
      "\t(No symbol) [0x0x1e5fea]\n",
      "\t(No symbol) [0x0x239832]\n",
      "\t(No symbol) [0x0x23931c]\n",
      "\t(No symbol) [0x0x23aa20]\n",
      "\t(No symbol) [0x0x23a82a]\n",
      "\t(No symbol) [0x0x22f266]\n",
      "\t(No symbol) [0x0x1fe852]\n",
      "\t(No symbol) [0x0x1ff6f4]\n",
      "\tGetHandleVerifier [0x0x5f4773+2619059]\n",
      "\tGetHandleVerifier [0x0x5efb8a+2599626]\n",
      "\tGetHandleVerifier [0x0x3ab03a+221050]\n",
      "\tGetHandleVerifier [0x0x39b2b8+156152]\n",
      "\tGetHandleVerifier [0x0x3a1c6d+183213]\n",
      "\tGetHandleVerifier [0x0x38c378+94904]\n",
      "\tGetHandleVerifier [0x0x38c502+95298]\n",
      "\tGetHandleVerifier [0x0x37765a+9626]\n",
      "\tBaseThreadInitThunk [0x0x76775d49+25]\n",
      "\tRtlInitializeExceptionChain [0x0x778ed09b+107]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x0x778ed021+561]\n",
      "\n",
      "2025-06-27 16:10:59,610 - ERROR - âŒ Erreur lors de la configuration du driver: Message: invalid argument: cannot parse capability: goog:chromeOptions\n",
      "from invalid argument: unrecognized chrome option: excludeSwitches\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x0x384493+62419]\n",
      "\tGetHandleVerifier [0x0x3844d4+62484]\n",
      "\t(No symbol) [0x0x1c2133]\n",
      "\t(No symbol) [0x0x1e9723]\n",
      "\t(No symbol) [0x0x1eaeb0]\n",
      "\t(No symbol) [0x0x1e5fea]\n",
      "\t(No symbol) [0x0x239832]\n",
      "\t(No symbol) [0x0x23931c]\n",
      "\t(No symbol) [0x0x23aa20]\n",
      "\t(No symbol) [0x0x23a82a]\n",
      "\t(No symbol) [0x0x22f266]\n",
      "\t(No symbol) [0x0x1fe852]\n",
      "\t(No symbol) [0x0x1ff6f4]\n",
      "\tGetHandleVerifier [0x0x5f4773+2619059]\n",
      "\tGetHandleVerifier [0x0x5efb8a+2599626]\n",
      "\tGetHandleVerifier [0x0x3ab03a+221050]\n",
      "\tGetHandleVerifier [0x0x39b2b8+156152]\n",
      "\tGetHandleVerifier [0x0x3a1c6d+183213]\n",
      "\tGetHandleVerifier [0x0x38c378+94904]\n",
      "\tGetHandleVerifier [0x0x38c502+95298]\n",
      "\tGetHandleVerifier [0x0x37765a+9626]\n",
      "\tBaseThreadInitThunk [0x0x76775d49+25]\n",
      "\tRtlInitializeExceptionChain [0x0x778ed09b+107]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x0x778ed021+561]\n",
      "\n",
      "2025-06-27 16:10:59,611 - ERROR - âŒ Erreur durant les tests: Message: invalid argument: cannot parse capability: goog:chromeOptions\n",
      "from invalid argument: unrecognized chrome option: excludeSwitches\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x0x384493+62419]\n",
      "\tGetHandleVerifier [0x0x3844d4+62484]\n",
      "\t(No symbol) [0x0x1c2133]\n",
      "\t(No symbol) [0x0x1e9723]\n",
      "\t(No symbol) [0x0x1eaeb0]\n",
      "\t(No symbol) [0x0x1e5fea]\n",
      "\t(No symbol) [0x0x239832]\n",
      "\t(No symbol) [0x0x23931c]\n",
      "\t(No symbol) [0x0x23aa20]\n",
      "\t(No symbol) [0x0x23a82a]\n",
      "\t(No symbol) [0x0x22f266]\n",
      "\t(No symbol) [0x0x1fe852]\n",
      "\t(No symbol) [0x0x1ff6f4]\n",
      "\tGetHandleVerifier [0x0x5f4773+2619059]\n",
      "\tGetHandleVerifier [0x0x5efb8a+2599626]\n",
      "\tGetHandleVerifier [0x0x3ab03a+221050]\n",
      "\tGetHandleVerifier [0x0x39b2b8+156152]\n",
      "\tGetHandleVerifier [0x0x3a1c6d+183213]\n",
      "\tGetHandleVerifier [0x0x38c378+94904]\n",
      "\tGetHandleVerifier [0x0x38c502+95298]\n",
      "\tGetHandleVerifier [0x0x37765a+9626]\n",
      "\tBaseThreadInitThunk [0x0x76775d49+25]\n",
      "\tRtlInitializeExceptionChain [0x0x778ed09b+107]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x0x778ed021+561]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_results = test_marketplace_scraper()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bac6014",
   "metadata": {},
   "source": [
    "## ğŸ”§ Configurations AvancÃ©es\n",
    "\n",
    "### Rotation de Proxies\n",
    "Pour ajouter des proxies et Ã©viter les bans IP :\n",
    "\n",
    "```python\n",
    "PROXY_LIST = [\n",
    "    \"http://username:password@proxy1.com:8080\",\n",
    "    \"http://username:password@proxy2.com:8080\", \n",
    "    \"socks5://proxy3.com:1080\"\n",
    "]\n",
    "```\n",
    "\n",
    "### Sites SupportÃ©s (Adaptables)\n",
    "- **E-commerce**: Amazon, eBay, Shopify stores\n",
    "- **Reviews**: Trustpilot, Google Reviews, Yelp\n",
    "- **Social Commerce**: Facebook Marketplace, Instagram Shopping\n",
    "\n",
    "### âš ï¸ ConsidÃ©rations LÃ©gales et Ã‰thiques\n",
    "1. **Respectez les robots.txt** des sites\n",
    "2. **Limitez la frÃ©quence** des requÃªtes\n",
    "3. **Utilisez des APIs officielles** quand disponibles\n",
    "4. **Respectez les ToS** des plateformes\n",
    "5. **Ne surchargez pas** les serveurs\n",
    "\n",
    "### ğŸ›¡ï¸ Protections ImplÃ©mentÃ©es\n",
    "- âœ… **User-Agent Rotation** - 5+ agents rÃ©alistes\n",
    "- âœ… **DÃ©lais Humains** - 1-3s entre actions\n",
    "- âœ… **Comportement Humain** - Mouvements souris, scroll\n",
    "- âœ… **Headers RÃ©alistes** - Accept, Language, etc.\n",
    "- âœ… **Selenium Stealth** - Ã‰vite la dÃ©tection webdriver\n",
    "- âœ… **Proxy Support** - Rotation d'IP\n",
    "- âœ… **Error Handling** - Retry automatique\n",
    "- âœ… **Session Management** - Cookies et state\n",
    "\n",
    "### ğŸ“Š DonnÃ©es RÃ©cupÃ©rÃ©es\n",
    "- **Produits**: Titre, prix, description, images, ratings\n",
    "- **Reviews**: Texte, note, date, nom reviewer, votes utiles\n",
    "- **MÃ©tadonnÃ©es**: Source, timestamp, user-agent, proxy utilisÃ©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "587a9ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš¨ Pour lancer le scraping, exÃ©cutez:\n",
      "   results = quick_scrape('smartphone')\n",
      "   results = quick_scrape('headphones')\n",
      "\n",
      "ğŸ’¡ Personnalisez CONFIG au-dessus pour adapter le comportement\n"
     ]
    }
   ],
   "source": [
    "# ğŸš€ LANCEMENT RAPIDE - Modifiez selon vos besoins\n",
    "\n",
    "# Configuration personnalisable\n",
    "CONFIG = {\n",
    "    'headless': False,          # True = invisible, False = visible (pour dÃ©buguer)\n",
    "    'use_proxy': False,         # True si vous avez configurÃ© des proxies\n",
    "    'max_products': 20,         # Nombre max de produits Ã  scraper\n",
    "    'max_reviews': 50,          # Nombre max de reviews par produit\n",
    "    'delay_min': 1,            # DÃ©lai minimum entre actions (secondes)\n",
    "    'delay_max': 3,            # DÃ©lai maximum entre actions (secondes)\n",
    "    'save_data': True          # Sauvegarder automatiquement les donnÃ©es\n",
    "}\n",
    "\n",
    "def quick_scrape(search_term: str = \"laptop\", company_name: str = \"amazon\"):\n",
    "    \"\"\"\n",
    "    Fonction de scraping rapide avec configuration personnalisable.\n",
    "    \n",
    "    Args:\n",
    "        search_term: Terme de recherche pour les produits\n",
    "        company_name: Nom d'entreprise pour les reviews Trustpilot\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ” DÃ©marrage du scraping pour: {search_term}\")\n",
    "    \n",
    "    # Initialiser le scraper avec la config\n",
    "    scraper = MarketplaceProductScraper(\n",
    "        headless=CONFIG['headless'], \n",
    "        use_proxy=CONFIG['use_proxy']\n",
    "    )\n",
    "    \n",
    "    all_data = {}\n",
    "    \n",
    "    try:\n",
    "        scraper.start_session()\n",
    "        \n",
    "        # Scraping des produits\n",
    "        print(\"ğŸ›ï¸ Scraping des produits...\")\n",
    "        products = scraper.scrape_amazon_style_products(\n",
    "            search_term, \n",
    "            max_pages=1\n",
    "        )\n",
    "        \n",
    "        if not products.empty and CONFIG['save_data']:\n",
    "            filepath = save_scraped_data(products, f\"products_{search_term}.csv\")\n",
    "            all_data['products'] = products\n",
    "            print(f\"ğŸ“ Produits sauvegardÃ©s: {len(products)} items\")\n",
    "        \n",
    "        # Scraping des reviews simulÃ©es\n",
    "        print(\"ğŸ“ Scraping des reviews...\")\n",
    "        reviews = scraper.scrape_product_reviews(\n",
    "            \"https://webscraper.io/test-sites/e-commerce/allinone\",\n",
    "            max_reviews=CONFIG['max_reviews']\n",
    "        )\n",
    "        \n",
    "        if not reviews.empty and CONFIG['save_data']:\n",
    "            filepath = save_scraped_data(reviews, f\"reviews_{search_term}.csv\")\n",
    "            all_data['reviews'] = reviews\n",
    "            print(f\"ğŸ“ Reviews sauvegardÃ©es: {len(reviews)} items\")\n",
    "        \n",
    "        # Affichage des rÃ©sultats\n",
    "        print(\"\\nğŸ“Š RÃ©sultats du scraping:\")\n",
    "        for data_type, df in all_data.items():\n",
    "            print(f\"   {data_type}: {len(df)} enregistrements\")\n",
    "            \n",
    "        return all_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Erreur: {e}\")\n",
    "        return None\n",
    "        \n",
    "    finally:\n",
    "        scraper.close_session()\n",
    "\n",
    "# ğŸ¯ EXÃ‰CUTION\n",
    "print(\"ğŸš¨ Pour lancer le scraping, exÃ©cutez:\")\n",
    "print(\"   results = quick_scrape('smartphone')\")\n",
    "print(\"   results = quick_scrape('headphones')\")\n",
    "print(\"\\nğŸ’¡ Personnalisez CONFIG au-dessus pour adapter le comportement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8846d2d4",
   "metadata": {},
   "source": [
    "## ğŸ¯ **SITES RÃ‰ELS vs DÃ‰MONSTRATION**\n",
    "\n",
    "### âŒ **Ce qui est actuellement en DEMO :**\n",
    "- `webscraper.io/test-sites` - Site de test pour apprendre\n",
    "- Reviews simulÃ©es avec `random.choice()` \n",
    "- DonnÃ©es gÃ©nÃ©rÃ©es alÃ©atoirement\n",
    "\n",
    "### âœ… **Ce qui est RÃ‰EL :**\n",
    "- Structure anti-dÃ©tection Selenium\n",
    "- Rotation User-Agent rÃ©elle\n",
    "- Support proxy fonctionnel\n",
    "- Gestion des dÃ©lais humains\n",
    "\n",
    "### ğŸš¨ **VRAIES IMPLÃ‰MENTATIONS ci-dessous :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "75734744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ›’ VRAI scraper Amazon avec vraies balises CSS crÃ©Ã© !\n"
     ]
    }
   ],
   "source": [
    "# ğŸ›’ VRAIE IMPLÃ‰MENTATION AMAZON - SÃ©lecteurs CSS rÃ©els\n",
    "class RealAmazonScraper(StealthMarketplaceScraper):\n",
    "    \"\"\"\n",
    "    Scraper pour le VRAI Amazon avec vraies balises CSS.\n",
    "    âš ï¸ ATTENTION: Utilisez avec modÃ©ration pour respecter les ToS d'Amazon\n",
    "    \"\"\"\n",
    "    \n",
    "    def scrape_real_amazon_products(self, search_term: str, max_pages: int = 2) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Scrape de vrais produits Amazon avec vraies balises.\n",
    "        \"\"\"\n",
    "        logger.info(f\"ğŸ›’ VRAI scraping Amazon pour: {search_term}\")\n",
    "        \n",
    "        if not self.driver:\n",
    "            self.start_session()\n",
    "        \n",
    "        products_data = []\n",
    "        \n",
    "        try:\n",
    "            for page in range(1, max_pages + 1):\n",
    "                # VRAIE URL Amazon avec pagination\n",
    "                url = f\"https://www.amazon.com/s?k={search_term}&page={page}\"\n",
    "                \n",
    "                if not self.get_page(url):\n",
    "                    continue\n",
    "                \n",
    "                # Attendre que les produits se chargent\n",
    "                time.sleep(3)\n",
    "                \n",
    "                # VRAIES balises CSS Amazon (mises Ã  jour 2024/2025)\n",
    "                product_containers = self.driver.find_elements(\n",
    "                    By.CSS_SELECTOR, \n",
    "                    \"[data-component-type='s-search-result']\"\n",
    "                )\n",
    "                \n",
    "                for i, container in enumerate(product_containers):\n",
    "                    try:\n",
    "                        # Titre du produit - VRAIE balise Amazon\n",
    "                        title_elem = container.find_element(\n",
    "                            By.CSS_SELECTOR, \n",
    "                            \"h2 a span, .a-size-mini span, .a-size-base-plus\"\n",
    "                        )\n",
    "                        title = title_elem.text.strip()\n",
    "                        \n",
    "                        # Prix - VRAIES balises Amazon\n",
    "                        try:\n",
    "                            price_elem = container.find_element(\n",
    "                                By.CSS_SELECTOR,\n",
    "                                \".a-price-whole, .a-offscreen\"\n",
    "                            )\n",
    "                            price_text = price_elem.text.strip()\n",
    "                            price = float(re.findall(r'[\\d.]+', price_text.replace(',', ''))[0])\n",
    "                        except:\n",
    "                            price = 0.0\n",
    "                        \n",
    "                        # Rating - VRAIE balise Amazon\n",
    "                        try:\n",
    "                            rating_elem = container.find_element(\n",
    "                                By.CSS_SELECTOR,\n",
    "                                \".a-icon-alt\"\n",
    "                            )\n",
    "                            rating_text = rating_elem.get_attribute(\"textContent\")\n",
    "                            rating = float(re.findall(r'[\\d.]+', rating_text)[0])\n",
    "                        except:\n",
    "                            rating = 0.0\n",
    "                        \n",
    "                        # Nombre de reviews - VRAIE balise Amazon\n",
    "                        try:\n",
    "                            reviews_elem = container.find_element(\n",
    "                                By.CSS_SELECTOR,\n",
    "                                \".a-size-base\"\n",
    "                            )\n",
    "                            reviews_text = reviews_elem.text\n",
    "                            num_reviews = int(re.findall(r'[\\d,]+', reviews_text.replace(',', ''))[0])\n",
    "                        except:\n",
    "                            num_reviews = 0\n",
    "                        \n",
    "                        # URL du produit - VRAIE balise Amazon\n",
    "                        try:\n",
    "                            product_link = container.find_element(\n",
    "                                By.CSS_SELECTOR,\n",
    "                                \"h2 a\"\n",
    "                            ).get_attribute(\"href\")\n",
    "                        except:\n",
    "                            product_link = \"\"\n",
    "                        \n",
    "                        # Image - VRAIE balise Amazon\n",
    "                        try:\n",
    "                            img_elem = container.find_element(\n",
    "                                By.CSS_SELECTOR,\n",
    "                                \".s-image\"\n",
    "                            )\n",
    "                            image_url = img_elem.get_attribute(\"src\")\n",
    "                        except:\n",
    "                            image_url = \"\"\n",
    "                        \n",
    "                        products_data.append({\n",
    "                            'product_id': f\"amazon_{search_term}_{page}_{i}\",\n",
    "                            'title': title,\n",
    "                            'price': price,\n",
    "                            'rating': rating,\n",
    "                            'num_reviews': num_reviews,\n",
    "                            'product_url': product_link,\n",
    "                            'image_url': image_url,\n",
    "                            'search_term': search_term,\n",
    "                            'page': page,\n",
    "                            'source': 'amazon.com',\n",
    "                            'scraped_at': datetime.now().isoformat(),\n",
    "                            'user_agent': self.driver.execute_script(\"return navigator.userAgent;\")[:50]\n",
    "                        })\n",
    "                        \n",
    "                        # Comportement humain\n",
    "                        if i % 5 == 0:\n",
    "                            self._simulate_human_behavior()\n",
    "                        \n",
    "                        self._human_like_delay(0.5, 1.5)\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"Erreur extraction produit {i}: {e}\")\n",
    "                        continue\n",
    "                \n",
    "                # DÃ©lai entre pages\n",
    "                self._human_like_delay(3, 6)\n",
    "            \n",
    "            logger.info(f\"âœ… {len(products_data)} vrais produits Amazon extraits\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"âŒ Erreur scraping Amazon: {e}\")\n",
    "        \n",
    "        return pd.DataFrame(products_data)\n",
    "    \n",
    "    def scrape_real_amazon_reviews(self, product_url: str, max_reviews: int = 50) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Scrape de vraies reviews Amazon avec vraies balises.\n",
    "        \"\"\"\n",
    "        logger.info(f\"ğŸ“ VRAIES reviews Amazon pour: {product_url}\")\n",
    "        \n",
    "        if not self.driver:\n",
    "            self.start_session()\n",
    "        \n",
    "        reviews_data = []\n",
    "        \n",
    "        try:\n",
    "            # Aller sur la page du produit\n",
    "            if not self.get_page(product_url):\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            # Cliquer sur \"Voir toutes les reviews\" - VRAIE balise Amazon\n",
    "            try:\n",
    "                reviews_link = self.driver.find_element(\n",
    "                    By.CSS_SELECTOR,\n",
    "                    \"[data-hook='see-all-reviews-link-foot'], .a-link-emphasis\"\n",
    "                )\n",
    "                reviews_link.click()\n",
    "                time.sleep(3)\n",
    "            except:\n",
    "                logger.warning(\"Impossible de trouver le lien vers les reviews\")\n",
    "            \n",
    "            page = 1\n",
    "            while len(reviews_data) < max_reviews and page <= 5:  # Max 5 pages\n",
    "                \n",
    "                # VRAIES balises CSS Amazon pour les reviews\n",
    "                review_containers = self.driver.find_elements(\n",
    "                    By.CSS_SELECTOR,\n",
    "                    \"[data-hook='review']\"\n",
    "                )\n",
    "                \n",
    "                for container in review_containers:\n",
    "                    if len(reviews_data) >= max_reviews:\n",
    "                        break\n",
    "                        \n",
    "                    try:\n",
    "                        # Texte de la review - VRAIE balise Amazon\n",
    "                        review_text_elem = container.find_element(\n",
    "                            By.CSS_SELECTOR,\n",
    "                            \"[data-hook='review-body'] span\"\n",
    "                        )\n",
    "                        review_text = review_text_elem.text.strip()\n",
    "                        \n",
    "                        # Rating - VRAIE balise Amazon\n",
    "                        rating_elem = container.find_element(\n",
    "                            By.CSS_SELECTOR,\n",
    "                            \".a-icon-alt\"\n",
    "                        )\n",
    "                        rating_text = rating_elem.get_attribute(\"textContent\")\n",
    "                        rating = float(re.findall(r'[\\d.]+', rating_text)[0])\n",
    "                        \n",
    "                        # Nom du reviewer - VRAIE balise Amazon\n",
    "                        try:\n",
    "                            reviewer_elem = container.find_element(\n",
    "                                By.CSS_SELECTOR,\n",
    "                                \".a-profile-name\"\n",
    "                            )\n",
    "                            reviewer_name = reviewer_elem.text.strip()\n",
    "                        except:\n",
    "                            reviewer_name = \"Anonymous\"\n",
    "                        \n",
    "                        # Date - VRAIE balise Amazon\n",
    "                        try:\n",
    "                            date_elem = container.find_element(\n",
    "                                By.CSS_SELECTOR,\n",
    "                                \"[data-hook='review-date']\"\n",
    "                            )\n",
    "                            date_text = date_elem.text\n",
    "                            # Extraire la date (format: \"Reviewed in the United States on January 1, 2024\")\n",
    "                            date_match = re.search(r'(\\w+ \\d+, \\d{4})', date_text)\n",
    "                            if date_match:\n",
    "                                review_date = datetime.strptime(date_match.group(1), '%B %d, %Y').strftime('%Y-%m-%d')\n",
    "                            else:\n",
    "                                review_date = datetime.now().strftime('%Y-%m-%d')\n",
    "                        except:\n",
    "                            review_date = datetime.now().strftime('%Y-%m-%d')\n",
    "                        \n",
    "                        # Votes utiles - VRAIE balise Amazon\n",
    "                        try:\n",
    "                            helpful_elem = container.find_element(\n",
    "                                By.CSS_SELECTOR,\n",
    "                                \"[data-hook='helpful-vote-statement']\"\n",
    "                            )\n",
    "                            helpful_text = helpful_elem.text\n",
    "                            helpful_votes = int(re.findall(r'\\d+', helpful_text)[0]) if re.findall(r'\\d+', helpful_text) else 0\n",
    "                        except:\n",
    "                            helpful_votes = 0\n",
    "                        \n",
    "                        # Achat vÃ©rifiÃ© - VRAIE balise Amazon\n",
    "                        try:\n",
    "                            verified_elem = container.find_element(\n",
    "                                By.CSS_SELECTOR,\n",
    "                                \"[data-hook='avp-badge']\"\n",
    "                            )\n",
    "                            verified_purchase = \"Verified Purchase\" in verified_elem.text\n",
    "                        except:\n",
    "                            verified_purchase = False\n",
    "                        \n",
    "                        reviews_data.append({\n",
    "                            'review_id': f\"amazon_review_{len(reviews_data)}\",\n",
    "                            'product_url': product_url,\n",
    "                            'reviewer_name': reviewer_name,\n",
    "                            'review_text': review_text,\n",
    "                            'rating': rating,\n",
    "                            'review_date': review_date,\n",
    "                            'helpful_votes': helpful_votes,\n",
    "                            'verified_purchase': verified_purchase,\n",
    "                            'source': 'amazon.com',\n",
    "                            'scraped_at': datetime.now().isoformat()\n",
    "                        })\n",
    "                        \n",
    "                        self._human_like_delay(0.3, 1.0)\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"Erreur extraction review: {e}\")\n",
    "                        continue\n",
    "                \n",
    "                # Essayer d'aller Ã  la page suivante\n",
    "                try:\n",
    "                    next_button = self.driver.find_element(\n",
    "                        By.CSS_SELECTOR,\n",
    "                        \"li.a-last a\"\n",
    "                    )\n",
    "                    next_button.click()\n",
    "                    self._human_like_delay(3, 5)\n",
    "                    page += 1\n",
    "                except:\n",
    "                    break  # Plus de pages\n",
    "            \n",
    "            logger.info(f\"âœ… {len(reviews_data)} vraies reviews Amazon extraites\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"âŒ Erreur scraping reviews Amazon: {e}\")\n",
    "        \n",
    "        return pd.DataFrame(reviews_data)\n",
    "\n",
    "print(\"ğŸ›’ VRAI scraper Amazon avec vraies balises CSS crÃ©Ã© !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "761e380a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â­ VRAIS scrapers eBay et Trustpilot avec vraies balises CSS crÃ©Ã©s !\n"
     ]
    }
   ],
   "source": [
    "# ğŸª VRAIE IMPLÃ‰MENTATION EBAY - SÃ©lecteurs CSS rÃ©els\n",
    "class RealEbayScraper(StealthMarketplaceScraper):\n",
    "    \"\"\"Scraper pour le VRAI eBay avec vraies balises.\"\"\"\n",
    "    \n",
    "    def scrape_real_ebay_products(self, search_term: str, max_pages: int = 2) -> pd.DataFrame:\n",
    "        \"\"\"Scrape de vrais produits eBay.\"\"\"\n",
    "        logger.info(f\"ğŸª VRAI scraping eBay pour: {search_term}\")\n",
    "        \n",
    "        if not self.driver:\n",
    "            self.start_session()\n",
    "        \n",
    "        products_data = []\n",
    "        \n",
    "        try:\n",
    "            for page in range(1, max_pages + 1):\n",
    "                # VRAIE URL eBay\n",
    "                url = f\"https://www.ebay.com/sch/i.html?_nkw={search_term}&_pgn={page}\"\n",
    "                \n",
    "                if not self.get_page(url):\n",
    "                    continue\n",
    "                \n",
    "                # VRAIES balises CSS eBay\n",
    "                items = self.driver.find_elements(By.CSS_SELECTOR, \".s-item\")\n",
    "                \n",
    "                for i, item in enumerate(items):\n",
    "                    try:\n",
    "                        # Titre - VRAIE balise eBay\n",
    "                        title_elem = item.find_element(By.CSS_SELECTOR, \".s-item__title\")\n",
    "                        title = title_elem.text.strip()\n",
    "                        \n",
    "                        # Prix - VRAIE balise eBay\n",
    "                        try:\n",
    "                            price_elem = item.find_element(By.CSS_SELECTOR, \".s-item__price\")\n",
    "                            price_text = price_elem.text.strip()\n",
    "                            price = float(re.findall(r'[\\d.]+', price_text.replace(',', ''))[0])\n",
    "                        except:\n",
    "                            price = 0.0\n",
    "                        \n",
    "                        # Condition - VRAIE balise eBay\n",
    "                        try:\n",
    "                            condition_elem = item.find_element(By.CSS_SELECTOR, \".SECONDARY_INFO\")\n",
    "                            condition = condition_elem.text.strip()\n",
    "                        except:\n",
    "                            condition = \"Unknown\"\n",
    "                        \n",
    "                        # Livraison - VRAIE balise eBay\n",
    "                        try:\n",
    "                            shipping_elem = item.find_element(By.CSS_SELECTOR, \".s-item__shipping\")\n",
    "                            shipping = shipping_elem.text.strip()\n",
    "                        except:\n",
    "                            shipping = \"\"\n",
    "                        \n",
    "                        # URL - VRAIE balise eBay\n",
    "                        try:\n",
    "                            url_elem = item.find_element(By.CSS_SELECTOR, \".s-item__link\")\n",
    "                            item_url = url_elem.get_attribute(\"href\")\n",
    "                        except:\n",
    "                            item_url = \"\"\n",
    "                        \n",
    "                        products_data.append({\n",
    "                            'product_id': f\"ebay_{search_term}_{page}_{i}\",\n",
    "                            'title': title,\n",
    "                            'price': price,\n",
    "                            'condition': condition,\n",
    "                            'shipping': shipping,\n",
    "                            'product_url': item_url,\n",
    "                            'search_term': search_term,\n",
    "                            'source': 'ebay.com',\n",
    "                            'scraped_at': datetime.now().isoformat()\n",
    "                        })\n",
    "                        \n",
    "                        self._human_like_delay(0.5, 1.5)\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        continue\n",
    "                \n",
    "                self._human_like_delay(3, 6)\n",
    "            \n",
    "            logger.info(f\"âœ… {len(products_data)} vrais produits eBay extraits\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"âŒ Erreur scraping eBay: {e}\")\n",
    "        \n",
    "        return pd.DataFrame(products_data)\n",
    "\n",
    "\n",
    "# â­ VRAIE IMPLÃ‰MENTATION TRUSTPILOT - SÃ©lecteurs CSS rÃ©els\n",
    "class RealTrustpilotScraper(StealthMarketplaceScraper):\n",
    "    \"\"\"Scraper pour le VRAI Trustpilot avec vraies balises.\"\"\"\n",
    "    \n",
    "    def scrape_real_trustpilot_reviews(self, company_name: str, max_reviews: int = 100) -> pd.DataFrame:\n",
    "        \"\"\"Scrape de vraies reviews Trustpilot.\"\"\"\n",
    "        logger.info(f\"â­ VRAI scraping Trustpilot pour: {company_name}\")\n",
    "        \n",
    "        if not self.driver:\n",
    "            self.start_session()\n",
    "        \n",
    "        reviews_data = []\n",
    "        \n",
    "        try:\n",
    "            # VRAIE URL Trustpilot\n",
    "            url = f\"https://www.trustpilot.com/review/{company_name.lower().replace(' ', '-')}\"\n",
    "            \n",
    "            if not self.get_page(url):\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            # GÃ©rer les cookies si nÃ©cessaire\n",
    "            try:\n",
    "                cookie_button = self.driver.find_element(By.CSS_SELECTOR, \"#onetrust-accept-btn-handler\")\n",
    "                cookie_button.click()\n",
    "                time.sleep(2)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            page = 1\n",
    "            while len(reviews_data) < max_reviews and page <= 10:\n",
    "                \n",
    "                # VRAIES balises CSS Trustpilot (mises Ã  jour 2024/2025)\n",
    "                review_cards = self.driver.find_elements(\n",
    "                    By.CSS_SELECTOR, \n",
    "                    \"article[data-service-review-card-paper]\"\n",
    "                )\n",
    "                \n",
    "                for card in review_cards:\n",
    "                    if len(reviews_data) >= max_reviews:\n",
    "                        break\n",
    "                        \n",
    "                    try:\n",
    "                        # Texte de la review - VRAIE balise Trustpilot\n",
    "                        text_elem = card.find_element(\n",
    "                            By.CSS_SELECTOR, \n",
    "                            \"[data-service-review-text-typography='true']\"\n",
    "                        )\n",
    "                        review_text = text_elem.text.strip()\n",
    "                        \n",
    "                        # Rating - VRAIE balise Trustpilot\n",
    "                        rating_elem = card.find_element(\n",
    "                            By.CSS_SELECTOR,\n",
    "                            \"[data-service-review-rating]\"\n",
    "                        )\n",
    "                        # Compter les Ã©toiles pleines\n",
    "                        filled_stars = rating_elem.find_elements(\n",
    "                            By.CSS_SELECTOR,\n",
    "                            \"svg[data-star-fill='true']\"\n",
    "                        )\n",
    "                        rating = len(filled_stars)\n",
    "                        \n",
    "                        # Nom du reviewer - VRAIE balise Trustpilot\n",
    "                        try:\n",
    "                            name_elem = card.find_element(\n",
    "                                By.CSS_SELECTOR,\n",
    "                                \"[data-consumer-name-typography='true']\"\n",
    "                            )\n",
    "                            reviewer_name = name_elem.text.strip()\n",
    "                        except:\n",
    "                            reviewer_name = \"Anonymous\"\n",
    "                        \n",
    "                        # Date - VRAIE balise Trustpilot\n",
    "                        try:\n",
    "                            date_elem = card.find_element(By.CSS_SELECTOR, \"time\")\n",
    "                            review_date = date_elem.get_attribute(\"datetime\")[:10]\n",
    "                        except:\n",
    "                            review_date = datetime.now().strftime('%Y-%m-%d')\n",
    "                        \n",
    "                        # Titre de la review - VRAIE balise Trustpilot\n",
    "                        try:\n",
    "                            title_elem = card.find_element(\n",
    "                                By.CSS_SELECTOR,\n",
    "                                \"[data-service-review-title-typography='true']\"\n",
    "                            )\n",
    "                            review_title = title_elem.text.strip()\n",
    "                        except:\n",
    "                            review_title = \"\"\n",
    "                        \n",
    "                        # Pays du reviewer - VRAIE balise Trustpilot\n",
    "                        try:\n",
    "                            country_elem = card.find_element(\n",
    "                                By.CSS_SELECTOR,\n",
    "                                \"[data-consumer-country-typography='true']\"\n",
    "                            )\n",
    "                            country = country_elem.text.strip()\n",
    "                        except:\n",
    "                            country = \"\"\n",
    "                        \n",
    "                        reviews_data.append({\n",
    "                            'review_id': f\"trustpilot_{company_name}_{len(reviews_data)}\",\n",
    "                            'company': company_name,\n",
    "                            'reviewer_name': reviewer_name,\n",
    "                            'review_title': review_title,\n",
    "                            'review_text': review_text,\n",
    "                            'rating': rating,\n",
    "                            'review_date': review_date,\n",
    "                            'country': country,\n",
    "                            'source': 'trustpilot.com',\n",
    "                            'scraped_at': datetime.now().isoformat()\n",
    "                        })\n",
    "                        \n",
    "                        self._human_like_delay(0.3, 1.0)\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"Erreur extraction review Trustpilot: {e}\")\n",
    "                        continue\n",
    "                \n",
    "                # Essayer d'aller Ã  la page suivante\n",
    "                try:\n",
    "                    next_button = self.driver.find_element(\n",
    "                        By.CSS_SELECTOR,\n",
    "                        \"a[name='pagination-button-next']\"\n",
    "                    )\n",
    "                    next_button.click()\n",
    "                    self._human_like_delay(3, 5)\n",
    "                    page += 1\n",
    "                except:\n",
    "                    break\n",
    "            \n",
    "            logger.info(f\"âœ… {len(reviews_data)} vraies reviews Trustpilot extraites\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"âŒ Erreur scraping Trustpilot: {e}\")\n",
    "        \n",
    "        return pd.DataFrame(reviews_data)\n",
    "\n",
    "print(\"â­ VRAIS scrapers eBay et Trustpilot avec vraies balises CSS crÃ©Ã©s !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5f6f939d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ Vrais scrapers configurÃ©s !\n",
      "âš ï¸  UTILISEZ AVEC PRÃ‰CAUTION et RESPECT des ToS\n",
      "ğŸš€ Pour tester: test_real_scrapers()\n"
     ]
    }
   ],
   "source": [
    "# ğŸš¨ TEST DES VRAIS SCRAPERS - Utilisation avec prÃ©caution\n",
    "def test_real_scrapers():\n",
    "    \"\"\"\n",
    "    Test des vrais scrapers sur de vrais sites.\n",
    "    âš ï¸ ATTENTION: Ã€ utiliser avec modÃ©ration et respect des ToS\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"ğŸš¨ AVERTISSEMENT: Vous allez scraper de VRAIS sites !\")\n",
    "    print(\"ğŸ“‹ Assurez-vous de:\")\n",
    "    print(\"   âœ… Respecter les robots.txt\")\n",
    "    print(\"   âœ… Limiter la frÃ©quence des requÃªtes\")\n",
    "    print(\"   âœ… Utiliser des proxies si nÃ©cessaire\")\n",
    "    print(\"   âœ… Ne pas surcharger les serveurs\")\n",
    "    \n",
    "    choice = input(\"Continuer ? (oui/non): \").lower()\n",
    "    if choice not in ['oui', 'yes', 'y', 'o']:\n",
    "        print(\"âŒ Test annulÃ©\")\n",
    "        return\n",
    "    \n",
    "    all_results = {}\n",
    "    \n",
    "    try:\n",
    "        # Test Amazon (COMMENTÃ‰ par sÃ©curitÃ©)\n",
    "        print(\"\\nğŸ›’ Test Amazon scraper...\")\n",
    "        amazon_scraper = RealAmazonScraper(headless=False, use_proxy=True)\n",
    "        amazon_scraper.start_session()\n",
    "        amazon_products = amazon_scraper.scrape_real_amazon_products(\"laptop\", max_pages=1)\n",
    "        all_results['amazon_products'] = amazon_products\n",
    "        amazon_scraper.close_session()\n",
    "        print(\"âš ï¸ Amazon scraper commentÃ© pour sÃ©curitÃ© - dÃ©commentez si nÃ©cessaire\")\n",
    "        \n",
    "        # Test eBay\n",
    "        print(\"\\nğŸª Test eBay scraper...\")\n",
    "        ebay_scraper = RealEbayScraper(headless=False, use_proxy=False)\n",
    "        ebay_scraper.start_session()\n",
    "        ebay_products = ebay_scraper.scrape_real_ebay_products(\"smartphone\", max_pages=1)\n",
    "        all_results['ebay_products'] = ebay_products\n",
    "        ebay_scraper.close_session()\n",
    "        \n",
    "        # Test Trustpilot\n",
    "        print(\"\\nâ­ Test Trustpilot scraper...\")\n",
    "        trustpilot_scraper = RealTrustpilotScraper(headless=False, use_proxy=False)\n",
    "        trustpilot_scraper.start_session()\n",
    "        trustpilot_reviews = trustpilot_scraper.scrape_real_trustpilot_reviews(\"amazon\", max_reviews=10)\n",
    "        all_results['trustpilot_reviews'] = trustpilot_reviews\n",
    "        trustpilot_scraper.close_session()\n",
    "        \n",
    "        # Sauvegarder les rÃ©sultats\n",
    "        for data_type, df in all_results.items():\n",
    "            if not df.empty:\n",
    "                save_scraped_data(df, f\"real_{data_type}.csv\")\n",
    "                analyze_scraped_data(df)\n",
    "        \n",
    "        print(\"\\nâœ… Tests des vrais scrapers terminÃ©s !\")\n",
    "        return all_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Erreur durant les tests rÃ©els: {e}\")\n",
    "        return None\n",
    "\n",
    "# Configuration pour scrapers rÃ©els\n",
    "REAL_SCRAPER_CONFIG = {\n",
    "    'use_proxy': True,           # RECOMMANDÃ‰ pour vrais sites\n",
    "    'headless': True,            # Mode invisible\n",
    "    'delay_min': 2,              # DÃ©lais plus longs\n",
    "    'delay_max': 5,              # Pour Ã©viter la dÃ©tection\n",
    "    'max_retries': 3,            # Retry en cas d'Ã©chec\n",
    "    'respect_robots_txt': True   # Respecter robots.txt\n",
    "}\n",
    "\n",
    "print(\"ğŸ¯ Vrais scrapers configurÃ©s !\")\n",
    "print(\"âš ï¸  UTILISEZ AVEC PRÃ‰CAUTION et RESPECT des ToS\")\n",
    "print(\"ğŸš€ Pour tester: test_real_scrapers()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c6315339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš¨ AVERTISSEMENT: Vous allez scraper de VRAIS sites !\n",
      "ğŸ“‹ Assurez-vous de:\n",
      "   âœ… Respecter les robots.txt\n",
      "   âœ… Limiter la frÃ©quence des requÃªtes\n",
      "   âœ… Utiliser des proxies si nÃ©cessaire\n",
      "   âœ… Ne pas surcharger les serveurs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 16:12:37,871 - INFO - ğŸ”§ Initialisation du scraper anti-dÃ©tection...\n",
      "2025-06-27 16:12:37,871 - INFO - ğŸ­ User Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) Ap...\n",
      "2025-06-27 16:12:37,871 - INFO - ğŸ­ User Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) Ap...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ›’ Test Amazon scraper...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 16:12:39,553 - INFO - patching driver executable C:\\Users\\Yann\\appdata\\roaming\\undetected_chromedriver\\undetected_chromedriver.exe\n",
      "2025-06-27 16:12:40,842 - ERROR - âŒ Erreur lors de la configuration du driver: Message: invalid argument: cannot parse capability: goog:chromeOptions\n",
      "from invalid argument: unrecognized chrome option: excludeSwitches\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x0x344493+62419]\n",
      "\tGetHandleVerifier [0x0x3444d4+62484]\n",
      "\t(No symbol) [0x0x182133]\n",
      "\t(No symbol) [0x0x1a9723]\n",
      "\t(No symbol) [0x0x1aaeb0]\n",
      "\t(No symbol) [0x0x1a5fea]\n",
      "\t(No symbol) [0x0x1f9832]\n",
      "\t(No symbol) [0x0x1f931c]\n",
      "\t(No symbol) [0x0x1faa20]\n",
      "\t(No symbol) [0x0x1fa82a]\n",
      "\t(No symbol) [0x0x1ef266]\n",
      "\t(No symbol) [0x0x1be852]\n",
      "\t(No symbol) [0x0x1bf6f4]\n",
      "\tGetHandleVerifier [0x0x5b4773+2619059]\n",
      "\tGetHandleVerifier [0x0x5afb8a+2599626]\n",
      "\tGetHandleVerifier [0x0x36b03a+221050]\n",
      "\tGetHandleVerifier [0x0x35b2b8+156152]\n",
      "\tGetHandleVerifier [0x0x361c6d+183213]\n",
      "\tGetHandleVerifier [0x0x34c378+94904]\n",
      "\tGetHandleVerifier [0x0x34c502+95298]\n",
      "\tGetHandleVerifier [0x0x33765a+9626]\n",
      "\tBaseThreadInitThunk [0x0x76775d49+25]\n",
      "\tRtlInitializeExceptionChain [0x0x778ed09b+107]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x0x778ed021+561]\n",
      "\t(No symbol) [0x0]\n",
      "\n",
      "2025-06-27 16:12:40,842 - ERROR - âŒ Erreur lors de la configuration du driver: Message: invalid argument: cannot parse capability: goog:chromeOptions\n",
      "from invalid argument: unrecognized chrome option: excludeSwitches\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x0x344493+62419]\n",
      "\tGetHandleVerifier [0x0x3444d4+62484]\n",
      "\t(No symbol) [0x0x182133]\n",
      "\t(No symbol) [0x0x1a9723]\n",
      "\t(No symbol) [0x0x1aaeb0]\n",
      "\t(No symbol) [0x0x1a5fea]\n",
      "\t(No symbol) [0x0x1f9832]\n",
      "\t(No symbol) [0x0x1f931c]\n",
      "\t(No symbol) [0x0x1faa20]\n",
      "\t(No symbol) [0x0x1fa82a]\n",
      "\t(No symbol) [0x0x1ef266]\n",
      "\t(No symbol) [0x0x1be852]\n",
      "\t(No symbol) [0x0x1bf6f4]\n",
      "\tGetHandleVerifier [0x0x5b4773+2619059]\n",
      "\tGetHandleVerifier [0x0x5afb8a+2599626]\n",
      "\tGetHandleVerifier [0x0x36b03a+221050]\n",
      "\tGetHandleVerifier [0x0x35b2b8+156152]\n",
      "\tGetHandleVerifier [0x0x361c6d+183213]\n",
      "\tGetHandleVerifier [0x0x34c378+94904]\n",
      "\tGetHandleVerifier [0x0x34c502+95298]\n",
      "\tGetHandleVerifier [0x0x33765a+9626]\n",
      "\tBaseThreadInitThunk [0x0x76775d49+25]\n",
      "\tRtlInitializeExceptionChain [0x0x778ed09b+107]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x0x778ed021+561]\n",
      "\t(No symbol) [0x0]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Erreur durant les tests rÃ©els: Message: invalid argument: cannot parse capability: goog:chromeOptions\n",
      "from invalid argument: unrecognized chrome option: excludeSwitches\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x0x344493+62419]\n",
      "\tGetHandleVerifier [0x0x3444d4+62484]\n",
      "\t(No symbol) [0x0x182133]\n",
      "\t(No symbol) [0x0x1a9723]\n",
      "\t(No symbol) [0x0x1aaeb0]\n",
      "\t(No symbol) [0x0x1a5fea]\n",
      "\t(No symbol) [0x0x1f9832]\n",
      "\t(No symbol) [0x0x1f931c]\n",
      "\t(No symbol) [0x0x1faa20]\n",
      "\t(No symbol) [0x0x1fa82a]\n",
      "\t(No symbol) [0x0x1ef266]\n",
      "\t(No symbol) [0x0x1be852]\n",
      "\t(No symbol) [0x0x1bf6f4]\n",
      "\tGetHandleVerifier [0x0x5b4773+2619059]\n",
      "\tGetHandleVerifier [0x0x5afb8a+2599626]\n",
      "\tGetHandleVerifier [0x0x36b03a+221050]\n",
      "\tGetHandleVerifier [0x0x35b2b8+156152]\n",
      "\tGetHandleVerifier [0x0x361c6d+183213]\n",
      "\tGetHandleVerifier [0x0x34c378+94904]\n",
      "\tGetHandleVerifier [0x0x34c502+95298]\n",
      "\tGetHandleVerifier [0x0x33765a+9626]\n",
      "\tBaseThreadInitThunk [0x0x76775d49+25]\n",
      "\tRtlInitializeExceptionChain [0x0x778ed09b+107]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x0x778ed021+561]\n",
      "\t(No symbol) [0x0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = test_real_scrapers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ff8626bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” SiteScout initialisÃ© - PrÃªt pour la reconnaissance !\n"
     ]
    }
   ],
   "source": [
    "# ğŸ” PHASE 1: RECONNAISSANCE ET VALIDATION DES BALISES RÃ‰ELLES\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "from datetime import datetime\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "class SiteScout:\n",
    "    \"\"\"\n",
    "    Classe pour reconnaÃ®tre et valider les vraies balises des sites avant scraping.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.driver = None\n",
    "        self.discovered_selectors = {}\n",
    "        self.validated_selectors = {}\n",
    "        \n",
    "    def setup_scout_driver(self):\n",
    "        \"\"\"Configure un driver spÃ©cial pour la reconnaissance\"\"\"\n",
    "        try:\n",
    "            from selenium import webdriver\n",
    "            from selenium.webdriver.chrome.service import Service\n",
    "            from selenium.webdriver.chrome.options import Options\n",
    "            from webdriver_manager.chrome import ChromeDriverManager\n",
    "            \n",
    "            options = Options()\n",
    "            options.add_argument('--no-sandbox')\n",
    "            options.add_argument('--disable-dev-shm-usage')\n",
    "            options.add_argument('--disable-gpu')\n",
    "            options.add_argument('--window-size=1920,1080')\n",
    "            \n",
    "            service = Service(ChromeDriverManager().install())\n",
    "            self.driver = webdriver.Chrome(service=service, options=options)\n",
    "            \n",
    "            logger.info(\"âœ… Scout driver configurÃ©\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"âŒ Erreur scout driver: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def scout_amazon_selectors(self):\n",
    "        \"\"\"DÃ©couvre les vraies balises Amazon actuelles\"\"\"\n",
    "        logger.info(\"ğŸ” Reconnaissance Amazon...\")\n",
    "        \n",
    "        if not self.driver:\n",
    "            if not self.setup_scout_driver():\n",
    "                return {}\n",
    "        \n",
    "        try:\n",
    "            # Test avec une recherche simple\n",
    "            self.driver.get(\"https://www.amazon.com/s?k=laptop\")\n",
    "            time.sleep(3)\n",
    "            \n",
    "            # Accepter les cookies si nÃ©cessaire\n",
    "            try:\n",
    "                cookie_btn = self.driver.find_element(By.ID, \"sp-cc-accept\")\n",
    "                cookie_btn.click()\n",
    "                time.sleep(1)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            selectors_to_test = {\n",
    "                'product_container': [\n",
    "                    '[data-component-type=\"s-search-result\"]',\n",
    "                    '.s-result-item',\n",
    "                    '[data-asin]',\n",
    "                    '.sg-col-inner'\n",
    "                ],\n",
    "                'title': [\n",
    "                    'h2 a span',\n",
    "                    '.a-size-medium span',\n",
    "                    '.a-size-base-plus',\n",
    "                    '[data-cy=\"title-recipe-price\"]'\n",
    "                ],\n",
    "                'price': [\n",
    "                    '.a-price-whole',\n",
    "                    '.a-price .a-offscreen',\n",
    "                    '.a-price-range',\n",
    "                    '.a-price-symbol'\n",
    "                ],\n",
    "                'rating': [\n",
    "                    '.a-icon-alt',\n",
    "                    '[data-hook=\"rating-out-of-text\"]',\n",
    "                    '.a-declarative .a-icon-alt'\n",
    "                ],\n",
    "                'image': [\n",
    "                    '.s-image',\n",
    "                    '.a-dynamic-image',\n",
    "                    'img[data-image-latency]'\n",
    "                ]\n",
    "            }\n",
    "            \n",
    "            amazon_selectors = {}\n",
    "            \n",
    "            for element_type, selectors in selectors_to_test.items():\n",
    "                for selector in selectors:\n",
    "                    try:\n",
    "                        elements = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                        if len(elements) > 0:\n",
    "                            # Prendre un Ã©chantillon de texte pour validation\n",
    "                            sample_text = \"\"\n",
    "                            try:\n",
    "                                if element_type == 'image':\n",
    "                                    sample_text = elements[0].get_attribute('src')[:50]\n",
    "                                else:\n",
    "                                    sample_text = elements[0].text[:50]\n",
    "                            except:\n",
    "                                sample_text = \"Element found\"\n",
    "                            \n",
    "                            amazon_selectors[element_type] = {\n",
    "                                'selector': selector,\n",
    "                                'count': len(elements),\n",
    "                                'sample': sample_text,\n",
    "                                'validated': True\n",
    "                            }\n",
    "                            logger.info(f\"âœ… {element_type}: {selector} ({len(elements)} Ã©lÃ©ments)\")\n",
    "                            break\n",
    "                    except Exception as e:\n",
    "                        continue\n",
    "                \n",
    "                if element_type not in amazon_selectors:\n",
    "                    amazon_selectors[element_type] = {\n",
    "                        'selector': None,\n",
    "                        'count': 0,\n",
    "                        'sample': None,\n",
    "                        'validated': False\n",
    "                    }\n",
    "                    logger.warning(f\"âš ï¸ {element_type}: Aucun sÃ©lecteur trouvÃ©\")\n",
    "            \n",
    "            self.discovered_selectors['amazon'] = amazon_selectors\n",
    "            return amazon_selectors\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"âŒ Erreur reconnaissance Amazon: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def scout_ebay_selectors(self):\n",
    "        \"\"\"DÃ©couvre les vraies balises eBay actuelles\"\"\"\n",
    "        logger.info(\"ğŸ” Reconnaissance eBay...\")\n",
    "        \n",
    "        try:\n",
    "            self.driver.get(\"https://www.ebay.com/sch/i.html?_nkw=smartphone\")\n",
    "            time.sleep(3)\n",
    "            \n",
    "            selectors_to_test = {\n",
    "                'product_container': [\n",
    "                    '.s-item',\n",
    "                    '.srp-results .s-item',\n",
    "                    '[data-view=\"mi:1686|iid:1\"]'\n",
    "                ],\n",
    "                'title': [\n",
    "                    '.s-item__title',\n",
    "                    '.it-ttl',\n",
    "                    '.s-item__link'\n",
    "                ],\n",
    "                'price': [\n",
    "                    '.s-item__price',\n",
    "                    '.notranslate',\n",
    "                    '.u-flL'\n",
    "                ],\n",
    "                'condition': [\n",
    "                    '.SECONDARY_INFO',\n",
    "                    '.s-item__subtitle',\n",
    "                    '.clipped'\n",
    "                ],\n",
    "                'shipping': [\n",
    "                    '.s-item__shipping',\n",
    "                    '.vi-s-ship-range',\n",
    "                    '.s-item__logisticsCost'\n",
    "                ]\n",
    "            }\n",
    "            \n",
    "            ebay_selectors = {}\n",
    "            \n",
    "            for element_type, selectors in selectors_to_test.items():\n",
    "                for selector in selectors:\n",
    "                    try:\n",
    "                        elements = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                        if len(elements) > 0:\n",
    "                            sample_text = \"\"\n",
    "                            try:\n",
    "                                sample_text = elements[0].text[:50]\n",
    "                            except:\n",
    "                                sample_text = \"Element found\"\n",
    "                            \n",
    "                            ebay_selectors[element_type] = {\n",
    "                                'selector': selector,\n",
    "                                'count': len(elements),\n",
    "                                'sample': sample_text,\n",
    "                                'validated': True\n",
    "                            }\n",
    "                            logger.info(f\"âœ… {element_type}: {selector} ({len(elements)} Ã©lÃ©ments)\")\n",
    "                            break\n",
    "                    except:\n",
    "                        continue\n",
    "                \n",
    "                if element_type not in ebay_selectors:\n",
    "                    ebay_selectors[element_type] = {\n",
    "                        'selector': None,\n",
    "                        'count': 0,\n",
    "                        'sample': None,\n",
    "                        'validated': False\n",
    "                    }\n",
    "            \n",
    "            self.discovered_selectors['ebay'] = ebay_selectors\n",
    "            return ebay_selectors\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"âŒ Erreur reconnaissance eBay: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def scout_trustpilot_selectors(self, company=\"amazon\"):\n",
    "        \"\"\"DÃ©couvre les vraies balises Trustpilot actuelles\"\"\"\n",
    "        logger.info(f\"ğŸ” Reconnaissance Trustpilot pour {company}...\")\n",
    "        \n",
    "        try:\n",
    "            self.driver.get(f\"https://www.trustpilot.com/review/{company}\")\n",
    "            time.sleep(3)\n",
    "            \n",
    "            # Accepter cookies\n",
    "            try:\n",
    "                cookie_btn = self.driver.find_element(By.ID, \"onetrust-accept-btn-handler\")\n",
    "                cookie_btn.click()\n",
    "                time.sleep(2)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            selectors_to_test = {\n",
    "                'review_container': [\n",
    "                    'article[data-service-review-card-paper]',\n",
    "                    '.review-card',\n",
    "                    '.styles_reviewCard__hcAvl'\n",
    "                ],\n",
    "                'review_text': [\n",
    "                    '[data-service-review-text-typography=\"true\"]',\n",
    "                    '.typography_body-l__KUYFJ',\n",
    "                    '.review-content__text'\n",
    "                ],\n",
    "                'rating': [\n",
    "                    '[data-service-review-rating]',\n",
    "                    '.star-rating',\n",
    "                    '.styles_reviewHeader__iU9Px img'\n",
    "                ],\n",
    "                'reviewer_name': [\n",
    "                    '[data-consumer-name-typography=\"true\"]',\n",
    "                    '.consumer-information__name',\n",
    "                    '.styles_consumerName__dxer2'\n",
    "                ],\n",
    "                'review_date': [\n",
    "                    'time[datetime]',\n",
    "                    '.typography_body-m__xgxZ_',\n",
    "                    '.styles_reviewDate__6_BBM'\n",
    "                ]\n",
    "            }\n",
    "            \n",
    "            trustpilot_selectors = {}\n",
    "            \n",
    "            for element_type, selectors in selectors_to_test.items():\n",
    "                for selector in selectors:\n",
    "                    try:\n",
    "                        elements = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                        if len(elements) > 0:\n",
    "                            sample_text = \"\"\n",
    "                            try:\n",
    "                                if element_type == 'review_date':\n",
    "                                    sample_text = elements[0].get_attribute('datetime') or elements[0].text\n",
    "                                else:\n",
    "                                    sample_text = elements[0].text[:50]\n",
    "                            except:\n",
    "                                sample_text = \"Element found\"\n",
    "                            \n",
    "                            trustpilot_selectors[element_type] = {\n",
    "                                'selector': selector,\n",
    "                                'count': len(elements),\n",
    "                                'sample': sample_text,\n",
    "                                'validated': True\n",
    "                            }\n",
    "                            logger.info(f\"âœ… {element_type}: {selector} ({len(elements)} Ã©lÃ©ments)\")\n",
    "                            break\n",
    "                    except:\n",
    "                        continue\n",
    "                \n",
    "                if element_type not in trustpilot_selectors:\n",
    "                    trustpilot_selectors[element_type] = {\n",
    "                        'selector': None,\n",
    "                        'count': 0,\n",
    "                        'sample': None,\n",
    "                        'validated': False\n",
    "                    }\n",
    "            \n",
    "            self.discovered_selectors['trustpilot'] = trustpilot_selectors\n",
    "            return trustpilot_selectors\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"âŒ Erreur reconnaissance Trustpilot: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def save_selectors_config(self, filename=\"../config/validated_selectors.json\"):\n",
    "        \"\"\"Sauvegarde les sÃ©lecteurs validÃ©s\"\"\"\n",
    "        import os\n",
    "        os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "        \n",
    "        config = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'sites': self.discovered_selectors\n",
    "        }\n",
    "        \n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(config, f, indent=2)\n",
    "        \n",
    "        logger.info(f\"ğŸ’¾ SÃ©lecteurs sauvegardÃ©s: {filename}\")\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Ferme le driver scout\"\"\"\n",
    "        if self.driver:\n",
    "            self.driver.quit()\n",
    "\n",
    "# Initialiser le scout\n",
    "scout = SiteScout()\n",
    "print(\"ğŸ” SiteScout initialisÃ© - PrÃªt pour la reconnaissance !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4ffe089e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ PrÃªt pour la reconnaissance !\n",
      "   run_full_site_reconnaissance() - Reconnaissance complÃ¨te\n",
      "   quick_selector_test('amazon', '.s-result-item', 'https://amazon.com/s?k=laptop') - Test rapide\n"
     ]
    }
   ],
   "source": [
    "# ğŸ§ª PHASE 1: EXÃ‰CUTION DE LA RECONNAISSANCE\n",
    "def run_full_site_reconnaissance():\n",
    "    \"\"\"\n",
    "    Lance la reconnaissance complÃ¨te de tous les sites pour dÃ©couvrir les vraies balises.\n",
    "    \"\"\"\n",
    "    print(\"ğŸ¯ LANCEMENT DE LA RECONNAISSANCE COMPLÃˆTE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    scout = SiteScout()\n",
    "    all_results = {}\n",
    "    \n",
    "    try:\n",
    "        # 1. Amazon\n",
    "        print(\"\\nğŸ›’ Reconnaissance Amazon...\")\n",
    "        amazon_selectors = scout.scout_amazon_selectors()\n",
    "        all_results['amazon'] = amazon_selectors\n",
    "        \n",
    "        if amazon_selectors:\n",
    "            print(\"âœ… Amazon reconnaissance terminÃ©e\")\n",
    "            for element_type, data in amazon_selectors.items():\n",
    "                status = \"âœ…\" if data['validated'] else \"âŒ\"\n",
    "                print(f\"   {status} {element_type}: {data.get('selector', 'Non trouvÃ©')}\")\n",
    "        \n",
    "        time.sleep(2)  # Pause entre sites\n",
    "        \n",
    "        # 2. eBay\n",
    "        print(\"\\nğŸª Reconnaissance eBay...\")\n",
    "        ebay_selectors = scout.scout_ebay_selectors()\n",
    "        all_results['ebay'] = ebay_selectors\n",
    "        \n",
    "        if ebay_selectors:\n",
    "            print(\"âœ… eBay reconnaissance terminÃ©e\")\n",
    "            for element_type, data in ebay_selectors.items():\n",
    "                status = \"âœ…\" if data['validated'] else \"âŒ\"\n",
    "                print(f\"   {status} {element_type}: {data.get('selector', 'Non trouvÃ©')}\")\n",
    "        \n",
    "        time.sleep(2)\n",
    "        \n",
    "        # 3. Trustpilot\n",
    "        print(\"\\nâ­ Reconnaissance Trustpilot...\")\n",
    "        trustpilot_selectors = scout.scout_trustpilot_selectors(\"amazon\")\n",
    "        all_results['trustpilot'] = trustpilot_selectors\n",
    "        \n",
    "        if trustpilot_selectors:\n",
    "            print(\"âœ… Trustpilot reconnaissance terminÃ©e\")\n",
    "            for element_type, data in trustpilot_selectors.items():\n",
    "                status = \"âœ…\" if data['validated'] else \"âŒ\"\n",
    "                print(f\"   {status} {element_type}: {data.get('selector', 'Non trouvÃ©')}\")\n",
    "        \n",
    "        # 4. Sauvegarde\n",
    "        print(\"\\nğŸ’¾ Sauvegarde des sÃ©lecteurs...\")\n",
    "        scout.save_selectors_config()\n",
    "        \n",
    "        # 5. RÃ©sumÃ©\n",
    "        print(\"\\nğŸ“Š RÃ‰SUMÃ‰ DE LA RECONNAISSANCE:\")\n",
    "        total_selectors = 0\n",
    "        valid_selectors = 0\n",
    "        \n",
    "        for site, selectors in all_results.items():\n",
    "            site_total = len(selectors)\n",
    "            site_valid = sum(1 for s in selectors.values() if s.get('validated', False))\n",
    "            total_selectors += site_total\n",
    "            valid_selectors += site_valid\n",
    "            \n",
    "            print(f\"   {site.upper()}: {site_valid}/{site_total} sÃ©lecteurs validÃ©s\")\n",
    "        \n",
    "        print(f\"\\nğŸ¯ TOTAL: {valid_selectors}/{total_selectors} sÃ©lecteurs fonctionnels\")\n",
    "        \n",
    "        return all_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"âŒ Erreur reconnaissance: {e}\")\n",
    "        return {}\n",
    "        \n",
    "    finally:\n",
    "        scout.close()\n",
    "\n",
    "def quick_selector_test(site_name, selector, url):\n",
    "    \"\"\"Test rapide d'un sÃ©lecteur spÃ©cifique\"\"\"\n",
    "    print(f\"ğŸ§ª Test rapide: {site_name} - {selector}\")\n",
    "    \n",
    "    scout = SiteScout()\n",
    "    if not scout.setup_scout_driver():\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        scout.driver.get(url)\n",
    "        time.sleep(3)\n",
    "        \n",
    "        elements = scout.driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "        \n",
    "        if len(elements) > 0:\n",
    "            print(f\"âœ… TrouvÃ© {len(elements)} Ã©lÃ©ments\")\n",
    "            print(f\"   Exemple: {elements[0].text[:100]}...\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"âŒ Aucun Ã©lÃ©ment trouvÃ©\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Erreur: {e}\")\n",
    "        return False\n",
    "    finally:\n",
    "        scout.close()\n",
    "\n",
    "# LANCEMENT DE LA RECONNAISSANCE\n",
    "print(\"ğŸš€ PrÃªt pour la reconnaissance !\")\n",
    "print(\"   run_full_site_reconnaissance() - Reconnaissance complÃ¨te\")\n",
    "print(\"   quick_selector_test('amazon', '.s-result-item', 'https://amazon.com/s?k=laptop') - Test rapide\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bce8b44e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ ValidatedScraper prÃªt !\n"
     ]
    }
   ],
   "source": [
    "# ğŸ¯ PHASE 2: SCRAPER AVEC BALISES VALIDÃ‰ES\n",
    "class ValidatedScraper:\n",
    "    \"\"\"\n",
    "    Scraper qui utilise les balises validÃ©es de la Phase 1\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, selectors_file=\"../config/validated_selectors.json\"):\n",
    "        self.driver = None\n",
    "        self.selectors = {}\n",
    "        self.load_validated_selectors(selectors_file)\n",
    "        \n",
    "    def load_validated_selectors(self, filename):\n",
    "        \"\"\"Charge les sÃ©lecteurs validÃ©s depuis le fichier JSON\"\"\"\n",
    "        try:\n",
    "            with open(filename, 'r') as f:\n",
    "                config = json.load(f)\n",
    "                self.selectors = config.get('sites', {})\n",
    "                logger.info(f\"âœ… SÃ©lecteurs chargÃ©s depuis {filename}\")\n",
    "                \n",
    "                # Afficher les sÃ©lecteurs chargÃ©s\n",
    "                for site, site_selectors in self.selectors.items():\n",
    "                    valid_count = sum(1 for s in site_selectors.values() if s.get('validated'))\n",
    "                    print(f\"   {site.upper()}: {valid_count} sÃ©lecteurs validÃ©s\")\n",
    "                    \n",
    "        except FileNotFoundError:\n",
    "            logger.warning(\"âš ï¸ Fichier de sÃ©lecteurs non trouvÃ© - Lancez d'abord la reconnaissance\")\n",
    "            self.selectors = {}\n",
    "        except Exception as e:\n",
    "            logger.error(f\"âŒ Erreur chargement sÃ©lecteurs: {e}\")\n",
    "            self.selectors = {}\n",
    "    \n",
    "    def setup_production_driver(self, headless=True, use_stealth=True):\n",
    "        \"\"\"Configure un driver optimisÃ© pour la production\"\"\"\n",
    "        try:\n",
    "            if use_stealth:\n",
    "                import undetected_chromedriver as uc\n",
    "                options = uc.ChromeOptions()\n",
    "                \n",
    "                # Options de base\n",
    "                if headless:\n",
    "                    options.add_argument('--headless=new')\n",
    "                options.add_argument('--no-sandbox')\n",
    "                options.add_argument('--disable-dev-shm-usage')\n",
    "                options.add_argument('--disable-gpu')\n",
    "                options.add_argument('--window-size=1920,1080')\n",
    "                \n",
    "                # User agent alÃ©atoire\n",
    "                user_agent = random.choice(REALISTIC_USER_AGENTS)\n",
    "                options.add_argument(f'--user-agent={user_agent}')\n",
    "                \n",
    "                # CrÃ©er driver stealth\n",
    "                self.driver = uc.Chrome(options=options)\n",
    "                \n",
    "                # Scripts anti-dÃ©tection\n",
    "                self.driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
    "                \n",
    "            else:\n",
    "                # Driver classique si stealth Ã©choue\n",
    "                from selenium import webdriver\n",
    "                from selenium.webdriver.chrome.service import Service\n",
    "                from webdriver_manager.chrome import ChromeDriverManager\n",
    "                \n",
    "                options = webdriver.ChromeOptions()\n",
    "                if headless:\n",
    "                    options.add_argument('--headless')\n",
    "                options.add_argument('--no-sandbox')\n",
    "                options.add_argument('--disable-dev-shm-usage')\n",
    "                \n",
    "                service = Service(ChromeDriverManager().install())\n",
    "                self.driver = webdriver.Chrome(service=service, options=options)\n",
    "            \n",
    "            logger.info(\"âœ… Driver production configurÃ©\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"âŒ Erreur driver production: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def human_like_behavior(self):\n",
    "        \"\"\"Simule un comportement humain\"\"\"\n",
    "        # DÃ©lai alÃ©atoire\n",
    "        time.sleep(random.uniform(1, 3))\n",
    "        \n",
    "        # Scroll alÃ©atoire parfois\n",
    "        if random.random() < 0.3:\n",
    "            scroll_amount = random.randint(200, 800)\n",
    "            self.driver.execute_script(f\"window.scrollBy(0, {scroll_amount});\")\n",
    "            time.sleep(random.uniform(0.5, 1.5))\n",
    "    \n",
    "    def scrape_amazon_products_validated(self, search_term, max_products=20):\n",
    "        \"\"\"Scrape Amazon avec sÃ©lecteurs validÃ©s\"\"\"\n",
    "        logger.info(f\"ğŸ›’ Scraping Amazon validÃ©: {search_term}\")\n",
    "        \n",
    "        if 'amazon' not in self.selectors:\n",
    "            logger.error(\"âŒ SÃ©lecteurs Amazon non disponibles - Lancez la reconnaissance\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        amazon_selectors = self.selectors['amazon']\n",
    "        products_data = []\n",
    "        \n",
    "        try:\n",
    "            # Navigation\n",
    "            url = f\"https://www.amazon.com/s?k={search_term}\"\n",
    "            self.driver.get(url)\n",
    "            self.human_like_behavior()\n",
    "            \n",
    "            # Accepter cookies\n",
    "            try:\n",
    "                cookie_btn = WebDriverWait(self.driver, 5).until(\n",
    "                    EC.element_to_be_clickable((By.ID, \"sp-cc-accept\"))\n",
    "                )\n",
    "                cookie_btn.click()\n",
    "                time.sleep(1)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Trouver les produits avec sÃ©lecteur validÃ©\n",
    "            container_selector = amazon_selectors.get('product_container', {}).get('selector')\n",
    "            if not container_selector:\n",
    "                logger.error(\"âŒ SÃ©lecteur conteneur produit non validÃ©\")\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            products = WebDriverWait(self.driver, 10).until(\n",
    "                EC.presence_of_all_elements_located((By.CSS_SELECTOR, container_selector))\n",
    "            )\n",
    "            \n",
    "            logger.info(f\"ğŸ“¦ TrouvÃ© {len(products)} produits\")\n",
    "            \n",
    "            for i, product in enumerate(products[:max_products]):\n",
    "                try:\n",
    "                    product_data = {\n",
    "                        'product_id': f\"amazon_{search_term}_{i}\",\n",
    "                        'search_term': search_term,\n",
    "                        'source': 'amazon.com',\n",
    "                        'scraped_at': datetime.now().isoformat()\n",
    "                    }\n",
    "                    \n",
    "                    # Extraire titre avec sÃ©lecteur validÃ©\n",
    "                    title_selector = amazon_selectors.get('title', {}).get('selector')\n",
    "                    if title_selector:\n",
    "                        try:\n",
    "                            title_elem = product.find_element(By.CSS_SELECTOR, title_selector)\n",
    "                            product_data['title'] = title_elem.text.strip()\n",
    "                        except:\n",
    "                            product_data['title'] = \"Titre non trouvÃ©\"\n",
    "                    \n",
    "                    # Extraire prix avec sÃ©lecteur validÃ©\n",
    "                    price_selector = amazon_selectors.get('price', {}).get('selector')\n",
    "                    if price_selector:\n",
    "                        try:\n",
    "                            price_elem = product.find_element(By.CSS_SELECTOR, price_selector)\n",
    "                            price_text = price_elem.text.strip()\n",
    "                            price_numbers = re.findall(r'[\\d.]+', price_text.replace(',', ''))\n",
    "                            product_data['price'] = float(price_numbers[0]) if price_numbers else 0.0\n",
    "                        except:\n",
    "                            product_data['price'] = 0.0\n",
    "                    \n",
    "                    # Extraire rating avec sÃ©lecteur validÃ©\n",
    "                    rating_selector = amazon_selectors.get('rating', {}).get('selector')\n",
    "                    if rating_selector:\n",
    "                        try:\n",
    "                            rating_elem = product.find_element(By.CSS_SELECTOR, rating_selector)\n",
    "                            rating_text = rating_elem.get_attribute('textContent') or rating_elem.text\n",
    "                            rating_numbers = re.findall(r'[\\d.]+', rating_text)\n",
    "                            product_data['rating'] = float(rating_numbers[0]) if rating_numbers else 0.0\n",
    "                        except:\n",
    "                            product_data['rating'] = 0.0\n",
    "                    \n",
    "                    # Extraire URL produit\n",
    "                    try:\n",
    "                        link_elem = product.find_element(By.CSS_SELECTOR, 'h2 a')\n",
    "                        product_data['product_url'] = link_elem.get_attribute('href')\n",
    "                    except:\n",
    "                        product_data['product_url'] = \"\"\n",
    "                    \n",
    "                    products_data.append(product_data)\n",
    "                    \n",
    "                    # Comportement humain\n",
    "                    if i % 5 == 0:\n",
    "                        self.human_like_behavior()\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Erreur produit {i}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            logger.info(f\"âœ… {len(products_data)} produits Amazon scrapÃ©s avec succÃ¨s\")\n",
    "            return pd.DataFrame(products_data)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"âŒ Erreur scraping Amazon: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def scrape_trustpilot_reviews_validated(self, company, max_reviews=50):\n",
    "        \"\"\"Scrape Trustpilot avec sÃ©lecteurs validÃ©s\"\"\"\n",
    "        logger.info(f\"â­ Scraping Trustpilot validÃ©: {company}\")\n",
    "        \n",
    "        if 'trustpilot' not in self.selectors:\n",
    "            logger.error(\"âŒ SÃ©lecteurs Trustpilot non disponibles\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        trustpilot_selectors = self.selectors['trustpilot']\n",
    "        reviews_data = []\n",
    "        \n",
    "        try:\n",
    "            # Navigation\n",
    "            url = f\"https://www.trustpilot.com/review/{company}\"\n",
    "            self.driver.get(url)\n",
    "            self.human_like_behavior()\n",
    "            \n",
    "            # Accepter cookies\n",
    "            try:\n",
    "                cookie_btn = WebDriverWait(self.driver, 5).until(\n",
    "                    EC.element_to_be_clickable((By.ID, \"onetrust-accept-btn-handler\"))\n",
    "                )\n",
    "                cookie_btn.click()\n",
    "                time.sleep(2)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Trouver reviews avec sÃ©lecteur validÃ©\n",
    "            container_selector = trustpilot_selectors.get('review_container', {}).get('selector')\n",
    "            if not container_selector:\n",
    "                logger.error(\"âŒ SÃ©lecteur conteneur review non validÃ©\")\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            reviews = WebDriverWait(self.driver, 10).until(\n",
    "                EC.presence_of_all_elements_located((By.CSS_SELECTOR, container_selector))\n",
    "            )\n",
    "            \n",
    "            logger.info(f\"ğŸ’¬ TrouvÃ© {len(reviews)} reviews\")\n",
    "            \n",
    "            for i, review in enumerate(reviews[:max_reviews]):\n",
    "                try:\n",
    "                    review_data = {\n",
    "                        'review_id': f\"trustpilot_{company}_{i}\",\n",
    "                        'company': company,\n",
    "                        'source': 'trustpilot.com',\n",
    "                        'scraped_at': datetime.now().isoformat()\n",
    "                    }\n",
    "                    \n",
    "                    # Extraire texte avec sÃ©lecteur validÃ©\n",
    "                    text_selector = trustpilot_selectors.get('review_text', {}).get('selector')\n",
    "                    if text_selector:\n",
    "                        try:\n",
    "                            text_elem = review.find_element(By.CSS_SELECTOR, text_selector)\n",
    "                            review_data['review_text'] = text_elem.text.strip()\n",
    "                        except:\n",
    "                            review_data['review_text'] = \"\"\n",
    "                    \n",
    "                    # Extraire rating avec sÃ©lecteur validÃ©\n",
    "                    rating_selector = trustpilot_selectors.get('rating', {}).get('selector')\n",
    "                    if rating_selector:\n",
    "                        try:\n",
    "                            rating_elem = review.find_element(By.CSS_SELECTOR, rating_selector)\n",
    "                            # Compter les Ã©toiles ou extraire de l'attribut\n",
    "                            stars = rating_elem.find_elements(By.CSS_SELECTOR, 'img[alt*=\"star\"]')\n",
    "                            review_data['rating'] = len([s for s in stars if 'filled' in s.get_attribute('alt')])\n",
    "                        except:\n",
    "                            review_data['rating'] = 0\n",
    "                    \n",
    "                    # Extraire nom reviewer avec sÃ©lecteur validÃ©\n",
    "                    name_selector = trustpilot_selectors.get('reviewer_name', {}).get('selector')\n",
    "                    if name_selector:\n",
    "                        try:\n",
    "                            name_elem = review.find_element(By.CSS_SELECTOR, name_selector)\n",
    "                            review_data['reviewer_name'] = name_elem.text.strip()\n",
    "                        except:\n",
    "                            review_data['reviewer_name'] = \"Anonymous\"\n",
    "                    \n",
    "                    # Extraire date avec sÃ©lecteur validÃ©\n",
    "                    date_selector = trustpilot_selectors.get('review_date', {}).get('selector')\n",
    "                    if date_selector:\n",
    "                        try:\n",
    "                            date_elem = review.find_element(By.CSS_SELECTOR, date_selector)\n",
    "                            date_text = date_elem.get_attribute('datetime') or date_elem.text\n",
    "                            # Parser la date\n",
    "                            if 'T' in date_text:  # Format ISO\n",
    "                                review_data['review_date'] = date_text[:10]\n",
    "                            else:\n",
    "                                review_data['review_date'] = date_text\n",
    "                        except:\n",
    "                            review_data['review_date'] = datetime.now().strftime('%Y-%m-%d')\n",
    "                    \n",
    "                    reviews_data.append(review_data)\n",
    "                    \n",
    "                    if i % 10 == 0:\n",
    "                        self.human_like_behavior()\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Erreur review {i}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            logger.info(f\"âœ… {len(reviews_data)} reviews Trustpilot scrapÃ©es avec succÃ¨s\")\n",
    "            return pd.DataFrame(reviews_data)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"âŒ Erreur scraping Trustpilot: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Ferme le driver\"\"\"\n",
    "        if self.driver:\n",
    "            self.driver.quit()\n",
    "\n",
    "# Initialiser le scraper validÃ©\n",
    "print(\"ğŸ¯ ValidatedScraper prÃªt !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "666d8a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ SYSTÃˆME COMPLET PRÃŠT !\n",
      "ğŸš€ ExÃ©cutez: main_menu() pour commencer\n",
      "ğŸ§ª Ou directement: test_marketplace_scraper()\n"
     ]
    }
   ],
   "source": [
    "# ğŸ§ª TESTS CORRIGÃ‰S ET INTÃ‰GRATION COMPLÃˆTE\n",
    "def test_marketplace_scraper():\n",
    "    \"\"\"\n",
    "    FONCTION DE TEST CORRIGÃ‰E - Compatible avec le systÃ¨me de reconnaissance\n",
    "    \"\"\"\n",
    "    logger.info(\"ğŸ§ª DÃ©marrage des tests du scraper...\")\n",
    "    \n",
    "    all_results = {}\n",
    "    \n",
    "    try:\n",
    "        # Phase 1: VÃ©rification de l'environnement\n",
    "        logger.info(\"ğŸ”§ VÃ©rification de l'environnement...\")\n",
    "        \n",
    "        # Test des imports\n",
    "        import pandas as pd\n",
    "        from fake_useragent import UserAgent\n",
    "        logger.info(\"âœ… Imports OK\")\n",
    "        \n",
    "        # Phase 2: Test de la reconnaissance (optionnel)\n",
    "        print(\"\\nğŸ” Phase 1 - Reconnaissance des balises (optionnel)\")\n",
    "        choice = input(\"Lancer la reconnaissance des vraies balises ? (o/n): \").lower()\n",
    "        \n",
    "        if choice in ['o', 'oui', 'y', 'yes']:\n",
    "            recognition_results = run_full_site_reconnaissance()\n",
    "            all_results['recognition'] = recognition_results\n",
    "        else:\n",
    "            print(\"â­ï¸ Reconnaissance ignorÃ©e - Utilisation des balises par dÃ©faut\")\n",
    "        \n",
    "        # Phase 3: Test du scraping validÃ©\n",
    "        print(\"\\nğŸ¯ Phase 2 - Test du scraping avec balises validÃ©es\")\n",
    "        \n",
    "        validated_scraper = ValidatedScraper()\n",
    "        \n",
    "        if not validated_scraper.setup_production_driver(headless=False):\n",
    "            logger.error(\"âŒ Impossible de configurer le driver\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            # Test Amazon (si sÃ©lecteurs disponibles)\n",
    "            print(\"\\nğŸ›’ Test Amazon avec balises validÃ©es...\")\n",
    "            amazon_products = validated_scraper.scrape_amazon_products_validated(\"laptop\", max_products=5)\n",
    "            \n",
    "            if not amazon_products.empty:\n",
    "                all_results['amazon_products'] = amazon_products\n",
    "                save_scraped_data(amazon_products, \"amazon_products_validated.csv\")\n",
    "                analyze_scraped_data(amazon_products)\n",
    "            else:\n",
    "                print(\"âš ï¸ Aucun produit Amazon rÃ©cupÃ©rÃ©\")\n",
    "            \n",
    "            time.sleep(3)  # Pause entre tests\n",
    "            \n",
    "            # Test Trustpilot (si sÃ©lecteurs disponibles)\n",
    "            print(\"\\nâ­ Test Trustpilot avec balises validÃ©es...\")\n",
    "            trustpilot_reviews = validated_scraper.scrape_trustpilot_reviews_validated(\"amazon\", max_reviews=5)\n",
    "            \n",
    "            if not trustpilot_reviews.empty:\n",
    "                all_results['trustpilot_reviews'] = trustpilot_reviews\n",
    "                save_scraped_data(trustpilot_reviews, \"trustpilot_reviews_validated.csv\")\n",
    "                analyze_scraped_data(trustpilot_reviews)\n",
    "            else:\n",
    "                print(\"âš ï¸ Aucune review Trustpilot rÃ©cupÃ©rÃ©e\")\n",
    "        \n",
    "        finally:\n",
    "            validated_scraper.close()\n",
    "        \n",
    "        # RÃ©sumÃ© final\n",
    "        print(\"\\nğŸ“Š RÃ‰SUMÃ‰ DES TESTS:\")\n",
    "        for test_type, data in all_results.items():\n",
    "            if isinstance(data, pd.DataFrame):\n",
    "                print(f\"   âœ… {test_type}: {len(data)} enregistrements\")\n",
    "            else:\n",
    "                print(f\"   â„¹ï¸ {test_type}: DonnÃ©es de reconnaissance\")\n",
    "        \n",
    "        logger.info(\"âœ… Tests terminÃ©s avec succÃ¨s !\")\n",
    "        return all_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"âŒ Erreur durant les tests: {e}\")\n",
    "        return None\n",
    "\n",
    "def production_scraping_workflow(sites=['amazon'], search_terms=['laptop'], max_items=50):\n",
    "    \"\"\"\n",
    "    Workflow de production complet : Reconnaissance + Scraping\n",
    "    \"\"\"\n",
    "    print(\"ğŸš€ WORKFLOW DE PRODUCTION - SCRAPING MARKETPLACE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Ã‰tape 1: Reconnaissance automatique\n",
    "    print(\"ğŸ” Ã‰TAPE 1: Reconnaissance des balises...\")\n",
    "    recognition_results = run_full_site_reconnaissance()\n",
    "    \n",
    "    if not recognition_results:\n",
    "        print(\"âŒ Reconnaissance Ã©chouÃ©e - ArrÃªt du workflow\")\n",
    "        return None\n",
    "    \n",
    "    # Ã‰tape 2: Configuration du scraper\n",
    "    print(\"\\nğŸ¯ Ã‰TAPE 2: Configuration du scraper validÃ©...\")\n",
    "    scraper = ValidatedScraper()\n",
    "    \n",
    "    if not scraper.setup_production_driver(headless=True, use_stealth=True):\n",
    "        print(\"âŒ Configuration driver Ã©chouÃ©e\")\n",
    "        return None\n",
    "    \n",
    "    all_data = {}\n",
    "    \n",
    "    try:\n",
    "        # Ã‰tape 3: Scraping par site\n",
    "        for site in sites:\n",
    "            print(f\"\\nğŸ“Š Ã‰TAPE 3: Scraping {site.upper()}...\")\n",
    "            \n",
    "            if site == 'amazon':\n",
    "                for term in search_terms:\n",
    "                    print(f\"   ğŸ” Recherche: {term}\")\n",
    "                    products = scraper.scrape_amazon_products_validated(term, max_items)\n",
    "                    \n",
    "                    if not products.empty:\n",
    "                        key = f\"amazon_products_{term}\"\n",
    "                        all_data[key] = products\n",
    "                        save_scraped_data(products, f\"production_{key}.csv\")\n",
    "                        print(f\"   âœ… {len(products)} produits rÃ©cupÃ©rÃ©s\")\n",
    "                    \n",
    "                    time.sleep(5)  # DÃ©lai entre recherches\n",
    "            \n",
    "            elif site == 'trustpilot':\n",
    "                companies = ['amazon', 'ebay', 'apple']\n",
    "                for company in companies:\n",
    "                    print(f\"   â­ Reviews: {company}\")\n",
    "                    reviews = scraper.scrape_trustpilot_reviews_validated(company, max_items)\n",
    "                    \n",
    "                    if not reviews.empty:\n",
    "                        key = f\"trustpilot_reviews_{company}\"\n",
    "                        all_data[key] = reviews\n",
    "                        save_scraped_data(reviews, f\"production_{key}.csv\")\n",
    "                        print(f\"   âœ… {len(reviews)} reviews rÃ©cupÃ©rÃ©es\")\n",
    "                    \n",
    "                    time.sleep(5)\n",
    "        \n",
    "        # Ã‰tape 4: RÃ©sumÃ© final\n",
    "        print(\"\\nğŸ‰ WORKFLOW TERMINÃ‰ !\")\n",
    "        total_records = sum(len(df) for df in all_data.values() if isinstance(df, pd.DataFrame))\n",
    "        print(f\"ğŸ“Š Total des enregistrements: {total_records}\")\n",
    "        \n",
    "        for key, df in all_data.items():\n",
    "            if isinstance(df, pd.DataFrame):\n",
    "                print(f\"   ğŸ“„ {key}: {len(df)} enregistrements\")\n",
    "        \n",
    "        return all_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"âŒ Erreur workflow: {e}\")\n",
    "        return None\n",
    "        \n",
    "    finally:\n",
    "        scraper.close()\n",
    "\n",
    "# MENU PRINCIPAL\n",
    "def main_menu():\n",
    "    \"\"\"Menu principal pour l'utilisation du scraper\"\"\"\n",
    "    print(\"ğŸ¯ MENU PRINCIPAL - DATA COLLECTION SCRAPER\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"1. ğŸ§ª Test complet (reconnaissance + scraping)\")\n",
    "    print(\"2. ğŸ” Reconnaissance seule des balises\")\n",
    "    print(\"3. ğŸ¯ Scraping avec balises validÃ©es\")\n",
    "    print(\"4. ğŸš€ Workflow de production complet\")\n",
    "    print(\"5. âŒ Quitter\")\n",
    "    \n",
    "    try:\n",
    "        choice = input(\"\\nVotre choix (1-5): \").strip()\n",
    "        \n",
    "        if choice == \"1\":\n",
    "            test_marketplace_scraper()\n",
    "        elif choice == \"2\":\n",
    "            run_full_site_reconnaissance()\n",
    "        elif choice == \"3\":\n",
    "            scraper = ValidatedScraper()\n",
    "            scraper.setup_production_driver(headless=False)\n",
    "            # Exemple simple\n",
    "            products = scraper.scrape_amazon_products_validated(\"smartphone\", 10)\n",
    "            if not products.empty:\n",
    "                save_scraped_data(products, \"quick_scraping.csv\")\n",
    "            scraper.close()\n",
    "        elif choice == \"4\":\n",
    "            production_scraping_workflow()\n",
    "        elif choice == \"5\":\n",
    "            print(\"ğŸ‘‹ Au revoir !\")\n",
    "        else:\n",
    "            print(\"âŒ Choix invalide\")\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nğŸ‘‹ ArrÃªt demandÃ© par l'utilisateur\")\n",
    "\n",
    "print(\"ğŸ¯ SYSTÃˆME COMPLET PRÃŠT !\")\n",
    "print(\"ğŸš€ ExÃ©cutez: main_menu() pour commencer\")\n",
    "print(\"ğŸ§ª Ou directement: test_marketplace_scraper()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3d5ce55e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ MENU PRINCIPAL - DATA COLLECTION SCRAPER\n",
      "==================================================\n",
      "1. ğŸ§ª Test complet (reconnaissance + scraping)\n",
      "2. ğŸ” Reconnaissance seule des balises\n",
      "3. ğŸ¯ Scraping avec balises validÃ©es\n",
      "4. ğŸš€ Workflow de production complet\n",
      "5. âŒ Quitter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 16:13:59,079 - INFO - ğŸ” Reconnaissance Amazon...\n",
      "2025-06-27 16:13:59,137 - INFO - ====== WebDriver manager ======\n",
      "2025-06-27 16:13:59,137 - INFO - ====== WebDriver manager ======\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ WORKFLOW DE PRODUCTION - SCRAPING MARKETPLACE\n",
      "============================================================\n",
      "ğŸ” Ã‰TAPE 1: Reconnaissance des balises...\n",
      "ğŸ¯ LANCEMENT DE LA RECONNAISSANCE COMPLÃˆTE\n",
      "============================================================\n",
      "\n",
      "ğŸ›’ Reconnaissance Amazon...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 16:14:00,442 - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2025-06-27 16:14:00,651 - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2025-06-27 16:14:00,651 - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2025-06-27 16:14:00,872 - INFO - There is no [win64] chromedriver \"138.0.7204.49\" for browser google-chrome \"138.0.7204\" in cache\n",
      "2025-06-27 16:14:00,873 - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2025-06-27 16:14:00,872 - INFO - There is no [win64] chromedriver \"138.0.7204.49\" for browser google-chrome \"138.0.7204\" in cache\n",
      "2025-06-27 16:14:00,873 - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2025-06-27 16:14:01,437 - INFO - WebDriver version 138.0.7204.49 selected\n",
      "2025-06-27 16:14:01,439 - INFO - Modern chrome version https://storage.googleapis.com/chrome-for-testing-public/138.0.7204.49/win32/chromedriver-win32.zip\n",
      "2025-06-27 16:14:01,439 - INFO - About to download new driver from https://storage.googleapis.com/chrome-for-testing-public/138.0.7204.49/win32/chromedriver-win32.zip\n",
      "2025-06-27 16:14:01,437 - INFO - WebDriver version 138.0.7204.49 selected\n",
      "2025-06-27 16:14:01,439 - INFO - Modern chrome version https://storage.googleapis.com/chrome-for-testing-public/138.0.7204.49/win32/chromedriver-win32.zip\n",
      "2025-06-27 16:14:01,439 - INFO - About to download new driver from https://storage.googleapis.com/chrome-for-testing-public/138.0.7204.49/win32/chromedriver-win32.zip\n",
      "2025-06-27 16:14:01,645 - INFO - Driver downloading response is 200\n",
      "2025-06-27 16:14:01,645 - INFO - Driver downloading response is 200\n",
      "2025-06-27 16:14:03,213 - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2025-06-27 16:14:03,213 - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2025-06-27 16:14:03,468 - INFO - Driver has been saved in cache [C:\\Users\\Yann\\.wdm\\drivers\\chromedriver\\win64\\138.0.7204.49]\n",
      "2025-06-27 16:14:03,468 - INFO - Driver has been saved in cache [C:\\Users\\Yann\\.wdm\\drivers\\chromedriver\\win64\\138.0.7204.49]\n",
      "2025-06-27 16:14:05,064 - INFO - âœ… Scout driver configurÃ©\n",
      "2025-06-27 16:14:05,064 - INFO - âœ… Scout driver configurÃ©\n",
      "2025-06-27 16:14:14,122 - INFO - âœ… product_container: [data-component-type=\"s-search-result\"] (21 Ã©lÃ©ments)\n",
      "2025-06-27 16:14:14,136 - INFO - âœ… title: .a-size-medium span (24 Ã©lÃ©ments)\n",
      "2025-06-27 16:14:14,122 - INFO - âœ… product_container: [data-component-type=\"s-search-result\"] (21 Ã©lÃ©ments)\n",
      "2025-06-27 16:14:14,136 - INFO - âœ… title: .a-size-medium span (24 Ã©lÃ©ments)\n",
      "2025-06-27 16:14:14,157 - INFO - âœ… price: .a-price-whole (31 Ã©lÃ©ments)\n",
      "2025-06-27 16:14:14,166 - INFO - âœ… rating: .a-icon-alt (33 Ã©lÃ©ments)\n",
      "2025-06-27 16:14:14,157 - INFO - âœ… price: .a-price-whole (31 Ã©lÃ©ments)\n",
      "2025-06-27 16:14:14,166 - INFO - âœ… rating: .a-icon-alt (33 Ã©lÃ©ments)\n",
      "2025-06-27 16:14:14,179 - INFO - âœ… image: .s-image (43 Ã©lÃ©ments)\n",
      "2025-06-27 16:14:14,179 - INFO - âœ… image: .s-image (43 Ã©lÃ©ments)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Amazon reconnaissance terminÃ©e\n",
      "   âœ… product_container: [data-component-type=\"s-search-result\"]\n",
      "   âœ… title: .a-size-medium span\n",
      "   âœ… price: .a-price-whole\n",
      "   âœ… rating: .a-icon-alt\n",
      "   âœ… image: .s-image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 16:14:16,181 - INFO - ğŸ” Reconnaissance eBay...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸª Reconnaissance eBay...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 16:14:21,305 - INFO - âœ… product_container: .s-item (82 Ã©lÃ©ments)\n",
      "2025-06-27 16:14:21,317 - INFO - âœ… title: .s-item__title (82 Ã©lÃ©ments)\n",
      "2025-06-27 16:14:21,317 - INFO - âœ… title: .s-item__title (82 Ã©lÃ©ments)\n",
      "2025-06-27 16:14:21,330 - INFO - âœ… price: .s-item__price (62 Ã©lÃ©ments)\n",
      "2025-06-27 16:14:21,342 - INFO - âœ… condition: .SECONDARY_INFO (62 Ã©lÃ©ments)\n",
      "2025-06-27 16:14:21,330 - INFO - âœ… price: .s-item__price (62 Ã©lÃ©ments)\n",
      "2025-06-27 16:14:21,342 - INFO - âœ… condition: .SECONDARY_INFO (62 Ã©lÃ©ments)\n",
      "2025-06-27 16:14:21,354 - INFO - âœ… shipping: .s-item__shipping (60 Ã©lÃ©ments)\n",
      "2025-06-27 16:14:21,354 - INFO - âœ… shipping: .s-item__shipping (60 Ã©lÃ©ments)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… eBay reconnaissance terminÃ©e\n",
      "   âœ… product_container: .s-item\n",
      "   âœ… title: .s-item__title\n",
      "   âœ… price: .s-item__price\n",
      "   âœ… condition: .SECONDARY_INFO\n",
      "   âœ… shipping: .s-item__shipping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 16:14:23,356 - INFO - ğŸ” Reconnaissance Trustpilot pour amazon...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â­ Reconnaissance Trustpilot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 16:14:32,028 - INFO - âœ… review_container: article[data-service-review-card-paper] (24 Ã©lÃ©ments)\n",
      "2025-06-27 16:14:32,045 - INFO - âœ… review_text: [data-service-review-text-typography=\"true\"] (20 Ã©lÃ©ments)\n",
      "2025-06-27 16:14:32,045 - INFO - âœ… review_text: [data-service-review-text-typography=\"true\"] (20 Ã©lÃ©ments)\n",
      "2025-06-27 16:14:32,102 - INFO - âœ… rating: [data-service-review-rating] (20 Ã©lÃ©ments)\n",
      "2025-06-27 16:14:32,102 - INFO - âœ… rating: [data-service-review-rating] (20 Ã©lÃ©ments)\n",
      "2025-06-27 16:14:32,119 - INFO - âœ… reviewer_name: [data-consumer-name-typography=\"true\"] (24 Ã©lÃ©ments)\n",
      "2025-06-27 16:14:32,136 - INFO - âœ… review_date: time[datetime] (24 Ã©lÃ©ments)\n",
      "2025-06-27 16:14:32,119 - INFO - âœ… reviewer_name: [data-consumer-name-typography=\"true\"] (24 Ã©lÃ©ments)\n",
      "2025-06-27 16:14:32,136 - INFO - âœ… review_date: time[datetime] (24 Ã©lÃ©ments)\n",
      "2025-06-27 16:14:32,138 - INFO - ğŸ’¾ SÃ©lecteurs sauvegardÃ©s: ../config/validated_selectors.json\n",
      "2025-06-27 16:14:32,138 - INFO - ğŸ’¾ SÃ©lecteurs sauvegardÃ©s: ../config/validated_selectors.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Trustpilot reconnaissance terminÃ©e\n",
      "   âœ… review_container: article[data-service-review-card-paper]\n",
      "   âœ… review_text: [data-service-review-text-typography=\"true\"]\n",
      "   âœ… rating: [data-service-review-rating]\n",
      "   âœ… reviewer_name: [data-consumer-name-typography=\"true\"]\n",
      "   âœ… review_date: time[datetime]\n",
      "\n",
      "ğŸ’¾ Sauvegarde des sÃ©lecteurs...\n",
      "\n",
      "ğŸ“Š RÃ‰SUMÃ‰ DE LA RECONNAISSANCE:\n",
      "   AMAZON: 5/5 sÃ©lecteurs validÃ©s\n",
      "   EBAY: 5/5 sÃ©lecteurs validÃ©s\n",
      "   TRUSTPILOT: 5/5 sÃ©lecteurs validÃ©s\n",
      "\n",
      "ğŸ¯ TOTAL: 15/15 sÃ©lecteurs fonctionnels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 16:14:38,646 - INFO - âœ… SÃ©lecteurs chargÃ©s depuis ../config/validated_selectors.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ¯ Ã‰TAPE 2: Configuration du scraper validÃ©...\n",
      "   AMAZON: 5 sÃ©lecteurs validÃ©s\n",
      "   EBAY: 5 sÃ©lecteurs validÃ©s\n",
      "   TRUSTPILOT: 5 sÃ©lecteurs validÃ©s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 16:14:40,746 - INFO - patching driver executable C:\\Users\\Yann\\appdata\\roaming\\undetected_chromedriver\\undetected_chromedriver.exe\n",
      "2025-06-27 16:14:41,551 - INFO - âœ… Driver production configurÃ©\n",
      "2025-06-27 16:14:41,552 - INFO - ğŸ›’ Scraping Amazon validÃ©: laptop\n",
      "2025-06-27 16:14:41,551 - INFO - âœ… Driver production configurÃ©\n",
      "2025-06-27 16:14:41,552 - INFO - ğŸ›’ Scraping Amazon validÃ©: laptop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Ã‰TAPE 3: Scraping AMAZON...\n",
      "   ğŸ” Recherche: laptop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 16:14:54,036 - INFO - ğŸ“¦ TrouvÃ© 21 produits\n",
      "2025-06-27 16:15:05,766 - INFO - âœ… 21 produits Amazon scrapÃ©s avec succÃ¨s\n",
      "2025-06-27 16:15:05,785 - INFO - ğŸ’¾ DonnÃ©es sauvegardÃ©es: ../data/raw\\20250627_161505_production_amazon_products_laptop.csv (21 enregistrements)\n",
      "2025-06-27 16:15:05,766 - INFO - âœ… 21 produits Amazon scrapÃ©s avec succÃ¨s\n",
      "2025-06-27 16:15:05,785 - INFO - ğŸ’¾ DonnÃ©es sauvegardÃ©es: ../data/raw\\20250627_161505_production_amazon_products_laptop.csv (21 enregistrements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… 21 produits rÃ©cupÃ©rÃ©s\n",
      "\n",
      "ğŸ‰ WORKFLOW TERMINÃ‰ !\n",
      "ğŸ“Š Total des enregistrements: 21\n",
      "   ğŸ“„ amazon_products_laptop: 21 enregistrements\n",
      "\n",
      "ğŸ‰ WORKFLOW TERMINÃ‰ !\n",
      "ğŸ“Š Total des enregistrements: 21\n",
      "   ğŸ“„ amazon_products_laptop: 21 enregistrements\n"
     ]
    }
   ],
   "source": [
    "main_menu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2265b9f8",
   "metadata": {},
   "source": [
    "# Scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca2dfa9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Classe ProductReviewScout crÃ©Ã©e\n"
     ]
    }
   ],
   "source": [
    "class ProductReviewScout:\n",
    "    \"\"\"\n",
    "    Classe spÃ©cialisÃ©e pour dÃ©tecter les balises de reviews produits sur les marketplaces\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.driver = None\n",
    "        self.selectors = {\n",
    "            'amazon': {\n",
    "                'products': {\n",
    "                    'product_container': '[data-component-type=\"s-search-result\"]',\n",
    "                    'product_title': 'h2 a span, h2 span',\n",
    "                    'product_url': 'h2 a',\n",
    "                    'product_price': '.a-price-whole, .a-price .a-offscreen',\n",
    "                    'product_rating': '.a-icon-alt',\n",
    "                    'product_category': '.a-color-state, .s-breadcrumb'\n",
    "                },\n",
    "                'reviews': {\n",
    "                    'reviews_container': '[data-hook=\"review\"]',\n",
    "                    'review_title': '[data-hook=\"review-title\"] span',\n",
    "                    'review_text': '[data-hook=\"review-body\"] span',\n",
    "                    'review_rating': '[data-hook=\"review-star-rating\"] .a-icon-alt',\n",
    "                    'reviewer_name': '.a-profile-name',\n",
    "                    'review_date': '[data-hook=\"review-date\"]',\n",
    "                    'next_page': '.a-pagination .a-last a',\n",
    "                    'review_helpfulness': '[data-hook=\"helpful-vote-statement\"]'\n",
    "                }\n",
    "            },\n",
    "            'ebay': {\n",
    "                'products': {\n",
    "                    'product_container': '.s-item',\n",
    "                    'product_title': '.s-item__title',\n",
    "                    'product_url': '.s-item__link',\n",
    "                    'product_price': '.s-item__price',\n",
    "                    'product_rating': '.ebay-review-star-rating',\n",
    "                    'product_category': '.breadcrumbs'\n",
    "                },\n",
    "                'reviews': {\n",
    "                    'reviews_container': '.review-item',\n",
    "                    'review_title': '.review-item-title',\n",
    "                    'review_text': '.review-item-content',\n",
    "                    'review_rating': '.star-rating',\n",
    "                    'reviewer_name': '.review-item-author',\n",
    "                    'review_date': '.review-item-date',\n",
    "                    'next_page': '.pager .next'\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def setup_driver(self, headless=True):\n",
    "        \"\"\"Configure le driver pour la dÃ©tection\"\"\"\n",
    "        try:\n",
    "            options = uc.ChromeOptions()\n",
    "            options.add_argument('--no-sandbox')\n",
    "            options.add_argument('--disable-dev-shm-usage')\n",
    "            options.add_argument('--disable-gpu')\n",
    "            options.add_argument('--disable-extensions')\n",
    "            \n",
    "            if headless:\n",
    "                options.add_argument('--headless')\n",
    "                \n",
    "            self.driver = uc.Chrome(options=options, version_main=None)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Erreur setup driver: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def detect_product_selectors(self, site_url, search_term=\"laptop\"):\n",
    "        \"\"\"DÃ©tecte automatiquement les sÃ©lecteurs pour les produits\"\"\"\n",
    "        if not self.driver:\n",
    "            print(\"âŒ Driver non initialisÃ©\")\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            # Aller sur la page de recherche\n",
    "            search_url = self._build_search_url(site_url, search_term)\n",
    "            self.driver.get(search_url)\n",
    "            time.sleep(3)\n",
    "            \n",
    "            detected_selectors = {}\n",
    "            \n",
    "            if 'amazon' in site_url:\n",
    "                detected_selectors = self._detect_amazon_selectors()\n",
    "            elif 'ebay' in site_url:\n",
    "                detected_selectors = self._detect_ebay_selectors()\n",
    "                \n",
    "            return detected_selectors\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Erreur dÃ©tection: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def detect_review_selectors(self, product_url):\n",
    "        \"\"\"DÃ©tecte automatiquement les sÃ©lecteurs pour les reviews\"\"\"\n",
    "        if not self.driver:\n",
    "            print(\"âŒ Driver non initialisÃ©\")\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            self.driver.get(product_url)\n",
    "            time.sleep(2)\n",
    "            \n",
    "            # Chercher le lien vers les reviews\n",
    "            review_links = self.driver.find_elements(By.CSS_SELECTOR, 'a[href*=\"review\"], a[href*=\"customer-review\"]')\n",
    "            if review_links:\n",
    "                review_url = review_links[0].get_attribute('href')\n",
    "                self.driver.get(review_url)\n",
    "                time.sleep(3)\n",
    "            \n",
    "            detected_selectors = {}\n",
    "            \n",
    "            if 'amazon' in product_url:\n",
    "                detected_selectors = self._detect_amazon_review_selectors()\n",
    "            elif 'ebay' in product_url:\n",
    "                detected_selectors = self._detect_ebay_review_selectors()\n",
    "                \n",
    "            return detected_selectors\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Erreur dÃ©tection reviews: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _build_search_url(self, site_url, search_term):\n",
    "        \"\"\"Construit l'URL de recherche selon le site\"\"\"\n",
    "        if 'amazon' in site_url:\n",
    "            return f\"https://www.amazon.com/s?k={search_term.replace(' ', '+')}\"\n",
    "        elif 'ebay' in site_url:\n",
    "            return f\"https://www.ebay.com/sch/i.html?_nkw={search_term.replace(' ', '+')}\"\n",
    "        return site_url\n",
    "    \n",
    "    def _detect_amazon_selectors(self):\n",
    "        \"\"\"DÃ©tecte les sÃ©lecteurs Amazon automatiquement\"\"\"\n",
    "        selectors = {}\n",
    "        \n",
    "        # Test de diffÃ©rents sÃ©lecteurs possibles\n",
    "        product_selectors = [\n",
    "            '[data-component-type=\"s-search-result\"]',\n",
    "            '.s-result-item',\n",
    "            '.s-widget-container'\n",
    "        ]\n",
    "        \n",
    "        for selector in product_selectors:\n",
    "            elements = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "            if len(elements) > 3:  # Au moins 3 produits trouvÃ©s\n",
    "                selectors['product_container'] = selector\n",
    "                break\n",
    "        \n",
    "        # Test des sÃ©lecteurs de titre\n",
    "        title_selectors = [\n",
    "            'h2 a span',\n",
    "            '.s-size-mini span',\n",
    "            'h2 span'\n",
    "        ]\n",
    "        \n",
    "        for selector in title_selectors:\n",
    "            elements = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "            if elements and elements[0].text.strip():\n",
    "                selectors['product_title'] = selector\n",
    "                break\n",
    "        \n",
    "        # Autres sÃ©lecteurs...\n",
    "        selectors.update({\n",
    "            'product_url': 'h2 a, .a-link-normal',\n",
    "            'product_price': '.a-price .a-offscreen, .a-price-whole',\n",
    "            'product_rating': '.a-icon-alt'\n",
    "        })\n",
    "        \n",
    "        return selectors\n",
    "    \n",
    "    def _detect_amazon_review_selectors(self):\n",
    "        \"\"\"DÃ©tecte les sÃ©lecteurs de reviews Amazon\"\"\"\n",
    "        selectors = {}\n",
    "        \n",
    "        # Test des conteneurs de review\n",
    "        review_selectors = [\n",
    "            '[data-hook=\"review\"]',\n",
    "            '.review',\n",
    "            '.cr-original-review-content'\n",
    "        ]\n",
    "        \n",
    "        for selector in review_selectors:\n",
    "            elements = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "            if len(elements) > 2:\n",
    "                selectors['reviews_container'] = selector\n",
    "                break\n",
    "        \n",
    "        selectors.update({\n",
    "            'review_title': '[data-hook=\"review-title\"] span, .review-title',\n",
    "            'review_text': '[data-hook=\"review-body\"] span, .review-text',\n",
    "            'review_rating': '[data-hook=\"review-star-rating\"] .a-icon-alt, .review-rating',\n",
    "            'reviewer_name': '.a-profile-name, .review-author',\n",
    "            'review_date': '[data-hook=\"review-date\"], .review-date'\n",
    "        })\n",
    "        \n",
    "        return selectors\n",
    "    \n",
    "    def _detect_ebay_selectors(self):\n",
    "        \"\"\"DÃ©tecte les sÃ©lecteurs eBay automatiquement\"\"\"\n",
    "        # Implementation similaire pour eBay\n",
    "        return self.selectors['ebay']['products']\n",
    "    \n",
    "    def _detect_ebay_review_selectors(self):\n",
    "        \"\"\"DÃ©tecte les sÃ©lecteurs de reviews eBay\"\"\"\n",
    "        return self.selectors['ebay']['reviews']\n",
    "    \n",
    "    def save_selectors(self, detected_selectors, filename=\"../config/product_review_selectors.json\"):\n",
    "        \"\"\"Sauvegarde les sÃ©lecteurs dÃ©tectÃ©s\"\"\"\n",
    "        try:\n",
    "            os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                json.dump(detected_selectors, f, indent=2, ensure_ascii=False)\n",
    "            print(f\"âœ… SÃ©lecteurs sauvegardÃ©s: {filename}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Erreur sauvegarde: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Ferme le driver\"\"\"\n",
    "        if self.driver:\n",
    "            self.driver.quit()\n",
    "\n",
    "print(\"âœ… Classe ProductReviewScout crÃ©Ã©e\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a701baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Classe ProductReviewScraper crÃ©Ã©e\n"
     ]
    }
   ],
   "source": [
    "class ProductReviewScraper:\n",
    "    \"\"\"\n",
    "    Scraper spÃ©cialisÃ© pour rÃ©cupÃ©rer les reviews de produits avec balises validÃ©es\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, selectors_file=\"../config/product_review_selectors.json\"):\n",
    "        self.driver = None\n",
    "        self.selectors = self.load_selectors(selectors_file)\n",
    "        self.scraped_data = []\n",
    "        \n",
    "    def load_selectors(self, filename):\n",
    "        \"\"\"Charge les sÃ©lecteurs validÃ©s\"\"\"\n",
    "        try:\n",
    "            with open(filename, 'r', encoding='utf-8') as f:\n",
    "                return json.load(f)\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Erreur chargement sÃ©lecteurs: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def setup_driver(self, headless=True):\n",
    "        \"\"\"Configure le driver anti-dÃ©tection pour le scraping\"\"\"\n",
    "        try:\n",
    "            options = uc.ChromeOptions()\n",
    "            options.add_argument('--no-sandbox')\n",
    "            options.add_argument('--disable-dev-shm-usage')\n",
    "            options.add_argument('--disable-gpu')\n",
    "            options.add_argument('--disable-extensions')\n",
    "            options.add_argument('--disable-notifications')\n",
    "            options.add_argument('--no-first-run')\n",
    "            \n",
    "            if headless:\n",
    "                options.add_argument('--headless')\n",
    "            \n",
    "            # User agent alÃ©atoire\n",
    "            user_agent = random.choice(REALISTIC_USER_AGENTS)\n",
    "            options.add_argument(f'--user-agent={user_agent}')\n",
    "            \n",
    "            # Options expÃ©rimentales\n",
    "            options.add_experimental_option('useAutomationExtension', False)\n",
    "            options.add_experimental_option(\"prefs\", {\n",
    "                \"profile.default_content_setting_values.notifications\": 2\n",
    "            })\n",
    "            \n",
    "            self.driver = uc.Chrome(options=options, version_main=None)\n",
    "            \n",
    "            # Scripts anti-dÃ©tection\n",
    "            self.driver.execute_cdp_cmd('Page.addScriptToEvaluateOnNewDocument', {\n",
    "                'source': '''\n",
    "                    Object.defineProperty(navigator, 'webdriver', {\n",
    "                        get: () => undefined,\n",
    "                    });\n",
    "                '''\n",
    "            })\n",
    "            \n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Erreur setup driver: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def scrape_category_product_reviews(self, site_url, category_search, max_products=10, reviews_per_rating=50):\n",
    "        \"\"\"\n",
    "        Scrape les reviews de produits d'une catÃ©gorie spÃ©cifique\n",
    "        \n",
    "        Args:\n",
    "            site_url: URL du site (amazon.com, ebay.com)\n",
    "            category_search: terme de recherche pour la catÃ©gorie\n",
    "            max_products: nombre max de produits Ã  scraper (dÃ©faut: 10)\n",
    "            reviews_per_rating: nombre de reviews par note (dÃ©faut: 50)\n",
    "        \"\"\"\n",
    "        if not self.driver:\n",
    "            print(\"âŒ Driver non initialisÃ©\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        try:\n",
    "            print(f\"ğŸ” Recherche de produits pour: {category_search}\")\n",
    "            \n",
    "            # 1. RÃ©cupÃ©rer la liste des produits\n",
    "            products = self._get_products_list(site_url, category_search, max_products)\n",
    "            \n",
    "            if not products:\n",
    "                print(\"âŒ Aucun produit trouvÃ©\")\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            print(f\"âœ… {len(products)} produits trouvÃ©s\")\n",
    "            \n",
    "            # 2. Pour chaque produit, rÃ©cupÃ©rer les reviews\n",
    "            all_reviews = []\n",
    "            \n",
    "            for i, product in enumerate(products[:max_products], 1):\n",
    "                print(f\"ğŸ“¦ Produit {i}/{len(products)}: {product['title'][:50]}...\")\n",
    "                \n",
    "                product_reviews = self._scrape_product_reviews(\n",
    "                    product, \n",
    "                    reviews_per_rating\n",
    "                )\n",
    "                \n",
    "                if product_reviews:\n",
    "                    all_reviews.extend(product_reviews)\n",
    "                    print(f\"âœ… {len(product_reviews)} reviews rÃ©cupÃ©rÃ©es\")\n",
    "                else:\n",
    "                    print(\"âš ï¸ Aucune review trouvÃ©e\")\n",
    "                \n",
    "                # DÃ©lai entre produits\n",
    "                time.sleep(random.uniform(2, 5))\n",
    "            \n",
    "            # 3. CrÃ©er le DataFrame final\n",
    "            df = pd.DataFrame(all_reviews)\n",
    "            \n",
    "            if not df.empty:\n",
    "                # Nettoyage des donnÃ©es\n",
    "                df = self._clean_review_data(df)\n",
    "                print(f\"âœ… Total: {len(df)} reviews rÃ©cupÃ©rÃ©es\")\n",
    "            \n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Erreur scraping: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def _get_products_list(self, site_url, search_term, max_products):\n",
    "        \"\"\"RÃ©cupÃ¨re la liste des produits Ã  partir de la recherche\"\"\"\n",
    "        try:\n",
    "            # Construire l'URL de recherche\n",
    "            search_url = self._build_search_url(site_url, search_term)\n",
    "            print(f\"ğŸ”— URL: {search_url}\")\n",
    "            \n",
    "            self.driver.get(search_url)\n",
    "            time.sleep(3)\n",
    "            \n",
    "            products = []\n",
    "            site_type = 'amazon' if 'amazon' in site_url else 'ebay'\n",
    "            \n",
    "            # Utiliser les sÃ©lecteurs appropriÃ©s\n",
    "            if site_type in self.selectors and 'products' in self.selectors[site_type]:\n",
    "                selectors = self.selectors[site_type]['products']\n",
    "            else:\n",
    "                print(f\"âš ï¸ SÃ©lecteurs non trouvÃ©s pour {site_type}\")\n",
    "                return []\n",
    "            \n",
    "            # RÃ©cupÃ©rer les conteneurs de produits\n",
    "            product_containers = self.driver.find_elements(\n",
    "                By.CSS_SELECTOR, \n",
    "                selectors.get('product_container', '.s-result-item')\n",
    "            )\n",
    "            \n",
    "            for container in product_containers[:max_products]:\n",
    "                try:\n",
    "                    # Extraire les infos du produit\n",
    "                    title_elem = container.find_element(\n",
    "                        By.CSS_SELECTOR, \n",
    "                        selectors.get('product_title', 'h2 span')\n",
    "                    )\n",
    "                    \n",
    "                    url_elem = container.find_element(\n",
    "                        By.CSS_SELECTOR, \n",
    "                        selectors.get('product_url', 'h2 a')\n",
    "                    )\n",
    "                    \n",
    "                    product_data = {\n",
    "                        'title': title_elem.text.strip(),\n",
    "                        'url': url_elem.get_attribute('href'),\n",
    "                        'category': search_term\n",
    "                    }\n",
    "                    \n",
    "                    # Prix optionnel\n",
    "                    try:\n",
    "                        price_elem = container.find_element(\n",
    "                            By.CSS_SELECTOR, \n",
    "                            selectors.get('product_price', '.a-price')\n",
    "                        )\n",
    "                        product_data['price'] = price_elem.text.strip()\n",
    "                    except:\n",
    "                        product_data['price'] = 'N/A'\n",
    "                    \n",
    "                    # Rating optionnel\n",
    "                    try:\n",
    "                        rating_elem = container.find_element(\n",
    "                            By.CSS_SELECTOR, \n",
    "                            selectors.get('product_rating', '.a-icon-alt')\n",
    "                        )\n",
    "                        product_data['rating'] = rating_elem.get_attribute('textContent')\n",
    "                    except:\n",
    "                        product_data['rating'] = 'N/A'\n",
    "                    \n",
    "                    if product_data['title'] and product_data['url']:\n",
    "                        products.append(product_data)\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    continue  # Ignorer les produits problÃ©matiques\n",
    "            \n",
    "            return products\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Erreur rÃ©cupÃ©ration produits: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def _scrape_product_reviews(self, product, reviews_per_rating):\n",
    "        \"\"\"Scrape les reviews d'un produit spÃ©cifique\"\"\"\n",
    "        try:\n",
    "            # Aller sur la page produit\n",
    "            self.driver.get(product['url'])\n",
    "            time.sleep(2)\n",
    "            \n",
    "            # Chercher le lien vers les reviews\n",
    "            reviews_url = self._find_reviews_url(product['url'])\n",
    "            \n",
    "            if not reviews_url:\n",
    "                print(\"âš ï¸ Lien reviews non trouvÃ©\")\n",
    "                return []\n",
    "            \n",
    "            # Aller sur la page des reviews\n",
    "            self.driver.get(reviews_url)\n",
    "            time.sleep(3)\n",
    "            \n",
    "            reviews = []\n",
    "            site_type = 'amazon' if 'amazon' in product['url'] else 'ebay'\n",
    "            \n",
    "            # RÃ©cupÃ©rer les reviews par note (5, 4, 3, 2, 1 Ã©toiles)\n",
    "            for rating in [5, 4, 3, 2, 1]:\n",
    "                rating_reviews = self._scrape_reviews_by_rating(\n",
    "                    site_type, \n",
    "                    rating, \n",
    "                    reviews_per_rating,\n",
    "                    product\n",
    "                )\n",
    "                reviews.extend(rating_reviews)\n",
    "                \n",
    "                # DÃ©lai entre les notes\n",
    "                time.sleep(random.uniform(1, 3))\n",
    "            \n",
    "            return reviews\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Erreur scraping reviews produit: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def _find_reviews_url(self, product_url):\n",
    "        \"\"\"Trouve l'URL de la page des reviews\"\"\"\n",
    "        try:\n",
    "            # Chercher les liens vers les reviews\n",
    "            review_selectors = [\n",
    "                'a[href*=\"customer-reviews\"]',\n",
    "                'a[href*=\"reviews\"]',\n",
    "                'a[href*=\"review\"]',\n",
    "                '.a-link-emphasis[href*=\"review\"]'\n",
    "            ]\n",
    "            \n",
    "            for selector in review_selectors:\n",
    "                try:\n",
    "                    link = self.driver.find_element(By.CSS_SELECTOR, selector)\n",
    "                    return link.get_attribute('href')\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            # Si aucun lien trouvÃ©, construire l'URL\n",
    "            if 'amazon' in product_url:\n",
    "                # Extraire l'ASIN depuis l'URL\n",
    "                import re\n",
    "                asin_match = re.search(r'/dp/([A-Z0-9]{10})', product_url)\n",
    "                if asin_match:\n",
    "                    asin = asin_match.group(1)\n",
    "                    return f\"https://www.amazon.com/product-reviews/{asin}\"\n",
    "            \n",
    "            return None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Erreur recherche URL reviews: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _scrape_reviews_by_rating(self, site_type, rating, max_reviews, product_info):\n",
    "        \"\"\"Scrape les reviews pour une note spÃ©cifique\"\"\"\n",
    "        reviews = []\n",
    "        \n",
    "        try:\n",
    "            # Filtrer par note si possible\n",
    "            self._filter_by_rating(site_type, rating)\n",
    "            time.sleep(2)\n",
    "            \n",
    "            # RÃ©cupÃ©rer les reviews\n",
    "            selectors = self.selectors.get(site_type, {}).get('reviews', {})\n",
    "            pages_scraped = 0\n",
    "            max_pages = 10  # Limite de pages\n",
    "            \n",
    "            while len(reviews) < max_reviews and pages_scraped < max_pages:\n",
    "                # Reviews de la page actuelle\n",
    "                page_reviews = self._extract_reviews_from_page(selectors, product_info, rating)\n",
    "                \n",
    "                if not page_reviews:\n",
    "                    break\n",
    "                \n",
    "                reviews.extend(page_reviews)\n",
    "                \n",
    "                # Passer Ã  la page suivante\n",
    "                if not self._go_to_next_page(selectors):\n",
    "                    break\n",
    "                \n",
    "                pages_scraped += 1\n",
    "                time.sleep(random.uniform(2, 4))\n",
    "            \n",
    "            # Limiter au nombre demandÃ©\n",
    "            return reviews[:max_reviews]\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Erreur scraping rating {rating}: {e}\")\n",
    "            return reviews\n",
    "    \n",
    "    def _filter_by_rating(self, site_type, rating):\n",
    "        \"\"\"Filtre les reviews par note\"\"\"\n",
    "        try:\n",
    "            if site_type == 'amazon':\n",
    "                # Chercher le filtre par Ã©toiles\n",
    "                filter_selector = f'a[href*=\"filterByStar=five_star\"], a[href*=\"star_rating={rating}\"]'\n",
    "                filter_links = self.driver.find_elements(By.CSS_SELECTOR, filter_selector)\n",
    "                \n",
    "                for link in filter_links:\n",
    "                    if f\"{rating}\" in link.get_attribute('href') or f\"{rating} star\" in link.text:\n",
    "                        link.click()\n",
    "                        return True\n",
    "        except:\n",
    "            pass\n",
    "        return False\n",
    "    \n",
    "    def _extract_reviews_from_page(self, selectors, product_info, target_rating):\n",
    "        \"\"\"Extrait les reviews de la page actuelle\"\"\"\n",
    "        reviews = []\n",
    "        \n",
    "        try:\n",
    "            # RÃ©cupÃ©rer tous les conteneurs de reviews\n",
    "            review_containers = self.driver.find_elements(\n",
    "                By.CSS_SELECTOR, \n",
    "                selectors.get('reviews_container', '[data-hook=\"review\"]')\n",
    "            )\n",
    "            \n",
    "            for container in review_containers:\n",
    "                try:\n",
    "                    review_data = {\n",
    "                        'product_name': product_info['title'],\n",
    "                        'product_category': product_info['category'],\n",
    "                        'product_url': product_info['url'],\n",
    "                        'target_rating': target_rating\n",
    "                    }\n",
    "                    \n",
    "                    # Titre de la review\n",
    "                    try:\n",
    "                        title_elem = container.find_element(\n",
    "                            By.CSS_SELECTOR, \n",
    "                            selectors.get('review_title', '[data-hook=\"review-title\"]')\n",
    "                        )\n",
    "                        review_data['review_title'] = title_elem.text.strip()\n",
    "                    except:\n",
    "                        review_data['review_title'] = ''\n",
    "                    \n",
    "                    # Texte de la review\n",
    "                    try:\n",
    "                        text_elem = container.find_element(\n",
    "                            By.CSS_SELECTOR, \n",
    "                            selectors.get('review_text', '[data-hook=\"review-body\"]')\n",
    "                        )\n",
    "                        review_data['review_text'] = text_elem.text.strip()\n",
    "                    except:\n",
    "                        review_data['review_text'] = ''\n",
    "                    \n",
    "                    # Note de la review\n",
    "                    try:\n",
    "                        rating_elem = container.find_element(\n",
    "                            By.CSS_SELECTOR, \n",
    "                            selectors.get('review_rating', '.a-icon-alt')\n",
    "                        )\n",
    "                        rating_text = rating_elem.get_attribute('textContent') or rating_elem.text\n",
    "                        # Extraire le chiffre de la note\n",
    "                        import re\n",
    "                        rating_match = re.search(r'(\\d+(?:\\.\\d+)?)', rating_text)\n",
    "                        review_data['user_rating'] = rating_match.group(1) if rating_match else 'N/A'\n",
    "                    except:\n",
    "                        review_data['user_rating'] = 'N/A'\n",
    "                    \n",
    "                    # Nom du reviewer\n",
    "                    try:\n",
    "                        name_elem = container.find_element(\n",
    "                            By.CSS_SELECTOR, \n",
    "                            selectors.get('reviewer_name', '.a-profile-name')\n",
    "                        )\n",
    "                        review_data['reviewer_name'] = name_elem.text.strip()\n",
    "                    except:\n",
    "                        review_data['reviewer_name'] = 'Anonymous'\n",
    "                    \n",
    "                    # Date de la review\n",
    "                    try:\n",
    "                        date_elem = container.find_element(\n",
    "                            By.CSS_SELECTOR, \n",
    "                            selectors.get('review_date', '[data-hook=\"review-date\"]')\n",
    "                        )\n",
    "                        review_data['review_date'] = date_elem.text.strip()\n",
    "                    except:\n",
    "                        review_data['review_date'] = 'N/A'\n",
    "                    \n",
    "                    # Timestamp de scraping\n",
    "                    review_data['scraped_at'] = datetime.now().isoformat()\n",
    "                    \n",
    "                    # Ajouter seulement si on a du contenu\n",
    "                    if review_data['review_text'] or review_data['review_title']:\n",
    "                        reviews.append(review_data)\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    continue  # Ignorer les reviews problÃ©matiques\n",
    "            \n",
    "            return reviews\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Erreur extraction reviews: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def _go_to_next_page(self, selectors):\n",
    "        \"\"\"Passe Ã  la page suivante des reviews\"\"\"\n",
    "        try:\n",
    "            next_button = self.driver.find_element(\n",
    "                By.CSS_SELECTOR, \n",
    "                selectors.get('next_page', '.a-pagination .a-last a')\n",
    "            )\n",
    "            next_button.click()\n",
    "            time.sleep(2)\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def _build_search_url(self, site_url, search_term):\n",
    "        \"\"\"Construit l'URL de recherche\"\"\"\n",
    "        if 'amazon' in site_url:\n",
    "            return f\"https://www.amazon.com/s?k={search_term.replace(' ', '+')}\"\n",
    "        elif 'ebay' in site_url:\n",
    "            return f\"https://www.ebay.com/sch/i.html?_nkw={search_term.replace(' ', '+')}\"\n",
    "        return site_url\n",
    "    \n",
    "    def _clean_review_data(self, df):\n",
    "        \"\"\"Nettoie et structure les donnÃ©es de reviews\"\"\"\n",
    "        try:\n",
    "            # Supprimer les doublons\n",
    "            df = df.drop_duplicates(subset=['review_text', 'product_name'], keep='first')\n",
    "            \n",
    "            # Nettoyer les textes\n",
    "            df['review_text'] = df['review_text'].str.strip()\n",
    "            df['review_title'] = df['review_title'].str.strip()\n",
    "            \n",
    "            # Convertir les ratings en numÃ©rique\n",
    "            df['user_rating'] = pd.to_numeric(df['user_rating'], errors='coerce')\n",
    "            \n",
    "            # Ajouter une colonne de longueur de texte\n",
    "            df['review_length'] = df['review_text'].str.len()\n",
    "            \n",
    "            # Filtrer les reviews trop courtes\n",
    "            df = df[df['review_length'] > 10]\n",
    "            \n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Erreur nettoyage donnÃ©es: {e}\")\n",
    "            return df\n",
    "    \n",
    "    def save_reviews(self, df, filename=None):\n",
    "        \"\"\"Sauvegarde les reviews dans un fichier CSV\"\"\"\n",
    "        try:\n",
    "            if filename is None:\n",
    "                timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                filename = f\"../data/raw/product_reviews_{timestamp}.csv\"\n",
    "            \n",
    "            os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "            df.to_csv(filename, index=False, encoding='utf-8')\n",
    "            print(f\"âœ… Reviews sauvegardÃ©es: {filename}\")\n",
    "            return filename\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Erreur sauvegarde: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Ferme le driver\"\"\"\n",
    "        if self.driver:\n",
    "            self.driver.quit()\n",
    "\n",
    "print(\"âœ… Classe ProductReviewScraper crÃ©Ã©e\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c106055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Workflow de reviews de produits crÃ©Ã©\n",
      "ğŸ“– Utilisez reviews_workflow_menu() pour commencer\n"
     ]
    }
   ],
   "source": [
    "def product_reviews_workflow(category_search=\"laptop\", site=\"amazon\", max_products=10, reviews_per_rating=50):\n",
    "    \"\"\"\n",
    "    Workflow complet pour rÃ©cupÃ©rer les reviews de produits d'une catÃ©gorie\n",
    "    \n",
    "    Phase 1: DÃ©tection automatique des balises\n",
    "    Phase 2: Scraping des reviews avec balises validÃ©es\n",
    "    \n",
    "    Args:\n",
    "        category_search: catÃ©gorie de produits Ã  rechercher\n",
    "        site: site Ã  scraper ('amazon' ou 'ebay')\n",
    "        max_products: nombre de produits Ã  analyser (dÃ©faut: 10)\n",
    "        reviews_per_rating: nombre de reviews par note 1-5 Ã©toiles (dÃ©faut: 50)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"ğŸ¯ WORKFLOW SPÃ‰CIALISÃ‰ - REVIEWS DE PRODUITS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"ğŸ“¦ CatÃ©gorie: {category_search}\")\n",
    "    print(f\"ğŸŒ Site: {site}\")\n",
    "    print(f\"ğŸ“Š Produits: {max_products}\")\n",
    "    print(f\"â­ Reviews par note: {reviews_per_rating}\")\n",
    "    print(f\"ğŸ“ˆ Total estimÃ©: {max_products * 5 * reviews_per_rating} reviews max\")\n",
    "    print()\n",
    "    \n",
    "    # URLs des sites\n",
    "    site_urls = {\n",
    "        'amazon': 'https://www.amazon.com',\n",
    "        'ebay': 'https://www.ebay.com'\n",
    "    }\n",
    "    \n",
    "    if site not in site_urls:\n",
    "        print(f\"âŒ Site non supportÃ©: {site}\")\n",
    "        return None\n",
    "    \n",
    "    site_url = site_urls[site]\n",
    "    \n",
    "    # ============================================================================\n",
    "    # PHASE 1: DÃ‰TECTION DES BALISES\n",
    "    # ============================================================================\n",
    "    print(\"ğŸ” PHASE 1: DÃ‰TECTION AUTOMATIQUE DES BALISES\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    scout = ProductReviewScout()\n",
    "    \n",
    "    try:\n",
    "        # Setup du driver pour la dÃ©tection\n",
    "        if not scout.setup_driver(headless=True):\n",
    "            print(\"âŒ Ã‰chec initialisation driver de dÃ©tection\")\n",
    "            return None\n",
    "        \n",
    "        print(\"âœ… Driver de dÃ©tection initialisÃ©\")\n",
    "        \n",
    "        # DÃ©tection des sÃ©lecteurs de produits\n",
    "        print(f\"ğŸ” DÃ©tection des sÃ©lecteurs de produits sur {site}...\")\n",
    "        product_selectors = scout.detect_product_selectors(site_url, category_search)\n",
    "        \n",
    "        if not product_selectors:\n",
    "            print(\"âŒ Ã‰chec dÃ©tection sÃ©lecteurs produits\")\n",
    "            scout.close()\n",
    "            return None\n",
    "        \n",
    "        print(\"âœ… SÃ©lecteurs produits dÃ©tectÃ©s:\")\n",
    "        for key, value in product_selectors.items():\n",
    "            print(f\"   â€¢ {key}: {value}\")\n",
    "        \n",
    "        # Test sur un produit pour dÃ©tecter les sÃ©lecteurs de reviews\n",
    "        print(\"ğŸ” Test dÃ©tection sÃ©lecteurs de reviews...\")\n",
    "        \n",
    "        # Simuler la rÃ©cupÃ©ration d'un produit test\n",
    "        scout.driver.get(scout._build_search_url(site_url, category_search))\n",
    "        time.sleep(3)\n",
    "        \n",
    "        test_product_url = None\n",
    "        try:\n",
    "            # Trouver le premier produit\n",
    "            product_links = scout.driver.find_elements(By.CSS_SELECTOR, 'h2 a, .s-item__link')\n",
    "            if product_links:\n",
    "                test_product_url = product_links[0].get_attribute('href')\n",
    "                print(f\"ğŸ“¦ Produit test: {test_product_url[:80]}...\")\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        review_selectors = None\n",
    "        if test_product_url:\n",
    "            review_selectors = scout.detect_review_selectors(test_product_url)\n",
    "        \n",
    "        if review_selectors:\n",
    "            print(\"âœ… SÃ©lecteurs reviews dÃ©tectÃ©s:\")\n",
    "            for key, value in review_selectors.items():\n",
    "                print(f\"   â€¢ {key}: {value}\")\n",
    "        else:\n",
    "            print(\"âš ï¸ SÃ©lecteurs reviews non dÃ©tectÃ©s, utilisation des sÃ©lecteurs par dÃ©faut\")\n",
    "            review_selectors = scout.selectors.get(site, {}).get('reviews', {})\n",
    "        \n",
    "        # Sauvegarder les sÃ©lecteurs validÃ©s\n",
    "        validated_selectors = {\n",
    "            site: {\n",
    "                'products': product_selectors,\n",
    "                'reviews': review_selectors\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        selectors_file = f\"../config/product_review_selectors_{site}.json\"\n",
    "        if scout.save_selectors(validated_selectors, selectors_file):\n",
    "            print(f\"âœ… SÃ©lecteurs sauvegardÃ©s: {selectors_file}\")\n",
    "        \n",
    "        scout.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Erreur phase dÃ©tection: {e}\")\n",
    "        scout.close()\n",
    "        return None\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    \n",
    "    # ============================================================================\n",
    "    # PHASE 2: SCRAPING DES REVIEWS\n",
    "    # ============================================================================\n",
    "    print(\"ğŸ“Š PHASE 2: SCRAPING DES REVIEWS\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    scraper = ProductReviewScraper(selectors_file)\n",
    "    \n",
    "    try:\n",
    "        # Setup du driver pour le scraping\n",
    "        if not scraper.setup_driver(headless=False):  # Visible pour monitoring\n",
    "            print(\"âŒ Ã‰chec initialisation driver de scraping\")\n",
    "            return None\n",
    "        \n",
    "        print(\"âœ… Driver de scraping initialisÃ©\")\n",
    "        print(f\"ğŸ­ User-Agent: {scraper.driver.execute_script('return navigator.userAgent;')[:80]}...\")\n",
    "        \n",
    "        # Avertissement utilisateur\n",
    "        print(\"\\n\" + \"ğŸš¨ AVERTISSEMENT: Scraping en cours sur site rÃ©el!\")\n",
    "        print(\"â° Estimation durÃ©e: {} minutes\".format(max_products * 5))  # ~5min par produit\n",
    "        print(\"ğŸ“ Respect des ToS et limitations de dÃ©bit\")\n",
    "        \n",
    "        input(\"Appuyer sur EntrÃ©e pour continuer ou Ctrl+C pour annuler...\")\n",
    "        \n",
    "        # Lancement du scraping\n",
    "        print(f\"\\nğŸš€ DÃ©but du scraping pour '{category_search}'...\")\n",
    "        \n",
    "        df_reviews = scraper.scrape_category_product_reviews(\n",
    "            site_url=site_url,\n",
    "            category_search=category_search,\n",
    "            max_products=max_products,\n",
    "            reviews_per_rating=reviews_per_rating\n",
    "        )\n",
    "        \n",
    "        if df_reviews.empty:\n",
    "            print(\"âŒ Aucune review rÃ©cupÃ©rÃ©e\")\n",
    "            scraper.close()\n",
    "            return None\n",
    "        \n",
    "        # Analyse des rÃ©sultats\n",
    "        print(\"\\n\" + \"ğŸ“Š RÃ‰SULTATS DU SCRAPING\")\n",
    "        print(\"-\" * 30)\n",
    "        print(f\"âœ… Total reviews: {len(df_reviews)}\")\n",
    "        print(f\"ğŸ“¦ Produits uniques: {df_reviews['product_name'].nunique()}\")\n",
    "        print(f\"â­ Distribution des notes:\")\n",
    "        \n",
    "        rating_dist = df_reviews['user_rating'].value_counts().sort_index()\n",
    "        for rating, count in rating_dist.items():\n",
    "            print(f\"   {rating} Ã©toiles: {count} reviews\")\n",
    "        \n",
    "        print(f\"ğŸ“ Longueur moyenne: {df_reviews['review_length'].mean():.0f} caractÃ¨res\")\n",
    "        \n",
    "        # Sauvegarde\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        output_file = f\"../data/raw/{site}_{category_search}_reviews_{timestamp}.csv\"\n",
    "        \n",
    "        saved_file = scraper.save_reviews(df_reviews, output_file)\n",
    "        \n",
    "        if saved_file:\n",
    "            print(f\"âœ… DonnÃ©es sauvegardÃ©es: {saved_file}\")\n",
    "            \n",
    "            # AperÃ§u des donnÃ©es\n",
    "            print(\"\\nğŸ“‹ APERÃ‡U DES DONNÃ‰ES:\")\n",
    "            print(df_reviews[['product_name', 'user_rating', 'review_text']].head(3).to_string())\n",
    "            \n",
    "        scraper.close()\n",
    "        \n",
    "        print(\"\\n\" + \"ğŸ‰ WORKFLOW TERMINÃ‰ AVEC SUCCÃˆS!\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        return df_reviews\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nâ¹ï¸ ArrÃªt demandÃ© par l'utilisateur\")\n",
    "        scraper.close()\n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Erreur phase scraping: {e}\")\n",
    "        scraper.close()\n",
    "        return None\n",
    "\n",
    "def quick_review_test(product_url, max_reviews=20):\n",
    "    \"\"\"\n",
    "    Test rapide pour scraper les reviews d'un produit spÃ©cifique\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ§ª TEST RAPIDE - Reviews d'un produit\")\n",
    "    print(f\"ğŸ”— URL: {product_url}\")\n",
    "    print(f\"ğŸ“Š Reviews max: {max_reviews}\")\n",
    "    \n",
    "    scraper = ProductReviewScraper()\n",
    "    \n",
    "    try:\n",
    "        if not scraper.setup_driver(headless=False):\n",
    "            print(\"âŒ Ã‰chec setup driver\")\n",
    "            return None\n",
    "        \n",
    "        # Simuler un produit\n",
    "        fake_product = {\n",
    "            'title': 'Produit Test',\n",
    "            'url': product_url,\n",
    "            'category': 'test'\n",
    "        }\n",
    "        \n",
    "        # Scraper les reviews\n",
    "        reviews = scraper._scrape_product_reviews(fake_product, max_reviews)\n",
    "        \n",
    "        if reviews:\n",
    "            df = pd.DataFrame(reviews)\n",
    "            print(f\"âœ… {len(reviews)} reviews rÃ©cupÃ©rÃ©es\")\n",
    "            print(\"\\nğŸ“‹ AperÃ§u:\")\n",
    "            for i, review in enumerate(reviews[:3], 1):\n",
    "                print(f\"\\nReview {i}:\")\n",
    "                print(f\"  Note: {review.get('user_rating', 'N/A')}\")\n",
    "                print(f\"  Titre: {review.get('review_title', 'N/A')[:50]}...\")\n",
    "                print(f\"  Texte: {review.get('review_text', 'N/A')[:100]}...\")\n",
    "            \n",
    "            return df\n",
    "        else:\n",
    "            print(\"âŒ Aucune review trouvÃ©e\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Erreur test: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        scraper.close()\n",
    "\n",
    "def reviews_workflow_menu():\n",
    "    \"\"\"Menu principal pour le workflow de reviews\"\"\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"ğŸ¯ WORKFLOW REVIEWS DE PRODUITS - MENU PRINCIPAL\")\n",
    "    print(\"=\"*80)\n",
    "    print()\n",
    "    print(\"1ï¸âƒ£ Workflow complet (dÃ©tection + scraping)\")\n",
    "    print(\"2ï¸âƒ£ Test rapide sur un produit\")\n",
    "    print(\"3ï¸âƒ£ Configuration personnalisÃ©e\")\n",
    "    print(\"4ï¸âƒ£ Voir les sÃ©lecteurs sauvegardÃ©s\")\n",
    "    print(\"5ï¸âƒ£ Quitter\")\n",
    "    print()\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            choice = input(\"ğŸ‘‰ Votre choix (1-5): \").strip()\n",
    "            \n",
    "            if choice == '1':\n",
    "                # Workflow complet\n",
    "                print(\"\\nğŸ“‹ Configuration du workflow complet:\")\n",
    "                category = input(\"ğŸ·ï¸ CatÃ©gorie de produits (ex: 'laptop', 'smartphone'): \").strip() or \"laptop\"\n",
    "                site = input(\"ğŸŒ Site (amazon/ebay): \").strip() or \"amazon\"\n",
    "                \n",
    "                try:\n",
    "                    max_products = int(input(\"ğŸ“¦ Nombre de produits (dÃ©faut: 10): \") or \"10\")\n",
    "                    reviews_per_rating = int(input(\"â­ Reviews par note (dÃ©faut: 50): \") or \"50\")\n",
    "                except ValueError:\n",
    "                    max_products, reviews_per_rating = 10, 50\n",
    "                \n",
    "                return product_reviews_workflow(category, site, max_products, reviews_per_rating)\n",
    "                \n",
    "            elif choice == '2':\n",
    "                # Test rapide\n",
    "                product_url = input(\"ğŸ”— URL du produit Ã  tester: \").strip()\n",
    "                if product_url:\n",
    "                    try:\n",
    "                        max_reviews = int(input(\"ğŸ“Š Nombre max de reviews (dÃ©faut: 20): \") or \"20\")\n",
    "                    except ValueError:\n",
    "                        max_reviews = 20\n",
    "                    return quick_review_test(product_url, max_reviews)\n",
    "                else:\n",
    "                    print(\"âŒ URL requise\")\n",
    "                    \n",
    "            elif choice == '3':\n",
    "                # Configuration avancÃ©e\n",
    "                print(\"\\nâš™ï¸ Configuration personnalisÃ©e disponible dans product_reviews_workflow()\")\n",
    "                print(\"ğŸ“– Consultez la documentation de la fonction\")\n",
    "                \n",
    "            elif choice == '4':\n",
    "                # Voir sÃ©lecteurs\n",
    "                print(\"\\nğŸ“‹ SÃ©lecteurs sauvegardÃ©s:\")\n",
    "                for filename in ['../config/product_review_selectors_amazon.json', '../config/product_review_selectors_ebay.json']:\n",
    "                    if os.path.exists(filename):\n",
    "                        print(f\"âœ… {filename}\")\n",
    "                        try:\n",
    "                            with open(filename, 'r') as f:\n",
    "                                data = json.load(f)\n",
    "                                print(f\"   Sites: {list(data.keys())}\")\n",
    "                        except:\n",
    "                            pass\n",
    "                    else:\n",
    "                        print(f\"âŒ {filename} (non trouvÃ©)\")\n",
    "                        \n",
    "            elif choice == '5':\n",
    "                print(\"ğŸ‘‹ Ã€ bientÃ´t!\")\n",
    "                break\n",
    "                \n",
    "            else:\n",
    "                print(\"âŒ Choix invalide, veuillez rÃ©essayer\")\n",
    "                \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nğŸ‘‹ Ã€ bientÃ´t!\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Erreur: {e}\")\n",
    "\n",
    "print(\"âœ… Workflow de reviews de produits crÃ©Ã©\")\n",
    "print(\"ğŸ“– Utilisez reviews_workflow_menu() pour commencer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4218b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ WORKFLOW REVIEWS DE PRODUITS - PRÃŠT Ã€ UTILISER\n",
      "================================================================================\n",
      "\n",
      "ğŸ“‹ EXEMPLES D'UTILISATION:\n",
      "\n",
      "1ï¸âƒ£ Workflow complet automatique:\n",
      "   df = product_reviews_workflow('laptop', 'amazon', 10, 50)\n",
      "   # RÃ©cupÃ¨re 50 reviews par note (1-5) pour 10 laptops sur Amazon\n",
      "\n",
      "2ï¸âƒ£ Menu interactif:\n",
      "   reviews_workflow_menu()\n",
      "   # Interface guidÃ©e pour configurer le scraping\n",
      "\n",
      "3ï¸âƒ£ Test rapide d'un produit:\n",
      "   df = quick_review_test('https://amazon.com/dp/XXXXXXXXXX', 20)\n",
      "   # Test sur un produit spÃ©cifique\n",
      "\n",
      "4ï¸âƒ£ Configurations personnalisÃ©es:\n",
      "   # Smartphones sur eBay\n",
      "   df = product_reviews_workflow('smartphone', 'ebay', 5, 30)\n",
      "\n",
      "   # Casques audio sur Amazon\n",
      "   df = product_reviews_workflow('headphones', 'amazon', 15, 40)\n",
      "\n",
      "ğŸ“Š DONNÃ‰ES RÃ‰CUPÃ‰RÃ‰ES:\n",
      "   â€¢ product_name: nom du produit\n",
      "   â€¢ product_category: catÃ©gorie recherchÃ©e\n",
      "   â€¢ review_title: titre de la review\n",
      "   â€¢ review_text: texte complet de la review\n",
      "   â€¢ user_rating: note donnÃ©e (1-5)\n",
      "   â€¢ reviewer_name: nom du reviewer\n",
      "   â€¢ review_date: date de la review\n",
      "   â€¢ scraped_at: timestamp du scraping\n",
      "\n",
      "ğŸ’¾ SAUVEGARDE AUTOMATIQUE:\n",
      "   â€¢ Format CSV dans ../data/raw/\n",
      "   â€¢ Nom: {site}_{categorie}_reviews_{timestamp}.csv\n",
      "\n",
      "ğŸ›¡ï¸ SÃ‰CURITÃ‰:\n",
      "   â€¢ Anti-dÃ©tection avec user-agents alÃ©atoires\n",
      "   â€¢ DÃ©lais humains entre requÃªtes\n",
      "   â€¢ Respect des limitations de dÃ©bit\n",
      "   â€¢ Options Chrome optimisÃ©es\n",
      "\n",
      "âš ï¸ IMPORTANT:\n",
      "   â€¢ Respecter les ToS des sites\n",
      "   â€¢ Utiliser avec modÃ©ration\n",
      "   â€¢ VÃ©rifier robots.txt\n",
      "   â€¢ Ne pas surcharger les serveurs\n",
      "\n",
      "ğŸš€ Pour commencer, utilisez:\n",
      "   reviews_workflow_menu()\n",
      "\n",
      "âœ… Workflow prÃªt! Tapez reviews_workflow_menu() pour commencer\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# EXEMPLES D'UTILISATION DU WORKFLOW REVIEWS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"ğŸ¯ WORKFLOW REVIEWS DE PRODUITS - PRÃŠT Ã€ UTILISER\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "print(\"ğŸ“‹ EXEMPLES D'UTILISATION:\")\n",
    "print()\n",
    "print(\"1ï¸âƒ£ Workflow complet automatique:\")\n",
    "print(\"   df = product_reviews_workflow('laptop', 'amazon', 10, 50)\")\n",
    "print(\"   # RÃ©cupÃ¨re 50 reviews par note (1-5) pour 10 laptops sur Amazon\")\n",
    "print()\n",
    "print(\"2ï¸âƒ£ Menu interactif:\")\n",
    "print(\"   reviews_workflow_menu()\")\n",
    "print(\"   # Interface guidÃ©e pour configurer le scraping\")\n",
    "print()\n",
    "print(\"3ï¸âƒ£ Test rapide d'un produit:\")\n",
    "print(\"   df = quick_review_test('https://amazon.com/dp/XXXXXXXXXX', 20)\")\n",
    "print(\"   # Test sur un produit spÃ©cifique\")\n",
    "print()\n",
    "print(\"4ï¸âƒ£ Configurations personnalisÃ©es:\")\n",
    "print(\"   # Smartphones sur eBay\")\n",
    "print(\"   df = product_reviews_workflow('smartphone', 'ebay', 5, 30)\")\n",
    "print()\n",
    "print(\"   # Casques audio sur Amazon\")\n",
    "print(\"   df = product_reviews_workflow('headphones', 'amazon', 15, 40)\")\n",
    "print()\n",
    "print(\"ğŸ“Š DONNÃ‰ES RÃ‰CUPÃ‰RÃ‰ES:\")\n",
    "print(\"   â€¢ product_name: nom du produit\")\n",
    "print(\"   â€¢ product_category: catÃ©gorie recherchÃ©e\")\n",
    "print(\"   â€¢ review_title: titre de la review\")\n",
    "print(\"   â€¢ review_text: texte complet de la review\")\n",
    "print(\"   â€¢ user_rating: note donnÃ©e (1-5)\")\n",
    "print(\"   â€¢ reviewer_name: nom du reviewer\")\n",
    "print(\"   â€¢ review_date: date de la review\")\n",
    "print(\"   â€¢ scraped_at: timestamp du scraping\")\n",
    "print()\n",
    "print(\"ğŸ’¾ SAUVEGARDE AUTOMATIQUE:\")\n",
    "print(\"   â€¢ Format CSV dans ../data/raw/\")\n",
    "print(\"   â€¢ Nom: {site}_{categorie}_reviews_{timestamp}.csv\")\n",
    "print()\n",
    "print(\"ğŸ›¡ï¸ SÃ‰CURITÃ‰:\")\n",
    "print(\"   â€¢ Anti-dÃ©tection avec user-agents alÃ©atoires\")\n",
    "print(\"   â€¢ DÃ©lais humains entre requÃªtes\")\n",
    "print(\"   â€¢ Respect des limitations de dÃ©bit\")\n",
    "print(\"   â€¢ Options Chrome optimisÃ©es\")\n",
    "print()\n",
    "print(\"âš ï¸ IMPORTANT:\")\n",
    "print(\"   â€¢ Respecter les ToS des sites\")\n",
    "print(\"   â€¢ Utiliser avec modÃ©ration\")\n",
    "print(\"   â€¢ VÃ©rifier robots.txt\")\n",
    "print(\"   â€¢ Ne pas surcharger les serveurs\")\n",
    "print()\n",
    "print(\"ğŸš€ Pour commencer, utilisez:\")\n",
    "print(\"   reviews_workflow_menu()\")\n",
    "\n",
    "# Exemple de configuration prÃªte Ã  l'emploi\n",
    "EXAMPLE_CONFIGS = {\n",
    "    'laptops_amazon': {\n",
    "        'category_search': 'laptop gaming',\n",
    "        'site': 'amazon',\n",
    "        'max_products': 8,\n",
    "        'reviews_per_rating': 40,\n",
    "        'description': 'Reviews de laptops gaming sur Amazon'\n",
    "    },\n",
    "    'smartphones_ebay': {\n",
    "        'category_search': 'smartphone iphone',\n",
    "        'site': 'ebay', \n",
    "        'max_products': 5,\n",
    "        'reviews_per_rating': 30,\n",
    "        'description': 'Reviews d\\'iPhones sur eBay'\n",
    "    },\n",
    "    'headphones_amazon': {\n",
    "        'category_search': 'wireless headphones',\n",
    "        'site': 'amazon',\n",
    "        'max_products': 12,\n",
    "        'reviews_per_rating': 35,\n",
    "        'description': 'Reviews de casques sans-fil sur Amazon'\n",
    "    }\n",
    "}\n",
    "\n",
    "def run_example_config(config_name):\n",
    "    \"\"\"ExÃ©cute une configuration d'exemple\"\"\"\n",
    "    if config_name in EXAMPLE_CONFIGS:\n",
    "        config = EXAMPLE_CONFIGS[config_name]\n",
    "        print(f\"ğŸš€ Lancement: {config['description']}\")\n",
    "        return product_reviews_workflow(**{k:v for k,v in config.items() if k != 'description'})\n",
    "    else:\n",
    "        print(f\"âŒ Configuration '{config_name}' non trouvÃ©e\")\n",
    "        print(f\"ğŸ“‹ Disponibles: {list(EXAMPLE_CONFIGS.keys())}\")\n",
    "        return None\n",
    "\n",
    "print(\"\\nâœ… Workflow prÃªt! Tapez reviews_workflow_menu() pour commencer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6933b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ¯ WORKFLOW REVIEWS DE PRODUITS - MENU PRINCIPAL\n",
      "================================================================================\n",
      "\n",
      "1ï¸âƒ£ Workflow complet (dÃ©tection + scraping)\n",
      "2ï¸âƒ£ Test rapide sur un produit\n",
      "3ï¸âƒ£ Configuration personnalisÃ©e\n",
      "4ï¸âƒ£ Voir les sÃ©lecteurs sauvegardÃ©s\n",
      "5ï¸âƒ£ Quitter\n",
      "\n",
      "\n",
      "ğŸ“‹ Configuration du workflow complet:\n",
      "\n",
      "ğŸ“‹ Configuration du workflow complet:\n",
      "================================================================================\n",
      "ğŸ¯ WORKFLOW SPÃ‰CIALISÃ‰ - REVIEWS DE PRODUITS\n",
      "================================================================================\n",
      "ğŸ“¦ CatÃ©gorie: laptop\n",
      "ğŸŒ Site: amazon\n",
      "ğŸ“Š Produits: 2\n",
      "â­ Reviews par note: 10\n",
      "ğŸ“ˆ Total estimÃ©: 100 reviews max\n",
      "\n",
      "ğŸ” PHASE 1: DÃ‰TECTION AUTOMATIQUE DES BALISES\n",
      "--------------------------------------------------\n",
      "================================================================================\n",
      "ğŸ¯ WORKFLOW SPÃ‰CIALISÃ‰ - REVIEWS DE PRODUITS\n",
      "================================================================================\n",
      "ğŸ“¦ CatÃ©gorie: laptop\n",
      "ğŸŒ Site: amazon\n",
      "ğŸ“Š Produits: 2\n",
      "â­ Reviews par note: 10\n",
      "ğŸ“ˆ Total estimÃ©: 100 reviews max\n",
      "\n",
      "ğŸ” PHASE 1: DÃ‰TECTION AUTOMATIQUE DES BALISES\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 16:24:40,828 - INFO - patching driver executable C:\\Users\\Yann\\appdata\\roaming\\undetected_chromedriver\\undetected_chromedriver.exe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Driver de dÃ©tection initialisÃ©\n",
      "ğŸ” DÃ©tection des sÃ©lecteurs de produits sur amazon...\n",
      "âœ… SÃ©lecteurs produits dÃ©tectÃ©s:\n",
      "   â€¢ product_container: [data-component-type=\"s-search-result\"]\n",
      "   â€¢ product_title: h2 span\n",
      "   â€¢ product_url: h2 a, .a-link-normal\n",
      "   â€¢ product_price: .a-price .a-offscreen, .a-price-whole\n",
      "   â€¢ product_rating: .a-icon-alt\n",
      "ğŸ” Test dÃ©tection sÃ©lecteurs de reviews...\n",
      "âœ… SÃ©lecteurs produits dÃ©tectÃ©s:\n",
      "   â€¢ product_container: [data-component-type=\"s-search-result\"]\n",
      "   â€¢ product_title: h2 span\n",
      "   â€¢ product_url: h2 a, .a-link-normal\n",
      "   â€¢ product_price: .a-price .a-offscreen, .a-price-whole\n",
      "   â€¢ product_rating: .a-icon-alt\n",
      "ğŸ” Test dÃ©tection sÃ©lecteurs de reviews...\n",
      "âš ï¸ SÃ©lecteurs reviews non dÃ©tectÃ©s, utilisation des sÃ©lecteurs par dÃ©faut\n",
      "âœ… SÃ©lecteurs sauvegardÃ©s: ../config/product_review_selectors_amazon.json\n",
      "âœ… SÃ©lecteurs sauvegardÃ©s: ../config/product_review_selectors_amazon.json\n",
      "âš ï¸ SÃ©lecteurs reviews non dÃ©tectÃ©s, utilisation des sÃ©lecteurs par dÃ©faut\n",
      "âœ… SÃ©lecteurs sauvegardÃ©s: ../config/product_review_selectors_amazon.json\n",
      "âœ… SÃ©lecteurs sauvegardÃ©s: ../config/product_review_selectors_amazon.json\n",
      "\n",
      "==================================================\n",
      "ğŸ“Š PHASE 2: SCRAPING DES REVIEWS\n",
      "--------------------------------------------------\n",
      "\n",
      "==================================================\n",
      "ğŸ“Š PHASE 2: SCRAPING DES REVIEWS\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 16:24:55,093 - INFO - patching driver executable C:\\Users\\Yann\\appdata\\roaming\\undetected_chromedriver\\undetected_chromedriver.exe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Erreur setup driver: Message: invalid argument: cannot parse capability: goog:chromeOptions\n",
      "from invalid argument: unrecognized chrome option: useAutomationExtension\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x0xf54493+62419]\n",
      "\tGetHandleVerifier [0x0xf544d4+62484]\n",
      "\t(No symbol) [0x0xd92133]\n",
      "\t(No symbol) [0x0xdb9723]\n",
      "\t(No symbol) [0x0xdbaeb0]\n",
      "\t(No symbol) [0x0xdb5fea]\n",
      "\t(No symbol) [0x0xe09832]\n",
      "\t(No symbol) [0x0xe0931c]\n",
      "\t(No symbol) [0x0xe0aa20]\n",
      "\t(No symbol) [0x0xe0a82a]\n",
      "\t(No symbol) [0x0xdff266]\n",
      "\t(No symbol) [0x0xdce852]\n",
      "\t(No symbol) [0x0xdcf6f4]\n",
      "\tGetHandleVerifier [0x0x11c4773+2619059]\n",
      "\tGetHandleVerifier [0x0x11bfb8a+2599626]\n",
      "\tGetHandleVerifier [0x0xf7b03a+221050]\n",
      "\tGetHandleVerifier [0x0xf6b2b8+156152]\n",
      "\tGetHandleVerifier [0x0xf71c6d+183213]\n",
      "\tGetHandleVerifier [0x0xf5c378+94904]\n",
      "\tGetHandleVerifier [0x0xf5c502+95298]\n",
      "\tGetHandleVerifier [0x0xf4765a+9626]\n",
      "\tBaseThreadInitThunk [0x0x76775d49+25]\n",
      "\tRtlInitializeExceptionChain [0x0x778ed09b+107]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x0x778ed021+561]\n",
      "\n",
      "âŒ Ã‰chec initialisation driver de scraping\n"
     ]
    }
   ],
   "source": [
    "reviews_workflow_menu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66698f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CORRECTION ROBUSTE DES OPTIONS CHROME\n",
    "# ============================================================================\n",
    "\n",
    "def create_robust_chrome_options(headless=True):\n",
    "    \"\"\"\n",
    "    CrÃ©e des options Chrome 100% compatibles avec toutes les versions\n",
    "    Ã‰vite toutes les options problÃ©matiques\n",
    "    \"\"\"\n",
    "    import undetected_chromedriver as uc\n",
    "    \n",
    "    try:\n",
    "        options = uc.ChromeOptions()\n",
    "        \n",
    "        # Arguments de base sÃ»rs et testÃ©s\n",
    "        safe_args = [\n",
    "            '--no-sandbox',\n",
    "            '--disable-dev-shm-usage',\n",
    "            '--disable-gpu',\n",
    "            '--disable-web-security',\n",
    "            '--disable-features=VizDisplayCompositor',\n",
    "            '--disable-extensions',\n",
    "            '--disable-plugins',\n",
    "            '--disable-default-apps',\n",
    "            '--disable-background-timer-throttling',\n",
    "            '--disable-backgrounding-occluded-windows',\n",
    "            '--disable-renderer-backgrounding',\n",
    "            '--disable-field-trial-config',\n",
    "            '--disable-back-forward-cache',\n",
    "            '--disable-ipc-flooding-protection',\n",
    "            '--window-size=1920,1080',\n",
    "            '--remote-debugging-port=9222'\n",
    "        ]\n",
    "        \n",
    "        # Ajouter les arguments sÃ»rs\n",
    "        for arg in safe_args:\n",
    "            options.add_argument(arg)\n",
    "        \n",
    "        # Mode headless si demandÃ©\n",
    "        if headless:\n",
    "            options.add_argument('--headless=new')  # Nouveau mode headless\n",
    "        \n",
    "        # User agent alÃ©atoire\n",
    "        try:\n",
    "            user_agent = random.choice(REALISTIC_USER_AGENTS)\n",
    "            options.add_argument(f'--user-agent={user_agent}')\n",
    "        except:\n",
    "            # Fallback si REALISTIC_USER_AGENTS n'existe pas\n",
    "            options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')\n",
    "        \n",
    "        # PrÃ©fÃ©rences sÃ»res SEULEMENT\n",
    "        prefs = {\n",
    "            \"profile.default_content_setting_values.notifications\": 2,\n",
    "            \"profile.default_content_settings.popups\": 0,\n",
    "            \"profile.managed_default_content_settings.images\": 2,\n",
    "            \"profile.default_content_setting_values.media_stream_mic\": 2,\n",
    "            \"profile.default_content_setting_values.media_stream_camera\": 2,\n",
    "            \"profile.default_content_setting_values.geolocation\": 2\n",
    "        }\n",
    "        \n",
    "        options.add_experimental_option(\"prefs\", prefs)\n",
    "        \n",
    "        # NE PAS AJOUTER: excludeSwitches, useAutomationExtension\n",
    "        # Ces options causent des erreurs dans les nouvelles versions\n",
    "        \n",
    "        return options\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Erreur crÃ©ation options Chrome: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_robust_driver(headless=True, max_retries=3):\n",
    "    \"\"\"\n",
    "    CrÃ©e un driver robuste avec plusieurs tentatives et fallbacks\n",
    "    \"\"\"\n",
    "    import undetected_chromedriver as uc\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\"ğŸ”§ Tentative {attempt + 1}/{max_retries} - CrÃ©ation driver...\")\n",
    "            \n",
    "            # Options robustes\n",
    "            options = create_robust_chrome_options(headless)\n",
    "            if not options:\n",
    "                continue\n",
    "            \n",
    "            # CrÃ©ation du driver avec paramÃ¨tres optimaux\n",
    "            driver = uc.Chrome(\n",
    "                options=options,\n",
    "                version_main=None,  # Auto-dÃ©tection\n",
    "                headless=headless,\n",
    "                use_subprocess=False,\n",
    "                log_level=3  # RÃ©duire les logs\n",
    "            )\n",
    "            \n",
    "            # Test rapide\n",
    "            driver.get(\"data:text/html,<html><body><h1>Test</h1></body></html>\")\n",
    "            \n",
    "            print(\"âœ… Driver robuste crÃ©Ã© avec succÃ¨s!\")\n",
    "            return driver\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Tentative {attempt + 1} Ã©chouÃ©e: {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                print(\"ğŸ”„ Nouvelle tentative...\")\n",
    "                time.sleep(2)\n",
    "            else:\n",
    "                print(\"âŒ Toutes les tentatives ont Ã©chouÃ©\")\n",
    "                return create_selenium_fallback_driver()\n",
    "    \n",
    "    return None\n",
    "\n",
    "def create_selenium_fallback_driver():\n",
    "    \"\"\"\n",
    "    Driver de secours avec Selenium classique\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"ğŸ”„ Fallback vers Selenium classique...\")\n",
    "        \n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.chrome.service import Service\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "        \n",
    "        # Options Selenium classiques\n",
    "        options = Options()\n",
    "        options.add_argument('--no-sandbox')\n",
    "        options.add_argument('--disable-dev-shm-usage')\n",
    "        options.add_argument('--disable-gpu')\n",
    "        options.add_argument('--window-size=1920,1080')\n",
    "        \n",
    "        # Service gÃ©rÃ© automatiquement\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        \n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        \n",
    "        print(\"âœ… Driver Selenium classique crÃ©Ã©!\")\n",
    "        return driver\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Fallback Selenium Ã©chouÃ©: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test du systÃ¨me robuste\n",
    "print(\"ğŸ§ª Test du systÃ¨me de crÃ©ation de driver robuste...\")\n",
    "\n",
    "test_driver = create_robust_driver(headless=True)\n",
    "if test_driver:\n",
    "    try:\n",
    "        test_driver.get(\"https://httpbin.org/user-agent\")\n",
    "        print(\"âœ… Navigation test rÃ©ussie!\")\n",
    "        test_driver.quit()\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Test navigation: {e}\")\n",
    "        test_driver.quit()\n",
    "else:\n",
    "    print(\"âŒ Impossible de crÃ©er un driver robuste\")\n",
    "\n",
    "print(\"âœ… SystÃ¨me de driver robuste prÃªt!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scraper_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
